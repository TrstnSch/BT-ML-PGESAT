{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'evaluation' from 'c:\\\\Users\\\\trist\\\\Git_repos\\\\BT-ML-PGESAT\\\\code\\\\PGExplainer\\\\evaluation.py'>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networks\n",
    "import datasetLoader\n",
    "import evaluation\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "importlib.reload(datasetLoader)\n",
    "importlib.reload(networks)\n",
    "importlib.reload(evaluation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PyG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[3, 1], edge_index=[2, 4])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "edge_index = torch.tensor([[0, 1, 1, 2],\n",
    "                           [1, 0, 2, 1]], dtype=torch.long)\n",
    "x = torch.tensor([[-1], [0], [1]], dtype=torch.float)\n",
    "\n",
    "data = Data(x=x, edge_index=edge_index)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rate_gnn = 0.001        #0.001 on ADAM\n",
    "epochs_graphgnn = 5000\n",
    "epochs_nodegnn = 1000\n",
    "\n",
    "\n",
    "learning_rate_mlp = 0.003        #0.003 on ADAM\n",
    "coefficientSizeReg = 0.05\n",
    "entropyReg = 1\n",
    "epochs_mlp = 30\n",
    "\n",
    "temperature =  0        #???\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 800 graphs with batch size 1\n",
      "100\n",
      "\n",
      "------------------ EPOCH 1 -------------------\n",
      "average training loss: 0.0008873338997364044, training acc: 0.5287500023841858\n",
      "tensor(65.2019)\n",
      "validation loss: 0.6520191431045532, validation acc: 0.5199999809265137, test loss: 0.6530731320381165, test acc: 0.49000000953674316\n",
      "\n",
      "------------------ EPOCH 2 -------------------\n",
      "average training loss: 0.0004222356528043747, training acc: 0.6675000190734863\n",
      "tensor(30.2551)\n",
      "validation loss: 0.30255112051963806, validation acc: 0.9399999976158142, test loss: 0.3106735944747925, test acc: 0.9300000071525574\n",
      "\n",
      "------------------ EPOCH 3 -------------------\n",
      "average training loss: 1.6606738790869713e-06, training acc: 0.9325000047683716\n",
      "tensor(2.0028)\n",
      "validation loss: 0.02002844400703907, validation acc: 1.0, test loss: 0.03854673355817795, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 4 -------------------\n",
      "average training loss: 3.2365109655074775e-06, training acc: 0.9950000047683716\n",
      "tensor(0.5951)\n",
      "validation loss: 0.005950516555458307, validation acc: 1.0, test loss: 0.025003882125020027, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 5 -------------------\n",
      "average training loss: 1.5838930266909302e-07, training acc: 0.9962499737739563\n",
      "tensor(0.2203)\n",
      "validation loss: 0.0022033238783478737, validation acc: 1.0, test loss: 0.01392050739377737, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 6 -------------------\n",
      "average training loss: 6.723085971316323e-07, training acc: 0.9950000047683716\n",
      "tensor(0.3656)\n",
      "validation loss: 0.0036559158470481634, validation acc: 1.0, test loss: 0.031482093036174774, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 7 -------------------\n",
      "average training loss: 1.0326078154321294e-07, training acc: 0.9975000023841858\n",
      "tensor(0.1880)\n",
      "validation loss: 0.0018797412049025297, validation acc: 1.0, test loss: 0.02692175842821598, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 8 -------------------\n",
      "average training loss: 3.0215902370400726e-07, training acc: 0.9987499713897705\n",
      "tensor(0.0283)\n",
      "validation loss: 0.0002828782598953694, validation acc: 1.0, test loss: 0.011022262275218964, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 9 -------------------\n",
      "average training loss: 6.55649387226731e-09, training acc: 0.9987499713897705\n",
      "tensor(0.0246)\n",
      "validation loss: 0.00024575224961154163, validation acc: 1.0, test loss: 0.015411227941513062, test acc: 0.9900000095367432\n",
      "\n",
      "------------------ EPOCH 10 -------------------\n",
      "average training loss: 3.382517888894654e-08, training acc: 0.9987499713897705\n",
      "tensor(0.0060)\n",
      "validation loss: 5.984480958431959e-05, validation acc: 1.0, test loss: 0.003198927268385887, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 11 -------------------\n",
      "average training loss: 1.0385678251623177e-07, training acc: 1.0\n",
      "tensor(0.0035)\n",
      "validation loss: 3.46611013810616e-05, validation acc: 1.0, test loss: 0.0007371418178081512, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 12 -------------------\n",
      "average training loss: 7.599568903060572e-09, training acc: 1.0\n",
      "tensor(0.0010)\n",
      "validation loss: 9.668856364442036e-06, validation acc: 1.0, test loss: 0.0015854905359447002, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 13 -------------------\n",
      "average training loss: 2.2351721895574884e-09, training acc: 1.0\n",
      "tensor(0.0008)\n",
      "validation loss: 7.986911441548727e-06, validation acc: 1.0, test loss: 0.0009830628987401724, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 14 -------------------\n",
      "average training loss: 2.2351721895574884e-09, training acc: 1.0\n",
      "tensor(0.0005)\n",
      "validation loss: 4.705123046733206e-06, validation acc: 1.0, test loss: 0.001368166645988822, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 15 -------------------\n",
      "average training loss: 7.599568903060572e-09, training acc: 1.0\n",
      "tensor(0.0005)\n",
      "validation loss: 4.97691235068487e-06, validation acc: 1.0, test loss: 0.0021054225508123636, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 16 -------------------\n",
      "average training loss: 1.1920923270736238e-09, training acc: 1.0\n",
      "tensor(0.0010)\n",
      "validation loss: 1.018968760035932e-05, validation acc: 1.0, test loss: 0.0033367604482918978, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 17 -------------------\n",
      "average training loss: 4.17231831306708e-09, training acc: 1.0\n",
      "tensor(0.0005)\n",
      "validation loss: 5.084231815999374e-06, validation acc: 1.0, test loss: 0.00012727169087156653, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 18 -------------------\n",
      "average training loss: 3.1292398716686875e-09, training acc: 1.0\n",
      "tensor(0.0002)\n",
      "validation loss: 1.9419137515797047e-06, validation acc: 1.0, test loss: 0.00013749714707955718, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 19 -------------------\n",
      "average training loss: 2.0712443529191658e-08, training acc: 1.0\n",
      "tensor(0.0075)\n",
      "validation loss: 7.462289067916572e-05, validation acc: 1.0, test loss: 8.830102160573006e-05, test acc: 1.0\n",
      "\n",
      "------------------ EPOCH 20 -------------------\n",
      "average training loss: 1.4901160305669236e-10, training acc: 1.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# avg loss\u001b[39;00m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maverage training loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, training acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_train_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m test, val_acc, valLoss, test_acc, testLoss \u001b[38;5;241m=\u001b[39m evaluation\u001b[38;5;241m.\u001b[39mevaluateGNN(gnn, val_loader, test_loader)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28mprint\u001b[39m(test)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, validation acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtestLoss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, test acc: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\trist\\Git_repos\\BT-ML-PGESAT\\code\\PGExplainer\\evaluation.py:43\u001b[0m, in \u001b[0;36mevaluateGNN\u001b[1;34m(gnn, val_loader, test_loader)\u001b[0m\n\u001b[0;32m      0\u001b[0m <Error retrieving source code with stack_data see ipython/ipython#13598>\n",
      "File \u001b[1;32mc:\\Users\\trist\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\trist\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\trist\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1293\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\n\u001b[0;32m   1294\u001b[0m         \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   1295\u001b[0m         target,\n\u001b[0;32m   1296\u001b[0m         weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1297\u001b[0m         ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index,\n\u001b[0;32m   1298\u001b[0m         reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1299\u001b[0m         label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing,\n\u001b[0;32m   1300\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\trist\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3479\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3478\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\n\u001b[0;32m   3480\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3481\u001b[0m     target,\n\u001b[0;32m   3482\u001b[0m     weight,\n\u001b[0;32m   3483\u001b[0m     _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction),\n\u001b[0;32m   3484\u001b[0m     ignore_index,\n\u001b[0;32m   3485\u001b[0m     label_smoothing,\n\u001b[0;32m   3486\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, test_loader = datasetLoader.loadDataset('BA2Motif', batch_size)               # TODO: adjs matrix is not compatible with GraphConv. Needs to be converted to edge_index (see RE_PGE datasets/utils)\n",
    "\n",
    "temp = next(iter(train_loader))\n",
    "gnn = networks.GraphGNN(features = temp.x.shape[1], labels=2)       # temp.y.shape[0] is wrong!!! TODO: how do I get #labels from loader??\n",
    "\n",
    "\n",
    "gnn_optimizer = torch.optim.Adam(params = gnn.parameters(), lr = learning_rate_gnn)         # TODO: understand params\n",
    "\n",
    "print(f\"Training on {len(train_loader) * batch_size} graphs with batch size {batch_size}\")\n",
    "print(len(val_loader))\n",
    "\n",
    "# TODO: adapt this to work on batch_sizes > 1 !!!\n",
    "\n",
    "loss = nn.CrossEntropyLoss()           # cross entropy loss?!\n",
    "\n",
    "for epoch in range(0, epochs_nodegnn) :\n",
    "    print(f'\\n------------------ EPOCH {epoch + 1} -------------------')\n",
    "\n",
    "    gnn.train()\n",
    "\n",
    "    train_acc_sum = 0\n",
    "    \n",
    "    for batch_index, data in enumerate(train_loader):\n",
    "        \n",
    "        train_loss = 0.0\n",
    "        gnn_optimizer.zero_grad()       # Reset parameters\n",
    "\n",
    "        # real label\n",
    "        label = data.y\n",
    "\n",
    "        # get model embeddings (node representations)?\n",
    "        # predicted label\n",
    "        out = gnn.forward(data.x, data.edge_index, data.batch)\n",
    "\n",
    "        # calc cross entropy(???)loss between real label and predicted label\n",
    "        # needs to be calculated across batch\n",
    "        #print(out.argmax(dim=1))\n",
    "        currLoss = loss(out, label)\n",
    "\n",
    "        # loss backward\n",
    "        currLoss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(gnn.parameters(), max_norm=2)    # clip gradient above 2(for ba2motfis according to reimplementation) to stop \"overlearning\"?\n",
    "\n",
    "        # optimizer step\n",
    "        gnn_optimizer.step()\n",
    "\n",
    "        preds = out.argmax(dim=1)\n",
    "        train_acc_sum += torch.sum(preds == data.y)         # TODO: works with batches?\n",
    "        \n",
    "        train_loss += currLoss.item()\n",
    "\n",
    "    gnn.eval()\n",
    "\n",
    "    final_train_acc = train_acc_sum/(len(train_loader)*batch_size)              # TODO: see evaluation.py, not quite correct\n",
    "\n",
    "    # avg loss\n",
    "    print(f\"average training loss: {train_loss/len(train_loader)}, training acc: {final_train_acc}\")\n",
    "\n",
    "    test, val_acc, valLoss, test_acc, testLoss = evaluation.evaluateGNN(gnn, val_loader, test_loader)\n",
    "    print(test)\n",
    "    print(f\"validation loss: {valLoss}, validation acc: {val_acc}, test loss: {testLoss}, test acc: {test_acc}\")\n",
    "    # Evaluation per epoch\n",
    "    # TODO: implement evaluation methods that wprk on batch_sizes > 1\n",
    "    \"\"\"gnn.eval()\n",
    "\n",
    "    num_batches = 0.0\n",
    "\n",
    "    gnn_losses = 0.0\n",
    "\n",
    "    test_acc_sum = 0\n",
    "\n",
    "    for batch_index, data in enumerate(test_loader):\n",
    "        batch_size_ratio = len(data)/batch_size\n",
    "        num_batches += batch_size_ratio\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = gnn.forward(data.x, data.edge_index, data.batch)\n",
    "            gnnLoss = loss(out, data.y)\n",
    "\n",
    "            preds = out.argmax(dim=1)\n",
    "\n",
    "            test_acc_sum += torch.sum(preds == data.y)\n",
    "            \n",
    "\n",
    "        gnn_losses += batch_size_ratio * gnnLoss\n",
    "\n",
    "    final_test_acc = test_acc_sum/len(test_loader)\n",
    "    print(f\"testing accuracay: {final_test_acc}\")\n",
    "    print(f\"testing loss average: {gnn_losses/num_batches}\")\"\"\"\n",
    "\n",
    "\n",
    "# move training loop Explainer\n",
    "#mlp_optimizer = torch.optim.Adam(lr = learning_rate_mlp)\n",
    "\n",
    "\n",
    "\"\"\"for i in enumerate(adjs):\n",
    "    #out = gnn.forward(feas[i], adjs[i].nonzero().t().contiguous())\n",
    "\n",
    "for epoch in epochs_graphgnn:\n",
    "    for graph in adjs:\n",
    "        # calculate latent variables? MLP?\n",
    "        for k in # k in monte carlo sampling?!\n",
    "            # sammple graph\n",
    "            # pred label on sampled graph\n",
    "\n",
    "    # compute loss\n",
    "    # update params with backprop\"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Saving Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(gnn.state_dict(), f\"models/BA2Motif\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = TheModelClass(*args, **kwargs)\n",
    "#model.load_state_dict(torch.load(PATH, weights_only=True))\n",
    "#model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "DataBatch(x=[25, 10], edge_index=[2, 52], y=[1], batch=[25], ptr=[2])\n",
      "test_loader contains 100 graphs with following labels:\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2440]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2392]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2350]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2379]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2447]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2868]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3183]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2349]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2182]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2408]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2530]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3427]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2202]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2978]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1891]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2166]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2485]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2011]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2230]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2641]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3771]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2312]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2306]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2393]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3100]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2234]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2395]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2376]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2263]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2599]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2563]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2202]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2506]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2361]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1995]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.4198]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2421]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3032]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2180]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2560]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3552]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2436]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2547]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2552]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2047]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3847]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2847]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2519]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2675]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2186]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2996]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2383]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2667]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2259]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3812]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2056]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2678]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2917]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2947]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3206]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2218]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2440]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2592]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2796]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2950]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2429]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1704]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2501]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3183]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2516]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1947]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2115]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1786]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1957]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3519]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2278]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2916]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2411]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2707]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3131]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2458]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2277]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2487]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2872]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2103]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2486]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2831]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.1943]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.3241]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2395]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2075]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2170]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2086]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2274]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2058]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.4103]], grad_fn=<AddmmBackward0>)\n",
      "tensor([1])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2465]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2720]], grad_fn=<AddmmBackward0>)\n",
      "tensor([0])\n",
      "10\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([[0.2821]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(len(test_loader))\n",
    "\n",
    "print(next(iter(train_loader)))         # edge_index = \"map\" for edges, x = features, y = labels\n",
    "#print(next(iter(train_loader)).x)\n",
    "\n",
    "print(\"test_loader contains 100 graphs with following labels:\")\n",
    "for i, curr in enumerate(test_loader):\n",
    "    print(curr.y)\n",
    "    print(curr.x.shape[1])\n",
    "    print(curr.batch)\n",
    "\n",
    "    out = gnn.forward(curr.x, curr.edge_index, curr.batch)\n",
    "    print(out.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataset.ConcatDataset object at 0x000001ED56EB32C0>\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2,\n",
      "        3])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2,\n",
      "        3])\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import ExplainerDataset\n",
    "from torch_geometric.datasets.graph_generator import BAGraph\n",
    "from torch_geometric.datasets.motif_generator import HouseMotif\n",
    "from torch_geometric.datasets.motif_generator import CycleMotif\n",
    "import torch_geometric.transforms as T\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "dataset1 = ExplainerDataset(\n",
    "            graph_generator=BAGraph(20, 1),\n",
    "            motif_generator=HouseMotif(),\n",
    "            num_motifs=1,\n",
    "            num_graphs=400,\n",
    "            transform=T.Constant()      # appends value 1 node feature for every node\n",
    "        )\n",
    "\n",
    "dataset2 = ExplainerDataset(\n",
    "            graph_generator=BAGraph(20, 1),\n",
    "            motif_generator=CycleMotif(5),\n",
    "            num_motifs=1,\n",
    "            num_graphs=400,\n",
    "            transform=T.Constant()\n",
    "        )\n",
    "\n",
    "dataset = torch.utils.data.ConcatDataset([dataset1, dataset2])\n",
    "\n",
    "print(dataset)\n",
    "dataset[0].y = torch.tensor([0])\n",
    "\n",
    "print(dataset[0].y)\n",
    "\n",
    "train_loader = DataLoader(dataset1, batch_size = 1, shuffle = True)\n",
    "\n",
    "print(next(iter(train_loader)).y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set 1:\n",
      "9\n",
      "3\n",
      "5\n",
      "Set 2:\n",
      "2\n",
      "6\n",
      "4\n",
      "8\n",
      "7\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "generator1 = torch.Generator().manual_seed(42)\n",
    "generator2 = torch.Generator().manual_seed(42)\n",
    "set1, set2 = torch.utils.data.random_split(range(10), [3, 7])\n",
    "set3, set4, set5 = torch.utils.data.random_split(range(30), [0.3, 0.3, 0.4])\n",
    "\n",
    "print(\"Set 1:\")\n",
    "print(set1[0])\n",
    "print(set1[1])\n",
    "print(set1[2])\n",
    "print(\"Set 2:\")\n",
    "print(set2[0])\n",
    "print(set2[1])\n",
    "print(set2[2])\n",
    "print(set2[3])\n",
    "print(set2[4])\n",
    "print(set2[5])\n",
    "print(set2[6])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
