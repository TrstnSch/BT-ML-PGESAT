@article{luo2020parameterized,
  title={Parameterized explainer for graph neural network},
  author={Luo, Dongsheng and Cheng, Wei and Xu, Dongkuan and Yu, Wenchao and Zong, Bo and Chen, Haifeng and Zhang, Xiang},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={19620--19631},
  year={2020}
}

@article{yuan2022explainability,
  title={Explainability in graph neural networks: A taxonomic survey},
  author={Yuan, Hao and Yu, Haiyang and Gui, Shurui and Ji, Shuiwang},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={5},
  pages={5782--5799},
  year={2022},
  publisher={IEEE}
}

@article{ying2019gnnexplainer,
  title={Gnnexplainer: Generating explanations for graph neural networks},
  author={Ying, Zhitao and Bourgeois, Dylan and You, Jiaxuan and Zitnik, Marinka and Leskovec, Jure},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}
@article{agarwal2023evaluating,
  title={Evaluating explainability for graph neural networks},
  author={Agarwal, Chirag and Queen, Owen and Lakkaraju, Himabindu and Zitnik, Marinka},
  journal={Scientific Data},
  volume={10},
  number={1},
  pages={144},
  year={2023},
  publisher={Nature Publishing Group UK London}
}

@article{guo2023machine,
  title={Machine learning methods in solving the boolean satisfiability problem},
  author={Guo, Wenxuan and Zhen, Hui-Ling and Li, Xijun and Luo, Wanqian and Yuan, Mingxuan and Jin, Yaohui and Yan, Junchi},
  journal={Machine Intelligence Research},
  volume={20},
  number={5},
  pages={640--655},
  year={2023},
  publisher={Springer}
}

@Inbook{Liu2020,
author="Liu, Zhiyuan
and Zhou, Jie",
title="Introduction",
bookTitle="Introduction to Graph Neural Networks",
year="2020",
publisher="Springer International Publishing",
address="Cham",
pages="1--3",
abstract="Graphs are a kind of data structure which models a set of objects (nodes) and their relationships (edges). Recently, researches of analyzing graphs with machine learning have received more and more attention because of the great expressive power of graphs, i.e., graphs can be used as denotation of a large number of systems across various areas including social science (social networks) [Hamilton et al., 2017b, Kipf and Welling, 2017], natural science (physical systems [Battaglia et al., 2016, Sanchez et al., 2018] and protein-protein interaction networks [Fout et al., 2017]), knowledge graphs [Hamaguchi et al., 2017] and many other research areas [Khalil et al., 2017]. As a unique non-Euclidean data structure for machine learning, graph draws attention on analyses that focus on node classification, link prediction, and clustering. Graph neural networks (GNNs) are deep learning-based methods that operate on graph domain. Due to its convincing performance and high interpretability, GNN has been a widely applied graph analysis method recently. In the following paragraphs, we will illustrate the fundamental motivations of GNNs.",
isbn="978-3-031-01587-8",
doi="10.1007/978-3-031-01587-8_1",
url="https://doi.org/10.1007/978-3-031-01587-8_1                 "
}

@ARTICLE{726791,
  author={Lecun, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
  journal={Proceedings of the IEEE}, 
  title={Gradient-based learning applied to document recognition}, 
  year={1998},
  volume={86},
  number={11},
  pages={2278-2324},
  keywords={Neural networks;Pattern recognition;Machine learning;Optical character recognition software;Character recognition;Feature extraction;Multi-layer neural network;Optical computing;Hidden Markov models;Principal component analysis},
  doi={10.1109/5.726791}}

@article{cai2018comprehensive,
  title={A comprehensive survey of graph embedding: Problems, techniques, and applications},
  author={Cai, Hongyun and Zheng, Vincent W and Chang, Kevin Chen-Chuan},
  journal={IEEE transactions on knowledge and data engineering},
  volume={30},
  number={9},
  pages={1616--1637},
  year={2018},
  publisher={IEEE}
}

@ARTICLE{4700287,
  author={Scarselli, Franco and Gori, Marco and Tsoi, Ah Chung and Hagenbuchner, Markus and Monfardini, Gabriele},
  journal={IEEE Transactions on Neural Networks}, 
  title={The Graph Neural Network Model}, 
  year={2009},
  volume={20},
  number={1},
  pages={61-80},
  keywords={Neural networks;Biological system modeling;Data engineering;Computer vision;Chemistry;Biology;Pattern recognition;Data mining;Supervised learning;Parameter estimation;Graphical domains;graph neural networks (GNNs);graph processing;recursive neural networks},
  doi={10.1109/TNN.2008.2005605}}

@book{asratian1998,
  title={Bipartite Graphs and their Applications},
  author={Asratian, A.S. and Denley, T.M.J. and H{\"a}ggkvist, R.},
  isbn={9781316582688},
  series={Cambridge Tracts in Mathematics},
  url={https://books.google.de/books?id=l8fLCgAAQBAJ},
  year={1998},
  publisher={Cambridge University Press}
}

@Inbook{Diestel2017,
  author="Diestel, Reinhard",
  title="The Basics",
  bookTitle="Graph Theory",
  year="2017",
  publisher="Springer Berlin Heidelberg",
  address="Berlin, Heidelberg",
  pages="1--34",
  abstract="This chapter gives a gentle yet concise introduction to most of the terminology used later in the book. Fortunately, much of standard graph theoretic terminology is so intuitive that it is easy to remember; the few terms better understood in their proper setting will be introduced later, when their time has come.",
  isbn="978-3-662-53622-3",
  doi="10.1007/978-3-662-53622-3_1",
  url="https://doi.org/10.1007/978-3-662-53622-3_1                    "
}

@inbook{Cover2005, 
  publisher = {John Wiley \& Sons, Ltd},
  isbn = {9780471748823},
  title = {Entropy, Relative Entropy, and Mutual Information},
  booktitle = {Elements of Information Theory},
  author={Cover, T.M. and Thomas, J.A.},
  pages = {13-55},
  doi = {https://doi.org/10.1002/047174882X.ch2}               ,
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/047174882X.ch2}               ,
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/047174882X.ch2}              ,
  year = {2005},
  keywords = {concept of entropy, relative entropy and mutual information, quantum channel capacity and quantum data compression},
  abstract = {Summary This chapter contains sections titled: Entropy Joint Entropy and Conditional Entropy Relative Entropy and Mutual Information Relationship Between Entropy and Mutual Information Chain Rules for Entropy, Relative Entropy, and Mutual Information Jensen's Inequality and Its Consequences Log Sum Inequality and Its Applications Data-Processing Inequality Sufficient Statistics Fano's Inequality Summary Problems Historical Notes}
}

@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}