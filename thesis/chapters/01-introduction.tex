\chapter{Introduction}
\label{ch:Introduction}

Solvers for \ac{SAT} are one of many applications that have seen progress through the development of \ac{DL} in recent years. NeuroSAT \cite{selsam2018learning} is one exemplary framework that combines \ac{ML} with SAT solving. Since SAT instances can naturally be represented as bipartite graphs, \acp{GNN} or \acp{MPNN} are a logical choice for solving this task. However, NeuroSAT lacks the ability of providing proofs to its predictions of unsatisfiability, which are required to compete with state-of-the-art SAT solvers \cite{audemard2009predicting}, \cite{een2003extensible}. This is one example of the general need for explanations that establish trust in the predictions of GNNs \cite{guo2023machine}, \cite{ribeiro2016should}. \bigskip

Several approaches have been developed to explain the predictions made by deep graph models. One perturbation-based method is the \ac{PGE} \cite{luo2020parameterized} whose explanations provide a global, generalizable understanding of the underlying trained GNN. It learns approximated discrete edge masks from GNN representations of input graphs to explain instance predictions. Since the masks are sampled from a mask predictor shared across all edges in the dataset, it is able to provide explanations with a global view of the model \cite{luo2020parameterized}, \cite{yuan2022explainability}. Moreover, this allows for the application in an inductive setting, where unexplained test instances can be explained without being used during training, as opposed to the collective setting used in previous works.\bigskip

We will start this work by introducing the theoretical background, including the concepts of GNNs and explainability in GNNs \cite{yuan2022explainability}. After explaining the main concepts of PGExplainer \cite{luo2020parameterized} we re-implement the model, as well as the underlying GNNs, using PyTorch Geometric \cite{Fey/Lenssen/2019} rather than the originally used library TensorFlow \cite{tensorflow2015-whitepaper}. Additionally, we introduce slight changes to the GNN design in regard to bipartite graphs to verify if the explainer, as claimed by the authors, is invariant to changes in model architecture. The generated explanations are evaluated using both qualitative and quantitative metrics, which we further use to optimize the model. We will then compare our results to the baseline of the original paper, as well as a present replication study, while focusing on the application in the inductive setting. \cite{holdijk2021re}. \bigskip

After a successful replication of the baseline experiments we aim to apply the explainer method to NeuroSAT. In doing so, we aim to learn and generate general, retrospective bipartite explanations of unsatisfiable instances that support the predictions of NeuroSAT. Since we require \acp{GT} to evaluate the accuracy of explanations, we propose treating \acp{MUS} - small unsatisfiable cores - of the instances as expected \ac{GT}. We justify this with the fact that unsatisfiable SAT instances can be "reduced" to MUSes by perturbing the edges outside the subset, which represent appearances of literals in clauses not included in the subset. Ultimately, our goal is testing whether the explanations provided for NeuroSAT align with "human-understandable" principles, specifically the appointed MUS \ac{GT}.