\chapter{Methodology}
\label{ch:PGExplainer}
%V1: In the following chapter, we introduce the PGExplainer\cite{luo2020parameterized} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models for any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. This approach seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{ying2019gnnexplainer}.

%V2: In the following chapter, we introduce the PGExplainer\cite{luo2020parameterized} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models that have proven to learn the concise underlying structures of GNNs most relevant to their predictions. This approach may be applied to any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. PGExplainer seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{ying2019gnnexplainer}. This means that all edges in the dataset are predicted by the same model, which leads to a global understanding of the DT.

In this chapter we present the Methodology of our work, including the detailed theory of the explainer model used in our approach. Thus, we start by presenting the concepts of the PGExplainer \cite{luo2020parameterized} in Section \ref{sec:TheoryPGE}.

In Section \ref{sec:NeuroSAT_extension} we present our idea of training the PGExplainer on the NeuroSAT framework to generate post-hoc explanations for a machine learning SAT-solving approach and comparing these to the "human-understandable" concept of unsatisfiable cores. 

We then describe our implementation in detail (see Section \ref{sec:implementation_details}), including the changes made and difficulties during the process, as well as the adaptations for the application on NeuroSAT.

\section{Theoretical Foundations of PGExplainer}
\label{sec:TheoryPGE}

In the following subchapter, we introduce the PGExplainer by Luo et al.~\cite{luo2020parameterized}. The idea is to generate explanations in the form of edge distributions or soft masks for input graphs using a probabilistic generative model for graph data. The explainer uncovers the concise underlying structures of the graphs, believed to have the biggest impact on the prediction of a GNN, as explanations. This approach may be applied to any trained GNN-based model, henceforth referred to as \ac{TM}. 

By utilizing a deep neural network to parameterize the generation process, the explainer learns to collectively explain multiple target instances. Since the parameters of the neural network are shared across the population of explained instances, PGExplainer provides "model-level explanations for each instance with a global view of the GNN model" \cite{luo2020parameterized}. Therefore, this approach cannot only be used in a collective setting, but also in an inductive setting, where explanations for not yet explained instances can be generated without retraining the explanation model. This improves the generalizability compared to previous works, particularly the GNNExplainer by Ying et al. \cite{ying2019gnnexplainer} and focuses on explaining graph structure rather than graph features. \bigskip

We follow the structure of the original paper \cite{luo2020parameterized} and start by describing the learning objective in Section \ref{sec:learning_objective}, the utilized reparameterization trick in Section \ref{sec:Reparameterization_Trick}, the idea of global explanations in Section \ref{sec:Global_Explanations} and finally the applied regularization terms in Section \ref{sec:Regularization_Terms}.

\subsection{Learning Objective}
\label{sec:learning_objective}
To explain the predictions made by a target GNN model for an original input graph $G_o$ with $m$ edges, Luo et al.~\cite{luo2020parameterized} first define the graph as a combination of two subgraphs: $G_o = G_s + \Delta G$. $G_s$ represents the subgraph holding the most relevant information for the prediction of a GNN, referred to as explanatory graph. $\Delta G$ contains the remaining edges that are deemed irrelevant for the prediction of the GNN. Inspired by GNNExplainer \cite{ying2019gnnexplainer}, the PGExplainer then finds $G_s$ by maximizing the mutual information between the predictions of the \ac{TM} and the underlying $G_s$:
\begin{equation}
    \max_{G_s} MI(Y_o;G_s) = H(Y_o) - H(Y_o|G=G_s),
\end{equation} 
where $Y_o$ is the prediction of the \ac{TM} with $G_o$ as input and number of possible classes $c$. This quantifies the probability of prediction $Y_o$ when the input graph is restricted to the explanatory graph $G_s$. In the case of $MI(Y_o;G_s) = 1$, knowing the explanatory graph $G_s$ gives us complete information about $Y_o$, and vice versa.

Intuitively, if removing an edge $(i,j)$ changes the prediction of a GNN drastically, this edge is considered important and should therefore be included in $G_s$. This idea originates from traditional forward propagation based methods for white-box explanations (see Dabkowski and Gal \cite{dabkowski2017real}).
It is important to note that $H(Y_o)$ is only related to the \ac{TM} with fixed parameters during the evaluation/explanation stage. This leads to the objective being equivalent to minimizing the conditional entropy $H(Y_o|G=G_s)$. \bigskip

To optimize this function, a relaxation is applied for the edges, since normally there would be $2^m$ candidates for $G_s$. The explanatory graph is henceforth assumed to be a Gilbert random graph, where the selections of edges from $G_o$ are conditionally independent to each other. However, Luo et al.~\cite{luo2020parameterized} describe a random graph with each edge having its own probability, rather than a shared probability as described in Section \ref{sec:random-graphs}, as follows: Let $e_{i,j}\in V \times V$ be the binary variable indicating whether the edge is selected, with $e_{i,j} = 1$ if edge $(i,j)$ is selected to be in the graph, and 0 otherwise. For the random graph variable $G$ the probability of a graph $G$ can be factorized as 
\begin{equation}
    P(G) = \prod_{(i,j)\in E(G)}P(e_{i,j}).
\end{equation}
$P(e_{i,j})$ is instantiated with the Bernoulli distribution $e_{i,j} \sim Bern(\theta_{ij})$, where $P(e_{i,j} = 1) = \theta_{ij}$ is the probability that edge $(i,j)$ exists in $G$. This can be understood as the generative model in the context of PGExplainer.

After this relaxation the learning objective becomes
\begin{equation}
    \label{eq:init_learning_obj}
    \min_{G_s}H(Y_o|G = G_s) = \min_{G_s} \mathbb{E}_{G_s}[H(Y_o|G = G_s)] \approx \min_{\Theta} \mathbb{E}_{G_s \sim q(\Theta)}[H(Y_o|G = G_s)],
\end{equation}
where $q(\Theta)$ is the distribution of the explanatory graph that is parameterized by $\Theta$'s.

\subsection{Reparameterization Trick}
\label{sec:Reparameterization_Trick}
As described in Section \ref{sec:gnn_explainability}, a reparameterization trick can be utilized to relax discrete edge weights to continuous variables in the range $(0,1)$. PGExplainer uses the reparameterizable Gumbel-Softmax estimator \cite{jang2016categorical} to allow for efficiently optimizing the objective function with gradient-based methods. This method introduces the Gumbel-Softmax distribution, a continuous distribution used to approximate samples from a categorical distribution. 

A temperature $\tau$ is used to control the smoothness of the approximation, usually starting from a high value and annealing to a small, non-zero value. Samples with $\tau > 0$ are not discrete, but are differentiable and therefore allow gradient-based optimization \cite{abid2019concrete}. The sampling process $G_s \sim q(\Theta)$ of PGExplainer is therefore approximated with a determinant function of parameters $\Omega$, a temperature $\tau$ and an independent random variable $\epsilon$: $G_s \approx \hat{G}=f_\Omega(G_o,\tau,\epsilon)$. 

The binary concrete distribution \cite{maddison2016concrete} is utilized as an instantiation for the sampling, yielding the weight $\hat{e}_{i,j} \in (0,1)$ for edge $(i,j)$ in $\hat{G}_s$, computed by
\begin{equation}
    \label{eq:reparam_trick}
    \epsilon \sim \text{Uniform}(0,1), \qquad \hat{e}_{i,j}=\sigma((\log \epsilon - \log(1-\epsilon)+\omega_{ij}/\tau),
\end{equation}
where $\sigma(\cdot)$ is the Sigmoid function and $\omega_{ij} \in \mathbb{R}$ is an explainer parameter, more specifically a logit associated with the corresponding edge (see Section \ref{sec:Global_Explanations}). When $\tau \rightarrow 0$, e.g. during the explanation stage, the weight $\hat{e}_{i,j}$ is binarized with the sigmoid function $\lim_{\tau\rightarrow 0}P(\hat{e}_{i,j} = 1) = \frac{e(\omega{ij})}{1+e(\omega{ij})}$. Since $P(e_{i,j} = 1) = \theta_{ij}$, choosing $\omega_{ij} = \log\frac{\theta_{ij}}{1-\theta_{ij}}$ leads to $\lim_{\tau\rightarrow 0}\hat{G}_s = G_s$ and justifies the approximation of the Bernoulli distribution with the binary concrete distribution. During training, when $\tau > 0$, the objective function in \eqref{eq:init_learning_obj} is smoothed with a well-defined gradient $\frac{\partial\hat{e}_{i,j}}{\partial\omega_{ij}}$ and becomes
\begin{equation}
    \min_\Omega \mathbb{E}_{\epsilon \sim \text{Uniform}(0,1)}H(Y_o| G = \hat{G}_s)
\end{equation}

The authors follow the approach of GNNExplainer \cite{ying2019gnnexplainer} and modify the objective by replacing the conditional entropy with cross entropy between the label class and the prediction of the \ac{TM}. This is justified by the greater importance of understanding the model's prediction of a certain class, rather than providing an explanation based solely on its confidence.

With the modification to cross-entropy $H(Y_o, \hat{Y}_s)$, where $\hat{Y}_s$ is the prediction of the \ac{TM} given the input $\hat{G}_s$, and the adaption of Monte Carlo sampling, the learning objective becomes
\begin{equation}
    \label{eq:monte_carlo}
    \begin{aligned}
        \min_\Omega\mathbb{E}_{\epsilon\sim\text{Uniform}(0,1)}H(Y_o, \hat{Y}_s) &\approx \min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P(Y_o = c) \log P(\hat{Y}_s = c) \\
        &= \min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P_\Phi (Y_o = c|G = G_o) \log P_\Phi(\hat{Y}_s = c|G=\hat{G}_s^{(k)}).
    \end{aligned}
    \end{equation}
$\Phi$ denotes the parameters in the \ac{TM}, $K$ is the number of total sampled graphs, $C$ is the number of class labels, and $\hat{G}_s^{(k)}$ denotes the $k$-th graph sampled with Equation \ref{eq:reparam_trick}, parameterized by $\Omega$. %Note that this objective is defined per explainable instance.



%The approach in PGExplainer is a common approach in ML for simplifying objectives? FIND LITERATURE THAT EXPLAINS APPROXIMATION OF COND. ENTROPY WITH CROSS ENTROPY. Explanation as simple as one formula for one graph variable example, cross entropy applied to whole distribution? \bigskip


\subsection{Global Explanations}
\label{sec:Global_Explanations}
The novelty of PGExplainer \cite{luo2020parameterized} lies in the ability to generate explanations for graph data with a global perspective, that allow for understanding the general picture of a model across a population. This saves resources when analyzing large graph datasets, as new instances can be explained without retraining the model, and can also be helpful for establishing the users' trust in these explanations. To achieve this the authors propose the use of a parameterized network that learns to generate explanations from the \ac{TM}, which also apply to not yet explained instances. 

GNN models generally apply two functions to first learn node representations and utilize these in downstream tasks such as graph classification or node classification \cite{kipf2016semi}, \cite{velivckovic2017graph}. Therefore, Luo et al. \cite{luo2020parameterized} define two general functions $\text{GNNE}_{\Phi_0}(\cdot)$ and $\text{GNNC}_{\Phi_1}(\cdot)$ for any target GNN in the context of PGExplainer. $\text{GNNE}_{\Phi_0}(\cdot)$ denotes a model of $L$ stacked GNN layers that returns higher dimensional node representations of the input graph and its node features. These are used as input for both the downstream task $\text{GNNC}_{\Phi_1}(\cdot)$ and the explainer network (see Equation \ref{eq:explainer_network}). For models without explicit classification layers the last layer is used instead. It follows
\begin{equation}
    \mathbf{Z} = \text{GNNE}_{\Phi_0}(G_o, \mathbf{X}), \qquad Y = \text{GNNC}_{\Phi_1}(\mathbf{Z}),
\end{equation}
where $\mathbf{Z} \in \mathbb{R}^{n\times D}$ denotes a matrix of $n$ node representations $\mathbf{z}$, referred to as node embeddings. $\mathbf{X} \in \mathbb{R}^{n\times d}$ denotes the $d$-dimensional node features of $G_o$. The general target GNN model is visualized in Figure \ref{fig:blackbox_downstream_model}. Note that $\mathbf{Z}$ encapsulates both features and structure of the input graph and therefore serves as input for the explainer network $g_\Psi$, defined as
\begin{equation}
    \label{eq:explainer_network}
    \Omega = g_\Psi(G_o,\mathbf{Z}).
\end{equation}
$\Psi$ denotes the parameters in the explanation network and the output $\Omega$ is treated as parameter in Equation \ref{eq:monte_carlo}. Since $\Psi$ is shared by all edges among the population, PGExplainer collectively provides explanations for multiple instances. Thus, the learning objective in a collective setting with $\mathcal{I}$ being the set of instances becomes
\begin{equation}
    \label{eq:mlp_loss}
    \min_\Psi -\frac{1}{K}\sum_{i\in \mathcal{I}}\sum_{k=1}^K\sum_{c=1}^C P_\Phi (Y_o = c|G = G_o^{(i)}) \log P_\Phi(\hat{Y}_s = c|G=\hat{G}_s^{(i,k)}).
\end{equation}
Consequently, $G_o^{(i)}$ and $G_s^{(i,k)}$ denote the input graph and the $k$-th graph sampled with Equation \ref{eq:reparam_trick}, respectively, for instance $i$. \bigskip 

\tikzstyle{process} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=purple!30, font=\small]
\tikzstyle{module} = [rectangle, rounded corners, minimum width=3cm, minimum height=1.2cm, text centered, draw=black, fill=gray!30, font=\small]
\tikzstyle{data} = [rectangle, sharp corners, minimum width=1.5cm, minimum height=1cm, text centered, draw=black, fill=cyan!30, font=\small]
\tikzstyle{emb} = [rectangle, sharp corners, minimum width=1.5cm, minimum height=1cm, text centered, draw=black, fill=yellow!30, font=\small]
\tikzstyle{arrow} = [thick,->,>=Stealth]    %very thick


\tikzstyle{module2} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=brown!30, font=\small]
\tikzstyle{module3} = [rectangle, rounded corners, minimum width=2cm, minimum height=1cm, text centered, draw=black, fill=teal!30, font=\small]

\begin{figure}
    \centering
    \begin{tikzpicture}
        \node (input) [data] {\small Input Graph $G_o$ with node features $\mathbf{X}$};

        %\node (gnn) [module, below=of input, minimum height=5cm] {L-Layer GNN};
        \node (GNNE) [module2, below=of input] {GNNE};
        %\node (layer1) [module2, below=of input] {GNN Layer 1};
        %\node (dots) [below=0.3cm of layer1] {$\vdots$};
        %\node (layerL) [module2, below=0.3cm of dots] {GNN Layer L};
        \node (Z) [emb, below=of GNNE] {$\mathbf{Z}$};
        \node (DT) [module3, below=of Z] {GNNC};

        \node (Z_out) [right=of Z] {Node representations};

        \node (Y_o) [data, right=of DT] {\small Prediction $Y_o$};

        
        %\node (z_i) [data, above=2cm of node_emb_j2] {$z_i$};

        %\node (h_2) [data, above=1cm of z_i] {$h_{2,i}$};
        %\node (h_1) [data, left=0.5cm of h_2] {$h_{1,i}$};
        %\node (h_3) [data, right=0.5cm of h_2] {$h_{L,i}$};
    
        %\node (O) [module, above=1cm of h_2] {Downstream Task};
        %\node (F_1) [module, above=0.2cm of O, minimum height=5cm] {L-Layer GNN};
        %\node (H_1) [data, right=0.5cm of F_1] {$H_1 = [h_{1,i}, h_{1,j},...]$};
        %\node (H_2) [data, below=0.5cm of H_1] {$H_2 = [h_{2,i}, h_{2,j},...]$};
        %\node (H_3) [data, below=0.5cm of H_2] {$H_L = [h_{L,i}, h_{L,j},...]$};

        %\node (input) [data, above=1cm of F_1] {$G_o$};

        %\node (concat3) at ($ (h_2 |- z_i) + (0,1)$) {\Large $\oplus$};

        \draw [arrow] (input) -- (GNNE);
        \draw [arrow] (GNNE) -- (Z);
        
        \draw [arrow] (Z) -- (Z_out);
        \draw [arrow] (Z) -- (DT);

        \draw [arrow] (DT) -- (Y_o);

        \begin{pgfonlayer}{background}
            \node[draw=gray, dashed, rounded corners, fit=(GNNE) (DT)] (backgroundevaluation) {};
            \node[
                rotate=90, 
                anchor=center
            ] at ([xshift=-3mm]backgroundevaluation.west) {Target Model};
        \end{pgfonlayer}
        
        %\begin{pgfonlayer}{background}
        %    \node[draw=gray, thick, rounded corners, fit=(layer1) (layerL)] (backgroundevaluation) {};
        %    \node[
        %        rotate=90, 
        %        anchor=center
        %    ] at ([xshift=-3mm]backgroundevaluation.west) {GNNE};
        %\end{pgfonlayer}
    \end{tikzpicture}
    \caption[Black-box target model in PGExplainer context]{A black-box target model in the context of PGExplainer.}
    \label{fig:blackbox_downstream_model}
\end{figure}

The authors propose two slightly different instantiations of $\Omega$ for node classification and graph classification tasks. \bigskip

\textbf{Explanation Network for Graph Classification}\par
For graph level tasks, the authors consider each graph to be an instance, independent  of specific nodes. The output $\Omega$ of the network (see Equation \ref{eq:explainer_network}) is thus specified as:
\begin{equation}
    \label{eq:mlp_graph_input}
    \omega_{ij} = \text{MLP}_\Psi ([\mathbf{z}_i\oplus\mathbf{z}_j]),
\end{equation}
where $\text{MLP}_\Psi$ is an MLP (see Section \ref{sec:Replication_of_PGExplainer} for implementation details) parameterized with $\Psi$ and $\oplus$ denotes the concatenation operation. Effectively, for each edge in $G_o$ a concatenation of both its nodes' embeddings is fed through the MLP. For the MLP used in both the original and this work the output $\omega_{ij}$ is an edge logit, which serves as a parameter in the sampling process. \bigskip

\textbf{Explanation Network for Node Classification}\par
%Since explanations for different nodes may not share a common explanation pattern, especially for nodes with different labels, ... DOES THIS NOT ALSO APPLY FOR GRAPHS?!
For node level tasks on the other hand, each prediction node is considered as an instance. Let an edge $(i,j)$ be considered relevant for the prediction of a node $u$, but irrelevant for the prediction of a node $v$. To explain the prediction of node $v$ the authors specify the output $\Omega$ of the network in Equation \ref{eq:explainer_network} as:
\begin{equation}
    \omega_{ij} = \text{MLP}_\Psi ([\mathbf{z}_i\oplus\mathbf{z}_j\oplus\mathbf{z}_v]).
\end{equation}
Thus, a concatenation of the node embeddings of nodes $i, j$ and $v$ is fed through the network. We denote this preprocessing step of creating edge embeddings from node embeddings as edge embedding transformation. The full pipeline of the PGExplainer can be observed in figure \ref{fig:PGExplainer_pipeline}. \bigskip


It is important to note, that for $L$-layer target GNNs utilizing a message passing mechanism, the prediction at node $v$ is fully determined by its local computation graph \cite{ying2019gnnexplainer}. The local computation graph of $v$ is defined as its $L-$hop neighborhood $\mathcal{N}_L(v)$ (see Figure \ref{fig:k-hop}).\bigskip

\textbf{Collective and Inductive Setting}\par
Due to the nature of its predecessor GNNExplainer~\cite{ying2019gnnexplainer}, the main focus of the PGExplainer~\cite{luo2020parameterized} was the application in a collective setting, where the goal is to explain a full population of instances by training on all these and thus being able to provide explanations for every single one. However, since the PGExplainer utilizes a deep neural network to parameterize the generation process of explanations for a population, it can be utilized in an inductive setting, unlike its predecessor. 

This means that explanations can be generated for a set of instances from the same population, that have not been seen during training. Thus, it is not necessary to retrain the explainer for new instances of the same population, effectively reducing the computational complexity by the training time when compared to the GNNExplainer. Moreover, the number of parameters in the explainer does no longer depend on the size of the input graph

%"PGExplainer learns a latent variable for each edge in the original input graph with a neural network parameterized by Ψ, which is shared by all edges in the population of input graphs." "In GNNExplainer, the parameter size is linear to the number of edges"

\begin{figure}
\centering
\begin{tikzpicture}[node distance=0.9cm and 1.2cm]

\tikzset{
  mymini/.pic={
    \node[circle, draw, fill=black, inner sep=2pt, label=left:$\mathbf{x}_i$] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt, label=above:$\mathbf{x}_j$] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt, label=left:$\mathbf{x}_k$] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt, label=left:$\mathbf{x}_l$] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (b) -- (x);
    \draw[-] (a) -- (x);
  }
}

\tikzset{
  mymini3/.pic={
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$\mathbf{z}_i$] (x) at (0,0) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=above:$\mathbf{z}_j$] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$\mathbf{z}_k$] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$\mathbf{z}_l$] (b) at (0,-0.5) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (b) -- (x);
    \draw[-] (a) -- (x);
  }
}

\tikzset{
  mymini2/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- node[midway, right, font=\scriptsize] {$0.9$} (y);
    \draw[-] (a) -- node[midway, above, font=\scriptsize] {$0.8$}(y);
    \draw[-] (b) -- node[midway, left, font=\scriptsize] {$0.1$}(x);
    \draw[-] (a) -- node[midway, left, font=\scriptsize] {$0.9$}(x);
  }
}

\tikzset{
  mymini4/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- node[midway, right, font=\scriptsize] {$\mathbf{z}_{i,j}$} (y);
    \draw[-] (a) -- node[midway, above, font=\scriptsize] {$\mathbf{z}_{j,k}$}(y);
    \draw[-] (b) -- node[midway, left, font=\scriptsize] {$\mathbf{z}_{i,l}$}(x);
    \draw[-] (a) -- node[midway, left, font=\scriptsize] {$\mathbf{z}_{i,k}$}(x);
  }
}

\tikzset{
  mymini5/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (a) -- (x);
  }
}


% Nodes
\node (input) [data] {\small Input Graph $G_o$ with node features $\mathbf{X}$};
\pic at ([xshift=7cm]input.center) {mymini};
\node (target) [module, below=of input] {Target Model};
\node (embeddings) [emb, below=of target] {Node Embeddings $\mathbf{Z}$ of $G_o$};
\node[anchor=center] at ([xshift=4cm]embeddings.center) 
 (node_embs) 
 {$\begin{bmatrix} \mathbf{z}_i \\ \mathbf{z}_j \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of node_embs, anchor=west] 
 {\scriptsize $\in \mathbb{R}^{|V(G_o)|}$};
\pic at ([xshift=7cm]embeddings.center) {mymini3};
\node (embedding_transformation) [process, below=of embeddings] {Edge Embedding Transformation};
\node (edge_embeddings) [emb, below=of embedding_transformation] {Edge Embeddings of $G_o$};
\node[anchor=center] at ([xshift=4cm]edge_embeddings.center) 
 (edge_embs) 
 {$\begin{bmatrix} \mathbf{z}_{i,j} \\ \mathbf{z}_{j,k} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of edge_embs, anchor=west] 
 {\scriptsize $\in \mathbb{R}^{|E(G_o)|}$};
\pic at ([xshift=7cm]edge_embeddings.center) {mymini4};
\node (explainer) [module, below=of edge_embeddings] {PGExplainer MLP};
\node (logits) [emb, below=of explainer] {Edge Logits/Latent variables $\Omega$};
\node[anchor=center] at ([xshift=4cm]logits.center) 
 (omega) 
 {$\begin{bmatrix} \omega_{i,j} \\ \omega_{j,k} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of omega, anchor=west] 
 {\scriptsize $\in \mathbb{R}^{|E(G_o)|}$};
\node (trick) [process, below=of logits] {Reparameterization Trick};
\node (weights) [emb, below=of trick] {Sampled edge importance weights};
\node[anchor=center] at ([xshift=4cm]weights.center) 
 (edge_score) 
 {$\begin{bmatrix} \hat{e}_{i,j} \\ \hat{e}_{j,k} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of edge_score, anchor=west] 
 {\scriptsize $\in (0,1)^{|E(G_o)|}$};

\node (sampled_graph) [data, below= of weights] {Sampled graph $\hat{G}_s$};
\pic at ([xshift=7cm]sampled_graph.center) {mymini2};

\node (sample_target) [module, below=of sampled_graph] {Target Model};
\node (original_target) [module, left=2cm of sample_target] {Target Mode};
\node (sample_prediction) [data, below=of sample_target] {$\hat{Y}_s$};
\pic at ([xshift=7cm]sample_prediction.center) {mymini5};
\node (original_prediction) [data, below=of original_target] {$Y_o$};

\node (topK) [process, right=of sample_target] {Sample top-$k$ edges};
\node (explanation) [data, below=of topK] {$G_s$};

%\node[draw=red, thick, dashed, fit=(input) (target) (embeddings), label=above:input Block] {};

%\begin{pgfonlayer}{background}
%    \node[draw=gray, thick, rounded corners, fit=(edge_embeddings) (weights), fill=blue!10, label=above:Sampling of PGExplainer] {};
%\end{pgfonlayer}

\begin{pgfonlayer}{background}
    \node[draw=gray, thick, rounded corners, fit=(original_target) (sample_target) (original_prediction) (sample_prediction), fill=orange!30] (backgroundtraining) {};
    \node[
        rotate=90, 
        anchor=center
    ] at ([xshift=-3mm]backgroundtraining.west) {Training};
\end{pgfonlayer}

\begin{pgfonlayer}{background}
    \node[draw=gray, thick, rounded corners, fit=(topK) (explanation), fill=YellowGreen!80] (backgroundevaluation) {};
    \node[
        rotate=90, 
        anchor=center
    ] at ([xshift=-3mm]backgroundevaluation.west) {Evaluation};
\end{pgfonlayer}

\coordinate (weight_sample_mid) at ($ (weights)!0.5!(sampled_graph) $);
\coordinate (left_of_input) at ($ (input) + (-3.5cm, 0) $);
\coordinate (right_of_input) at ($ (original_target |- input) $);
\coordinate (drop) at (left_of_input |- weight_sample_mid);
\coordinate (right_of_sampled_graph) at ($ (topK |- sampled_graph) $);

% Arrows
\draw [arrow] (input) -- (target);
\draw [arrow] (target) -- (embeddings);
\draw [arrow] (embeddings) -- (embedding_transformation);
\draw [arrow] (embedding_transformation) -- (edge_embeddings);
\draw [arrow] (edge_embeddings) -- (explainer);
\draw [arrow] (explainer) -- (logits);
\draw [arrow] (logits) -- (trick);
\draw [arrow] (trick) -- (weights);

%\draw [arrow] (input.west) --  ++(-2cm,0) -- ($ (input) + (-2cm,0) $ |- sampled_graph) -- (sampled graph.west);
\draw [arrow] (weights) -- (sampled_graph);

\draw[arrow] 
  (input) -- (left_of_input) 
       -- (drop) 
       -- (weight_sample_mid);

\draw[arrow] 
  (left_of_input) -- (right_of_input) 
       -- (original_target);

\draw [arrow] (sampled_graph) -- (sample_target);
\draw [arrow] (sample_target) -- (sample_prediction);

\draw [arrow] (original_target) -- (original_prediction);

\draw [arrow] (sampled_graph) -- (right_of_sampled_graph) -- (topK);

\draw [arrow] (topK) -- (explanation);

\draw [<->, dashed] (sample_prediction) -- node[midway, above, font=\scriptsize] {$\min_\Omega H(Y_o,\hat{Y}_s)$} (original_prediction);

\end{tikzpicture}
\caption[Pipeline of PGExplainer]{The complete pipeline of PGExplainer.}
\label{fig:PGExplainer_pipeline}
\end{figure}

\subsection{Regularization Terms}
\label{sec:Regularization_Terms}
To enhance the preservation of desired properties of explanations the authors propose various regularization terms. These are added to the learning objective, depending on the specific downstream task at hand.\bigskip

\textbf{Size and Entropy Constraints}\par
Inspired by GNNExplainer \cite{ying2019gnnexplainer}, to obtain compact and precise explanations, a constraint on the size of the explanations is added in the form of $||\Omega||_1$, the $l_1$ norm on latent variables $\Omega$. Additionally, to encourage the discreteness of edge weights, element-wise entropy is added as a constraint:
\begin{equation}
    H_{\hat{G}_s} = -\frac{1}{|E(\hat{G}_s)|}\sum_{(i,j)\in E(\hat{G}_s)} (\hat{e}_{i,j}\log \hat{e}_{i,j} + (1-\hat{e}_{i,j})\log(1-\hat{e}_{i,j})),
\end{equation}
for each explanatory graph $\hat{G_s}$. \bigskip

Note that the following two constraints are not used in the original experimental setup, but serve as inspiration for the constraints we introduce in our NeuroSAT application (see Section \ref{sec:NeuroSAT_extension}) and are therefore included. \bigskip

\textbf{Budget Constraint}\par
The authors propose the modification of the size constraint to a budget constraint, for a predefined available budget $B$. Let $|E(\hat{G}_s)| \leq B$, then the budget regularization is defined as
\begin{equation}
    R_b = \text{ReLU}(\sum_{(i,j)\in E(\hat{G}_s)}\hat{e}_{i,j}-B).
\end{equation}
Note that $R_b = 0$ when the explanatory graph is smaller than the budget. When out of budget, the regularization is similar to that of the size constraint. \bigskip

\textbf{Connectivity Constraint}\par
To enhance the effect of the explainer detecting a small, connected subgraph, motivated through real-life motifs being inherently connected, the authors suggest adding the cross-entropy of adjacent edges. Let $(i,j)$ and $(i,k)$ be two edges that both connect to the node $i$, then $(i,k)$ should rather be included in the explanatory graph if the edge $(i,j)$ is selected to be included. This is formally defined by Luo et al. \cite{luo2020parameterized} as
\begin{equation}
    H(\hat{e}_{i,j},\hat{e}_{i,k}) = -[1-\hat{e}_{i,j}\log(1-\hat{e}_{i,k})+\hat{e}_{i,j}\log \hat{e}_{i,k}].
\end{equation}
We note that in practice this is implemented only for the two highest edge weights for each edge. The definition therefore would change to $(i,j)$ and $(i,k)$ being the edges carrying the top two edge weights from all nodes connecting to node $i$.


\section{Extension to Application on NeuroSAT}
\label{sec:NeuroSAT_extension}
In this section we propose additional restraints to fit the explanations of PGExplainer to the structure of SAT formulae. We start by giving a short introduction to NeuroSAT\cite{selsam2018learning} and how it may function as a \ac{TM}.\bigskip

Proposed by Selsam et al. \cite{selsam2018learning}, NeuroSAT utilizes a MPNN, generally utilized to express GNNs, to solve SAT formulae. It is able to generalize in the sense that it may solve substantially larger and more difficult formulae than seen during training by running for more iterations. Though it may be used to calculate variable assignments that satisfy a formula, it is unable to provide proofs for predictions of formulae that are unsatisfiable. \bigskip

In a separate experiment the authors were able to make NeuroSAT identify specific unsatisfiable cores. This is assumed to be due to the network memorizing the subgraphs rather than learning a generic procedure that proves unsatisfiability. To support and validate the predictions of unsatisfiability, we aim to generate global, generalized explanations for unsatisfiable formulae using the PGExplainer, trained on predictions of a trained NeuroSAT model.

We test the quality and "human-understandability" of an explanation by comparing it to a MUS of the input problem, since the prediction of an unsatisfiable SAT instance can in theory be reduced to the prediction of said MUS. Furthermore, the fundamental idea of perturbing edges, or sets of edges, to learn explanations is analogous to the process of deleting clauses from a problem until a minimal unsatisfiable core is attained, as done in deletion-based MUS extractors (see Marques-Silva \cite{5489199}). Thus, we test whether PGExplainer is able to generate explanations for NeuroSAT that align with MUSes. \bigskip

%Let $n$ denote the number of variables in a SAT formula and $m$ the number of clauses.
Since the following restraints are tailored to the application on bipartite graphs representing SAT formulae, we have to define the specifics of the input LCG in this context. A formula is encoded as an undirected bipartite graph $G_b$, with bipartition $(L,C)$. It contains one node for every literal $l \in L$, one for every clause $c \in C$, and an edge for every pair $(l,c)$ where $l$ appears in clause $c$. 

Additionally, in the context of NeuroSAT connections exist between each literal and its negation, since messages are also passed along these. Note that these edges are not present in the biadjacency matrix $\mathbf{B}\in \{0,1\}^{L\times C}$. $\mathbf{B}$ is used as input for the NeuroSAT model, without explicit node features. In the following definitions we let $G_b$ be the original input graph for the PGExplainer, completely defined by its biadjacency matrix $\mathbf{B}$. \bigskip

%aggregation: sum the outgoing messages of each of a node’s neighbors to form the incoming message

Since PGExplainer generates edge wise explanations, and we want to explain the SAT instance predictions with unsatisfiable cores as \ac{GT}, we need to adapt the framework to account for the definition of unsatisfiable cores. Since a core is a subset of clauses, predicting singular edges that represent literals being present in a clause, may not provide sufficient results in the sense of human understandable explanations. Therefore, we propose a soft and a hard restraint that encourage the explainer to predict "sets" of edges that connect to the same node $c$, approximating the prediction of a complete clause.

The remainder of the explainer pipeline stays identical. The target model - NeuroSAT - calculates hidden node representations $\mathbf{h}^t$ for the input graph $G_b$, now modeling a SAT instance, at each iteration $t$. The representations in the last iteration $T$ are extracted as node embeddings $\mathbf{z}$ for clause and literal nodes respectively, and transformed into edge embeddings (see Equation \ref{eq:mlp_graph_input}). Though this is only done for node level tasks in the original, we also consider using a concatenation of multiple hidden representations $\mathbf{h}^{\frac{1}{2}T} \oplus \mathbf{h}^{\frac{3}{4}T} \oplus \mathbf{h}^T$ as node embeddings $\mathbf{z}$. These serve as the input of the explainer MLP and are processed as usual, with either of the following additional limitations. \bigskip

\newpage
\textbf{Soft Modified Connectivity Constraint}\par
To account for the definition of unsatisfiable cores - a subset of clauses in the original formula whose conjunction is still unsatisfiable - we add a constraint that reinforces the prediction of complete clauses. Therefore, if the explainer assigns a high score to an edge $(l_1,c)$, all edges $(l_k,c) \in E(c)$ that also connect to the clause node $c$ should receive a high score. Therefore, we introduce a soft constraint that punishes varying edge weights for the same clause. For our sampled bipartite Graph $\hat{G}_s$ with node sets $L$ and $C$ containing literal nodes and clause nodes respectively, we define
\begin{equation}
    \label{eq:soft_restraint}
    R_C = \sum_{c \in C}  \text{Var}(\hat{E}_c) = \sum_{c \in C} \frac{1}{|E(c)|} \sum_{(l,c) \in E(c)} (\hat{e}_{l,c} - \bar{E_c})^2,
\end{equation}
where
\begin{equation}
    \hat{E}_c = \{\hat{e}_{l,c} \mid (l,c)\in E(\hat{G}_s)\}
\end{equation}
denotes the set of edge weights corresponding to edges incident to $c$ and 
\begin{equation}
    \bar{E_c} = \frac{1}{|\hat{E}_c|}\sum_{\hat{e}_{l,c} \in \hat{E}_c} \hat{e}_{l,c}
\end{equation}
denotes the mean of $\hat{E}_c$. This is added to our objective function during training. \bigskip


\textbf{Hard Constraint}\par
Since the soft constraint only encourages the prediction of entire clauses but does not enforce it, we also propose a hard constraint that modifies the prediction process. We restrain the edge logits $\omega_{i,j}$ calculated by the MLP to be identical for all edges that connect to the same clause. %Let $\Omega_c = \{\hat{\omega}_{i,c} \mid (i,c)\in E\}$ denote the set of logits corresponding to the edges connected to node $c$. We update these logits with:
%\begin{equation}
%    \mu_c = \frac{1}{|\Omega_c|} \sum_{\hat{\omega}_{i,c} \in \Omega_c} \hat{\omega}_{i,c}
%\end{equation}
For all clause nodes $c \in C$, we calculate the mean logit $\mu_c$ of all edges incident to $c$ with
\begin{equation}
    \mu_c = \frac{1}{|E(c)|} \sum_{(l,c)\in E(c)}\omega_{l,c}.
\end{equation}
The update rule is then defined as 
%\begin{equation}
%    \omega_{l,c}' =\mu_c, \qquad \forall(l,c) \in E(c) \mid 
%\end{equation}
\begin{equation}
    \omega_{l,c}'\leftarrow \mu_c,
\end{equation}
since edges in the biadjacency matrix are from literals to clauses at all times.


The reparameterization trick is still applied for each edge, but $\epsilon_c$ is sampled per clause instead of per edge, so that all edges that connect to a clause are forced to not only bear the same logit, but also the same importance score. The Equation \ref{eq:reparam_trick} thus changes to:
\begin{equation}
    \epsilon_c \sim \text{Uniform}(0,1), \qquad \hat{e}_{l,c}=\sigma((\log \epsilon_c - \log(1-\epsilon_c)+\omega_{l,c}/\tau).
\end{equation}


\section{Implementation Details}
\label{sec:implementation_details}
In the following, we provide the implementation details needed to reproduce our results. This includes the general replication of the PGExplainer \cite{luo2020parameterized} with the adapted \acp{TM} in Section \ref{sec:Replication_of_PGExplainer} and the specifics for the application on NeuroSAT \cite{selsam2018learning} in Section \ref{sec:Application_to_NeuroSAT}.


\subsection{Replication of PGExplainer}
\label{sec:Replication_of_PGExplainer}

For our replication we try to implement the methods and details as close to the original paper as possible. Thus, we follow the general pseudocode algorithms presented by the authors (see Appendix \ref{sec:PGE_material}). Since the paper differs from the original codebase and is imprecise about certain descriptions, as found by Holdijk et al. \cite{holdijk2021re}, we aim to give a thorough description. This includes the tools we used, resulting changes regarding the data processing, the general architecture and hyperparameters of the \acp{TM}, the architecture and hyperparameters of the explainer model, as well as concrete methods implemented in the model. \bigskip

\textbf{Libraries}\par
 To reimplement the framework, we utilize a couple of libraries that we introduce shortly. Most notably, we use PyTorch Geometric \cite{Fey/Lenssen/2019}, a library built upon PyTorch \cite{paszke2019pytorch}, that provides methods to create and train GNNs. For evaluation of the explainer model, specifically for calculating the AUROC score, we use TorchEval, a model evaluation library that is part of PyTorch. Furthermore, we integrate WandB \cite{wandb} to monitor model performance and allow for easy hyperparameter searches. To visualize the graphs and their explanations we employ NetworkX \cite{SciPyProceedings_11}. Lastly, we utilize seaborn \cite{Waskom2021} to plot loss curves and other metrics.\bigskip

\textbf{Preprocessing}\par
We use the original datasets that are provided in the PGExplainer codebase\footnote{\url{https://github.com/flyingdoog/PGExplainer}}. We transform the data to fit our PyTorch Geometric framework. Each graph is stored as a torch-geometric Data object. This holds the $d$-dimensional node features as a tensor, the graph-level label index or alternatively all node-level labels as a tensor of class indices, a \ac{GT} edge mask that contains the edges belonging to the motif, as well as boolean node masks for training, evaluating and testing the \ac{TM}. 
 
In PyTorch Geometric edges are stored in the edge-index format as a COO tensor - a PyTorch coordinate format that stores tuples of element indices and their corresponding values. In the context of graph edges in PyTorch Geometric, for an edge $(i,j)$ the element index is the starting node $i$ and the corresponding value its incident node $j$. This is computed from the adjacency matrix $A$ as \lstinline|A.nonzero().t().contiguous()|.
First, the matrix indices or coordinates of the edges - non-zero elements - are extracted. These are then transposed and lastly stored in contiguous memory. The resulting shape of the edge-index is $[2, E]$, where E denotes the number of edges in the graph. Therefore, we only transform the data without changing its content. \bigskip
%$\mathbb{N}^{2\times \text{\#edges}}$
\newpage
\textbf{Reproducibility}\par
 Inspired by Holdijk et al. \cite{holdijk2021re} we implement the ability to seed the experiments performed on PGExplainer. PyTorch Geometric provides a way of seeding all modules that generate random numbers during the training process, including torch and python random.
To further increase reproducibility, we utilize PyTorch's \lstinline|use_deterministic_algorithms|, forcing the learning algorithm to only use deterministic algorithms. For the dataset splits we use a separate fixed seed that consistently creates the same instance sets across all explainer training runs and experiments.\bigskip

\textbf{Target Model Specifications}\par
 In PGExplainer two slightly different architectures of GNNs for node classification and graph classification are introduced. We recreate these in PyTorch Geometric, while changing the exact layers used in the network to test whether the claim that the explainer does apply to any target GNN model holds. These models implement the same downstream classification tasks on the given datasets that achieve accuracies of at least $85\%$, a baseline set in GNNExplainer \cite{ying2019gnnexplainer}. The datasets as well as the exact accuracies of each of the models are presented in the experimental setup (see Section \ref{sec:experiments_replication}). 

Our model for both node- and graph classification consist of $3$ \lstinline|GraphConv| layers, a PyTorch Geometric implementation of the stacked GNN by Morris et al. \cite{morris2019weisfeiler} (see Equation \ref{eq:higher-order-gnn}). Since this layer allows for the passing of edge weights, weights of one are passed by default to simply maintain the discrete adjacency matrix, e.g., during training of the \ac{TM}. Each of these layers has $20$ hidden units and is followed by a ReLU activation function. The first layer processes the $d$-dimensional input node features, while the remaining layers retain the hidden dimensionality of $20$.

Holdijk et al. \cite{holdijk2021re} found that the original code wrongfully uses undocumented batch normalization layers in training mode during evaluation, which leads to a deviation in results, and thus completely omit the use of batch normalization. We choose to add batch normalization between the first two layers and their activation functions for both models, similar to the original codebase, without the training mode error. Additionally, we add an optional dropout layer after each activation function to improve the generalizability on more difficult tasks. \bigskip


Note that in their codebase the authors use a concatenation of the hidden representations at each layer instead of solely the final layer representation as node embeddings for node level tasks.
%For a target GNN consisting of $L$ graph layers with TODO: CONCRETE DEFINITION?!
%\begin{align*}
%    \mathbf{H}^{(1)} &= F_1(G_o, \mathbf{X}), \\
%    \mathbf{H}^{(2)} &= F_2(G_o, \mathbf{H}^{(1)}), \\
%    &\vdots \\
%    \mathbf{H}^{(L)} &= F_L(G_o, \mathbf{H}^{(L-1)},), \\
%\end{align*}
This leads to $\mathbf{Z} \in \mathbb{R}^{n\times (LD)}$ being the matrix of node embeddings $\mathbf{z}$ that are computed as:
\begin{equation}
    \mathbf{z}_i = \mathbf{h}^{(1)}_i \oplus \mathbf{h}^{(2)}_i \oplus \mathbf{h}^{(3)}_i,
\end{equation}
with $\mathbf{h}^{(l)}_i \in  \mathbb{R}^D$ denoting the hidden representations of node $i$ in layer $l$ for the specified $L=3$ layer network with constant embedding dim $D=20$.\bigskip


Thus, the node embeddings used in the explainer as well as in the downstream classification task each have a shape of $\mathbb{R}^{3(20)}$. For the classification a final linear layer is added to the model that maps each $60$-dimensional node embedding to $C$ classes. A softmax is applied to the model output to get class probabilities for each node. We also adopt this in our implementation.

The softmax function is defined as follows \cite{Goodfellow-et-al-2016}:
Given a vector \( \mathbf{z} = [z_1, z_2, \dots, z_K] \in \mathbb{R}^K \), the softmax function maps \( \mathbf{z} \) to a probability distribution over \( K \) classes:

\begin{equation}
    \text{softmax}(\mathbf{z})_i = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}} \quad \text{for } i = 1, 2, \dots, K,
\end{equation}
where $e$ denotes the exponential function. \bigskip

The model used for graph tasks differs slightly, in the sense that only the hidden embeddings of the last \lstinline|GraphConv| layer are treated as node embeddings. In addition, before the final linear layer used for classification, both a max pooling and mean pooling operation are performed on the embeddings of each graph and the results concatenated to get a representation of a complete graph. Note that the paper only states to use max pooling, while in practice a concatenation of the max pooling and sum pooling is used. We adopt the combination of mean pooling and max pooling from the replication study \cite{holdijk2021re}, as this has been used in recent graph neural networks \cite{ma2021unsupervised}, \cite{simonovsky2017dynamic}, \cite{zhao2023faithful}. 

The max pooling operation extracts the maximum value of each feature dimension across the nodes of a graph, while the mean pooling extracts the mean value of each feature dimension across all nodes of a graph. Since the results of both pooling operations are in $\mathbb{R}^{20}$, the resulting $40$-dimensional graph embedding is fed into the linear layer, again mapping to $C$ classes, and a softmax is applied to get probabilities of a graph belonging to each class. Figure \ref{fig:gnn-models} visualizes both described models. \bigskip

\tikzstyle{function} = [rectangle, sharp corners, minimum width=2cm, minimum height=0.5cm, text centered, draw=black, fill=brown!30, font=\small]

\begin{figure}[htbp]
    \centering
    % Subfigure 1: Node Classification
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \node (input) [data] {$G_o$};

            %\node (gnn) [module, below=of input, minimum height=5cm] {L-Layer GNN};
            \node (layer1) [module2, below=of input] {GNN Layer 1};
            \node (layer2) [module2, below=of layer1] {GNN Layer 2};
            \node (layer3) [module2, below=of layer2] {GNN Layer 3};
            \node (concat) [process, below=of layer3] {concat};
            \coordinate (right_of_concat) at ($ (concat) + (3.5cm, 0) $);
            \node (Z) [emb, below=1cm of concat] {$\mathbf{Z}$};
            \node (DT) [module3, below=0.5cm of Z] {Linear Layer};
            \node (softmax) [process, below=of DT] {Softmax};

            \node (Z_out) [left=1.5cm of Z] {};

            \node (Y_o) [data, below=0.5cm of softmax] {$Y_o$};

            
            %\node (z_i) [data, above=2cm of node_emb_j2] {$z_i$};

            %\node (h_2) [data, above=1cm of z_i] {$h_{2,i}$};
            %\node (h_1) [data, left=0.5cm of h_2] {$h_{1,i}$};
            %\node (h_3) [data, right=0.5cm of h_2] {$h_{L,i}$};
        
            %\node (O) [module, above=1cm of h_2] {Downstream Task};
            %\node (F_1) [module, above=0.2cm of O, minimum height=5cm] {L-Layer GNN};
            \node (H_1) [emb, right=0.5cm of layer1] {$\mathbf{H}^{(1)}$};
            \coordinate (right_of_layer1) at ($ (layer1) + (3.5cm, 0) $);
            \node (H_2) [emb, right=0.5cm of layer2] {$\mathbf{H}^{(2)}$};
            \coordinate (right_of_layer2) at ($ (layer2) + (3.5cm, 0) $);
            \node (H_3) [emb, right=0.5cm of layer3] {$\mathbf{H}^{(3)}$};
            \coordinate (right_of_layer3) at ($ (layer3) + (3.5cm, 0) $);

            \node (features) [data, above=of H_1] {\small $\mathbf{X}$};

            %\node (input) [data, above=1cm of F_1] {$G_o$};

            %\node (concat3) at ($ (h_2 |- z_i) + (0,1)$) {\Large $\oplus$};

            
            \draw [arrow] (features) -- (layer1);
            \draw [arrow] (layer1) -- (H_1);
            \draw [arrow] (H_1) -- (layer2);
            \draw [arrow] (layer2) -- (H_2);
            \draw [arrow] (H_2) -- (layer3);
            \draw [arrow] (layer3) -- (H_3);
        
            \draw [arrow] (H_1) -- (right_of_layer1) -- (right_of_concat) -- (concat);
            \draw [-] (H_2) -- (right_of_layer2);
            \draw [-] (H_3) -- (right_of_layer3);

            \draw [arrow] (concat) -- (Z);
            \draw [arrow] (Z) -- (Z_out);
            \draw [arrow] (Z) -- (DT);

            \draw [arrow] (DT) -- (softmax);
            \draw [arrow] (softmax) -- (Y_o);

            \begin{pgfonlayer}{background}
                \draw [arrow] (input) -- (layer3);
                \node[draw=gray, dashed, rounded corners, inner sep=15pt,  fit=(GNNE) (H_1) (DT)] (backgroundevaluation) {};
                \node[
                    rotate=90, 
                    anchor=center
                ] at ([xshift=-3mm]backgroundevaluation.west) {Target Model Node Classification};
            \end{pgfonlayer}
            
            %\begin{pgfonlayer}{background}
            %    \node[draw=gray, thick, rounded corners, fit=(layer1) (layerL)] (backgroundevaluation) {};
            %    \node[
            %        rotate=90, 
            %        anchor=center
            %    ] at ([xshift=-3mm]backgroundevaluation.west) {GNNE};
            %\end{pgfonlayer}
        \end{tikzpicture}
        \caption{Node classification model}
        \label{fig:node-classification}
    \end{subfigure}
    \hfill
    \begin{minipage}[t]{0.01\textwidth}
    \rule{0.4pt}{16.1cm} % Manually adjust this height
    \end{minipage}
    \hfill
    % Subfigure 2: Graph Classification
    \begin{subfigure}[t]{0.48\textwidth}
        \centering
        \begin{tikzpicture}
            \node (input) [data] {$G_o$};

            %\node (gnn) [module, below=of input, minimum height=5cm] {L-Layer GNN};
            \node (layer1) [module2, below=of input] {GNN Layer 1};
            \node (layer2) [module2, below=of layer1] {GNN Layer 2};
            \node (layer3) [module2, below=of layer2] {GNN Layer 3};
            \node (Z) [emb, below=of layer3] {$\mathbf{Z}$};
            \node at ($(Z.south)+(0,-0.97)$) (belowZ) {};
            \node (meanP) [process, left=0.3cm of belowZ] {MeanPool};
            \node (maxP) [process, right=0.3cm of belowZ] {MaxPool};
            \node (concat) [below=0.5cm of belowZ] {\Large $\oplus$};
            \coordinate (right_of_concat) at ($ (maxP |- concat) $);
            \coordinate (left_of_concat) at ($ (meanP |- concat) $);
            \node (DT) [module3, below=0.3cm of concat] {Linear Layer};
            \node (softmax) [process, below=of DT] {Softmax};

            \node (Z_out) [right=3.5cm of Z] {};

            \node (Y_o) [data, below=0.5cm of softmax] {$Y_o$};

            
            %\node (z_i) [data, above=2cm of node_emb_j2] {$z_i$};

            %\node (h_2) [data, above=1cm of z_i] {$h_{2,i}$};
            %\node (h_1) [data, left=0.5cm of h_2] {$h_{1,i}$};
            %\node (h_3) [data, right=0.5cm of h_2] {$h_{L,i}$};
        
            %\node (O) [module, above=1cm of h_2] {Downstream Task};
            %\node (F_1) [module, above=0.2cm of O, minimum height=5cm] {L-Layer GNN};
            \node (H_1) [emb, right=0.5cm of layer1] {$\mathbf{H}^{(1)}$};
            \node (H_2) [emb, right=0.5cm of layer2] {$\mathbf{H}^{(2)}$};
            \node (H_3) [emb, right=0.5cm of layer3] {$\mathbf{H}^{(3)}$};

            \node (features) [data, above=of H_1] {\small $\mathbf{X}$};

            %\node (input) [data, above=1cm of F_1] {$G_o$};

            %\node (concat3) at ($ (h_2 |- z_i) + (0,1)$) {\Large $\oplus$};

            \draw [arrow] (features) -- (layer1);
            \draw [arrow] (layer1) -- (H_1);
            \draw [arrow] (H_1) -- (layer2);
            \draw [arrow] (layer2) -- (H_2);
            \draw [arrow] (H_2) -- (layer3);
            \draw [arrow] (layer3) -- (H_3);
            \draw [arrow] (H_3) -- (Z);

            \draw [arrow] (Z) -- (Z_out);
            \draw [arrow] (Z) -- (maxP);
            \draw [arrow] (Z) -- (meanP);

            \draw [arrow] (meanP) -- (left_of_concat) -- (concat);
            \draw [arrow] (maxP) -- (right_of_concat) -- (concat);
            

            \draw [arrow] (concat) -- (DT);
            \draw [arrow] (DT) -- (softmax);
            \draw [arrow] (softmax) -- (Y_o);

            \begin{pgfonlayer}{background}
                \draw [arrow] (input) -- (layer3);
                \node[draw=gray, dashed, rounded corners, inner sep=10pt, fit=(GNNE) (H_1) (meanP) (DT)] (backgroundevaluation) {};
                \node[
                    rotate=90, 
                    anchor=center
                ] at ([xshift=-3mm]backgroundevaluation.west) {Target Model Graph Classification};
            \end{pgfonlayer}
            
            %\begin{pgfonlayer}{background}
            %    \node[draw=gray, thick, rounded corners, fit=(layer1) (layerL)] (backgroundevaluation) {};
            %    \node[
            %        rotate=90, 
            %        anchor=center
            %    ] at ([xshift=-3mm]backgroundevaluation.west) {GNNE};
            %\end{pgfonlayer}
        \end{tikzpicture}
        \caption{Graph classification model}
        \label{fig:graph-classification}
    \end{subfigure}
    \caption[Visualization of used target models]{Visualizations of the two target GNN models used for node classification (a) and graph classification (b). "concat" denotes the row-wise concatenation along the feature dimension.}
    \label{fig:gnn-models}
\end{figure}

Furthermore, following the original paper, all layer weights are initialized with Xavier initialization \cite{glorot2010understanding}, while biases are initialized with $0$. The datasets are split into training, testing and validation sets with an 80/10/10 split ratio. The models are trained for 1000 epochs, with a learning rate of $1.0 \times 10^{-3}$ and Adam \cite{kingma2014adam} is used as an optimizer. The loss is defined as the PyTorch \lstinline|CrossEntropyLoss()| between the GNN prediction and the actual label of the input. \bigskip

Note that the main differences between our \ac{TM} and the model described in PGExplainer \cite{luo2020parameterized} lie in the used graph layer, as well as the addition of dropout, two batch normalizations before the activations, rather than after activations, and the usage of a slightly different global pooling.

We want to stress that though not documented in the paper, early stopping is utilized in the \ac{TM} training and the model state with the highest validation accuracy is selected. For models that achieve the highest accuracy at multiple epochs, the model state with the lowest validation loss is chosen. If the validation loss does not decrease below the minimum for 500 epochs, the training is stopped early. This is very important for a fair comparison of the models, as we found that the explanations of an overfit \ac{TM} differ significantly from those of the best performing one.\bigskip

In a more recent version of the PGExplainer \cite{10423141}, the authors add a formal description of the \ac{TM} layer used in their framework. The used GNN layer is defined as:
\begin{equation}
    f(\mathbf{H}^{(l)},\mathbf{A})=\sigma(\mathbf{W}^{(l)}\mathbf{A}\mathbf{H}^{(l)}),
\end{equation}
where $A$ is the normalized Laplacian matrix.

Note that a PyTorch Geometric implementation of the GCN layer described in equation \ref{eq:GCN} is used in the replication paper \cite{holdijk2021re} with a ReLU activation function. Thus, the sole difference between the two is the order of the matrix multiplication. \bigskip

We use a PyTorch Geometric implementation of the stacked GNN layer with a ReLU activation function, as defined in Equation \ref{eq:higher-order-gnn}. On the one hand, it works on bipartite graphs by definition. On the other hand, it explicitly allows for edge weights to be passed, which is essential for applying the sampled, approximate discrete explanation mask to the input graph during training. This is necessary since in practice, applying a hard mask to the input graph would prevent the computation of gradients during backpropagation.

Thus, instead of actually removing edges of the graph, the edge importance scores that are learned by the MLP are treated as edge weights in the prediction process of the \ac{TM}, with higher scores indicating edges being more relevant for the prediction. During training, the prediction for the original input graph is computed from the graph defined by its node features and edge-index alone, while for the sampled graph the edge weights are passed additionally and internally multiplied with the edge-index.\bigskip

\textbf{Explainer Architecture}\par
We implement the explainer MLP architecture described in the paper. It consists of two linear layers, with a ReLU activation function applied after the first. The first layer maps the input edge embedding dimension to 64 hidden units, and the second maps the hidden units to a single output scalar. The input edge embeddings for the MLP are calculated as described in Section \ref{sec:Global_Explanations}. 

For the described \acp{TM}, the resulting MLP input - a concatenation of the node embeddings - has a shape of $\mathbb{R}^{2\cdot20}$ for graph tasks and $\mathbb{R}^{3\cdot60}$ for node tasks. Again, this is taken from the original codebase, as the paper does not consider the node embeddings as a concatenation of all GNN layers for node tasks.

We implement the calculation of edge embeddings as seen in Listing \ref{lst:edge_embedding_calc}.
\begin{lstlisting}[language=Python, caption=Implementation of edge embedding calculation., label=lst:edge_embedding_calc]
    def getEdgeEmbeddings(self, modelGNN, x, edge_index, nodeToPred=None):
        emb = modelGNN.getNodeEmbeddings(x, edge_index)
        i, j = edge_index[0], edge_index[1]
        
        if nodeToPred is not None:
            node_emb = emb[nodeToPred].repeat(len(i), 1)
            embCat = torch.cat([emb[i], emb[j], node_emb], dim=1)
        else:
            embCat = torch.cat([emb[i], emb[j]], dim=1)
        return embCat
\end{lstlisting}
The trained, fixed \ac{TM} \lstinline|modelGNN| is passed to the explainer and returns its node embeddings for the input graph, defined by its node features \lstinline|x| and the \lstinline|edge_index|. Depending on the task at hand, the node embeddings of each two connected nodes, as well as the node embeddings of the node to be predicted \lstinline|nodeToPred| in the case of a node task, are concatenated and returned as edge embeddings. \bigskip

\textbf{Explainer Training Specifications}\par
We initialize all linear layer weights of the explainer with He initialization \cite{he2015delving}, which is used by default for linear layers with ReLU activation in PyTorch. We also test the effect of adopting the initializations from the \acp{TM} and initializing all layer weights with Xavier \cite{glorot2010understanding} (see Section \ref{sec:ind_results}). Biases are generally initialized with zero. During training, the Adam \cite{kingma2014adam} optimizer is used to update the model parameters based on gradients. Additionally, gradients are clipped during training using PyTorch's \lstinline|clip_grad_norm_| with a maximum norm of 2. The authors define the temperature used in the reparameterization trick (see Equation \ref{eq:reparam_trick}) with an annealing temperature schedule, as proposed by Abid et al. \cite{abid2019concrete}:
\begin{equation}
    \tau^{(t)} = \tau_0(\frac{\tau_T}{\tau_0})^{\frac{t}{T}},
\end{equation} 
where $t$ is the current epoch and $T$ is the total number of epochs. $\tau_0$ and $\tau_T$ are hyperparameters that define the initial and final temperature, respectively.
The reason for this is that small temperatures tend to generate more discrete graphs, as they more closely approximate samples from the binary concrete distribution. While this is desirable at convergence, it may hinder the optimization early in training due to a reduced gradient signal. It should be noted that in the original codebase $t$ is initialized with $0$, leading to a temperature $\tau^{(0)} = \tau_0$ in the first epoch, and $\tau^{(T-1)} = \tau_0(\frac{\tau_T}{\tau_0})^{\frac{T-1}{T}}$ in the last epoch. We initialize $t$ with $1$ to get a temperature of $\tau^{(T)} = \tau_T$ in the last epoch, as we believe this to be more inline with the proposition by Abid et al. \cite{abid2019concrete}. \bigskip

As established in Section \ref{sec:Global_Explanations}, the node prediction of an $L$-layer GNN is completely determined by its local computation graph, defined as its $L$-hop neighborhood \cite{ying2019gnnexplainer}. This is capitalized on during explainer training by computing the neighborhood graphs of each input node and treating this subgraph as a base for the node predictions, rather than using the full graph (see Algorithm \ref{alg:node-alg}). Since the \acp{TM} for node tasks consist of three graph layers, the $3$-hop subgraph of each node is computed. We implement this using the \lstinline|k_hop_subgraph()| function from PyTorch Geometric. \bigskip

We compute the loss as described in Equation \ref{eq:mlp_loss}, with each regularization term being added according to a regularization coefficient hyperparameter. It is noteworthy that the loss function used in the original codebase can formally be defined as:
\begin{equation}
    \min_\Psi \frac{1}{K}\sum_{i\in \mathcal{I}}\sum_{k=1}^K -\log P_\Phi(\hat{Y}_s = c_i|G=\hat{G}_s^{(i,k)}),
\end{equation}
where $c_i$ denotes the class label according to the prediction on the original graph $G_o^{(i)}$ and $K=1$. However, this is not mentioned in the paper and therefore not adopted.  \bigskip

A further undocumented specification is that for \acp{TM} that perform node tasks, only node instances that belong to the "motif classes" are selected for training and evaluation, since for nodes that do not belong to these classes there does not exist a specific \ac{GT} motif that may serve as an explanation. This is only documented for one graph classification dataset, where only one of two possible classes has a dedicated motif. The specific node sets used for each dataset vary as well and are presented in Section \ref{sec:experiments_replication}.\bigskip

In the PGExplainer paper, Luo et al. \cite{luo2020parameterized} propose the use of high temperatures, with $\tau_0 = 5.0$ and $\tau_T=2.0$. Holdijk et al. \cite{holdijk2021re} found that all hyperparameters described in the paper are overly simplified and in practice, different parameters are used for each dataset. Therefore, we conduct hyperparameter searches for each task in \ref{sec:experiments_replication}. We add that the number of sampled graphs $K$ is not defined in the original work, and the codebase suggests the use of $K=1$. We implement the ability to sample multiple graphs as described in the pseudocode (see Appendix \ref{sec:PGE_material}) and also consider this in our hyperparameter searches. \bigskip

\textbf{Extension Description}\par
This is not described in the original paper, but since undirected graphs contain bidirectional edges, effectively each edge carries two importance scores. The authors alleviate this in the code by symmetrizing the edge weight matrix, corresponding to the adjacency matrix of the graph. We propose meaning the logits of each pair of bidirectional edges after the MLP output has been calculated. We implement the logic of meaning each edge's logits as seen in Listing \ref{lst:edge_meaning}.
\begin{lstlisting}[language=Python, caption=Implementation of meaning bidirectional edge weights., label=lst:edge_meaning]
    edge_pairs = edge_index.t()
    # Sort node pairs so that (i, j) and (j, i) are treated the same
    canonical_pairs, _ = edge_pairs.sort(dim=1)
    # Find unique undirected edges and get mapping indices
    unique_pairs, inverse_indices = torch.unique(canonical_pairs, dim=0)
    
    # Average weights for duplicate edges
    for i in range(unique_pairs.size(0)):
        mask = inverse_indices == i
        mean_weight = torch.mean(w_ij[mask])
        w_ij[mask] = mean_weight
\end{lstlisting}
All edge pairs from the edge-index that connect the same two nodes, e.g. $(i,j)$ and $(j,i)$, are treated as a unique pair. For each of these unique pairs the mean of the logits $\omega_{i,j}$ and $\omega_{j,i}$ corresponding to its two edges is calculated and used to update each of the two logits. To account for this in the reparameterization trick, the unique pairs are further passed there to sample $\epsilon$ for each edge pair, rather than each edge. This process guarantees edge pairs that connect the same two nodes to always carry identical importance scores. \bigskip

\textbf{Evaluation Implementation}\par
To quantitatively evaluate the explanations of each prediction (see Section \ref{sec:experiments_replication}), the authors utilize the ROC-AUC as a metric to compare the predicted importance scores of the edges to the \ac{GT} edges. Since the exact procedure is not further described by the authors, we extract it from the codebase as far as possible.

For graph tasks the metric is computed globally, meaning that for all graph instances the edge predictions and \acp{GT} are gathered, and the global thresholds are computed, while for node tasks the metric is computed locally for the $3$-hop subgraph of each node instance and a mean is calculated later on.
Since a reason for this difference in calculation is not provided, we choose to only consider the mean of the local values, regardless of task, to get a uniform procedure. Additionally, we believe that this is more in line with the inductive setting, where individual unseen instances can be explained.

It is notable that motif nodes may be selected for explanation, where the local computation graph only consist of motif nodes and consequently only \ac{GT} edges. Since the local AUROC score is not computable for these binary cases with only one class present, it is explicitly disregarded and skipped for these nodes in the evaluation process. Note that these node instances are not disregarded during training.

We implement this calculation using the \lstinline|BinaryAUROC| from TorchEval, which operates identical to the \lstinline|roc_auc_score| from scikit-learn \cite{pedregosa2011scikit} in the binary case, used in PGExplainer \cite{luo2020parameterized}.

The qualitative explanations provided by the explainer are implemented as a NetworkX visualization of the graph instance in the case of graph tasks, or a visualization of the $3$-hop subgraph of the instance node in the case of node tasks. Only the edges with the top-$2k$ importance scores are drawn, where $k$ equals $\text{\#motif-edges}$, since edges are bidirectional and guaranteed to carry identical weights. $k$ can therefore be understood as a needed parameter for qualitative evaluation \cite{holdijk2021re}. \bigskip

The inference time is measured as the time it takes to generate an explanation for any instance. This starts with the computation of node embeddings from the \ac{TM} and ends after the reparameterization trick - the simple application of a Sigmoid to the edge logits during evaluation - that returns the edge importance scores. This process is illustrated in Figure \ref{fig:PGExplainer_pipeline}, starting from input graph $G_o$ and ending at the sampled graph $\hat{G}_s$. \bigskip

\subsection{Application on NeuroSAT}
\label{sec:Application_to_NeuroSAT}

In this section we describe the necessary changes for explaining predictions of NeuroSAT \cite{selsam2018learning} with our PGExplainer framework. We want to stress that the NeuroSAT codebase was provided by a fellow student and only the changes mentioned in this section are part of our work. The source code for NeuroSAT is publicly available.\footnote{Daniel Selsam et al. “NeuroSAT” (2018). URL: \url{https://github.com/dselsam/neurosat}} \bigskip

Since NeuroSAT can be regarded as a black box MPNN in the context of our work, we only describe how it passes messages superficially. In each iteration, each clause receives messages from its neighboring literals to update its embedding. Then, each literal receives messages not only from its neighboring clauses, but also from its complementary literal, to update its embedding. The number of iterations is set to $26$ for the model used, which was trained in the standard manner on both satisfiable and unsatisfiable SAT problems - formulae in CNF with the goal of determining whether they are satisfiable. The downstream task of NeuroSAT can hence be understood as a graph classification task.

Besides the prediction of satisfiability, NeuroSAT returns the node embeddings of both clauses and literals at each iteration. We use these to generate edge embeddings for all edges in the biadjacency matrix as input for the explainer MLP (see Section \ref{sec:NeuroSAT_extension}), using the same procedure as in the previous replication study (see Section \ref{sec:Replication_of_PGExplainer}). Since the SAT instances are defined as biadjacency matrices, edges only exist in one direction - from literals to clauses. Thus, we dismiss the meaning of edge logits and qualitatively evaluate using the top-$k$ edges, rather than the previous top-$2k$. \bigskip

The only change we have to make is allowing the NeuroSAT forward pass to receive edge weights as a parameter. If no edge weights are passed, the forward pass behaves as usual and the biadjacency matrix contains discrete values of 0 or 1. When predicting sampled graphs from the explainer, we pass the sampled edge weights and multiply these with the biadjacency matrix, to receive a weighted biadjacency matrix. NeuroSAT then calculates the prediction for the weighted matrix. This change is implemented as seen in Listing \ref{lst:NeuroSAT_adaptation}.
\begin{lstlisting}[language=Python, caption=Adaptation of NeuroSAT., label=lst:NeuroSAT_adaptation]
    connections = torch.sparse_coo_tensor(
            indices=edges,
            values=torch.ones(problem.n_cells, device=self.device) 
                    if edge_weights is None else edge_weights,
            size=torch.Size([n_literals, n_clauses])
        ).to_dense()
\end{lstlisting}
Usually, the \lstinline|connections| tensor - a biadjacency matrix - is initialized with ones at the coordinates of the edge-index tensor \lstinline|edges|, containing \lstinline|problem.n_cells| edges of a SAT problem batch. However, when \lstinline|edge_weights| is passed into the function, the edge weights are used as initialization, rather than ones. This can be understood as a multiplication of the edge weights with the ones representing edges.

Furthermore, the input of the explainer pipeline changes to a SAT problem instance, containing only the edge-index representation of a SAT formula, as well as a label 1 or 0, denoting satisfiability or unsatisfiability, respectively. Node features are not used in NeuroSAT and therefore irrelevant for the explainer. \bigskip

The hard constraint described in Section \ref{sec:NeuroSAT_extension} is implemented as seen in Listing \ref{lst:hard_constraint}.

\begin{lstlisting}[language=Python, caption=Implementation of Hard Constraint., label=lst:hard_constraint]
    batch_clauses = torch.tensor(problem.batch_edges[:, 1])
    clauses, inverse_indices = torch.unique(batch_clauses, dim=0, 
        return_inverse=True)

    for clause_id in clauses:
        mask = batch_clauses == clause_id
        clause_edge_probs = w_ij[mask]
        
        if len(clause_edge_probs) > 1:
            clause_mean_weight = torch.mean(clause_edge_probs)
            w_ij[mask] = clause_mean_weight

    if self.training:
        rand_vals = torch.rand(len(clauses), device=w_ij.device) + 1e-8
        epsilon = rand_vals[inverse_indices].reshape(w_ij.shape)
        
        trick = (torch.log(epsilon)-torch.log(1-epsilon)+w_ij)/temp
        edge_ij = nn.Sigmoid()(trick)    
    else:
        edge_ij = nn.Sigmoid()(w_ij)
\end{lstlisting}

First, all clauses are extracted from an edge index batch \lstinline|problem.batch_edges|. Since these appear in the edge index for each literal that connects to it, we need to calculate the \lstinline|torch.unique| clauses. For each \lstinline|clause| in this list of unique \lstinline|clauses| we create a mask that stores its appearance in the \lstinline|batch_clauses| and extract the masked edge logits \lstinline|w_ij| that have the same shape as \lstinline|batch_clauses|. For all clause logits \lstinline|clause_edge_probs| the mean is calculated and the logits \lstinline|w_ij| are updated accordingly.
The unique \lstinline|clauses| are further used in the reparameterization trick to sample a unique \lstinline|epsilon| for each clause rather than for each edge.

The soft constraint uses the same idea of iterating over the unique \lstinline|clauses| in a batch. Instead of calculating the \lstinline|torch.mean| it calculates the \lstinline|torch.var| and adds this to the loss after multiplication with a fixed connectivity coefficient hyperparameter. \bigskip

To evaluate the explanations provided by the PGExplainer, we require \acp{GT} to calculate the AUROC score and understand the visualizations of explanations. Since our goal is testing whether the explanations of unsatisfiable problems, based on NeuroSAT predictions, align with human-understandable concepts, we propose the use of MUSes as \ac{GT}. Therefore, we utilize the deletion-based MUS extractor \lstinline|MUSX| from PySAT \cite{imms-sat18} to generate a MUS for each unsatisfiable problem. This \ac{MUS} is transformed into a mask matching the corresponding edges in its problem's graph representation, and treated as expected \ac{GT}. This allows us to calculate the local AUROC for each SAT problem as described in Section \ref{sec:Replication_of_PGExplainer}. We visualize the provided explanations and \acp{GT} as a pyvis \cite{perrone2020network} \lstinline|Network| for qualitative evaluation. \bigskip

We adopt the MLP architecture described in Section \ref{sec:Replication_of_PGExplainer}, but also perform experiments with a more complex architecture (see Section \ref{sec:SAT-experiments}). This introduces two additional hidden layers, leading to the architecture described in Listing \ref{lst:ext_mlp_arch}.
\begin{lstlisting}[language=Python, caption=Implementation of extended MLP architecture., label=lst:ext_mlp_arch]
    self.model = nn.Sequential(
            nn.Linear(self.inputSize, 256),
            nn.ReLU(),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Linear(64, 20),
            nn.ReLU(),
            nn.Linear(20, 1)
        )
\end{lstlisting}
The \lstinline|inputSize| depends on whether the node embeddings $\mathbf{z}$ are used from the last iteration or if a concatenation of the states at multiple iterations shall be used. Since the embedding size of NeuroSAT is 128, this leads to an edge embedding \lstinline|inputSize| of either 256 or 768, respectively. Besides these changes, the explainer operates as previously explained.