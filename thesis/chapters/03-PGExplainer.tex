\chapter{PGExplainer - Main part}
\label{ch:PGExplainer}
V1: In the following chapter, we introduce the PGExplainer\cite{} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models for any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. This approach seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{}. \\

V2: In the following chapter, we introduce the PGExplainer\cite{} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models that have proven to learn the concise underlying structures of GNNs most relevant to their predictions. This approach may be applied to any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. PGExplainer seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{}. This means that all edges in the dataset are predicted by the same model, which leads to a global understanding of the DT. \\

V3: In the following chapter, we introduce the PGExplainer\cite{} and its concepts. The idea is to generate explanations in the form of edge distributions or soft masks using a probabilistic generative model for graph data, known for being able to learn the concise underlying structures from the observed graph data. The explainer uncovers said underlying structures, believed to have the biggest impact on the prediction of a GNNs, as explanations. This approach may be applied to any trained GNN model, henceforth referred to as the downstream task (DT). 
By utilizing a deep neural network to parameterize the generation process, the explainer learns to collectively explain multiple instances of a model. Since the parameters of the neural network are shared across the population of explained instance, PGExplainer provides "model-level explanations for each instance with a global view of the GNN model". Furthermore, this approach cannot only be used in a transductive setting, but also in an inductive setting, where explanations for unexplained nodes can be generated without retraining the explanation model. This improves the generalizability compared to previous works, particularly the GNNExplainer\cite{}.

The focus in this approach lies in explaining the graph structure, rather than the graph features, as feature explanations are already common in non-graph neural networks. \\

Transductive/Inductive explanation? \\

We then describe our reimplementation in detail (Section 3.2), including the changes made and difficulties during the process. \\

In Section 3.3 we present the idea of applying PGExplainer on the NeuroSAT framework to generate explanations for the machine learning SAT-solving approach and comparing these to "human-understandable" concepts like UNSAT cores and backbone variables. 
%Usable in transductive and inductive settings, the latter being the reason for better generalizability, as the explainer model does not have to be retrained to infer explanations of unexplained nodes.
%"generative probabilistic model for graph data", able to learn succinct(prÃ¤gnant) underlying structure (The probabilistic model is the explanation)
%Explainer aims to uncover the concise underlying structure, believed to have the highest impact on the prediction of the DS, as an explanation.
\section{Theory}
We follow the structure of the original paper\cite{} and start by describing the learning objective (Section 3.1.1), the reparameterization trick (Section 3.1.2), the global explanations (Section 3.1.3) and regularization terms (Section 3.1.4).

\subsection{Learning Objective}
To explain the predictions made by a GNN model for an original input graph $G_o$ with $m$ edges we first define the graph as a combination of two subgraphs: $G_o = G_s + \Delta G$, where $G_s$ represents the subgraph holding the most relevant information for the prediction of our GNN, referred to as explanatory graph. $\Delta G$ contains the remaining edges that are deemed irrelevant for the prediction of the GNN. (Similar to GNNexplainer,) The PGExplainer then finds $G_s$ by maximizing the mutual information between the predictions of the DT and the underlying $G_s$:
\begin{equation}
    \max_{G_s} I(Y_o;G_s) = H(Y_o) - H(Y_o|G=G_s),
\end{equation} 
where $Y_o$ is the prediction of the DT model with $G_o$ as input. This quantifies the probability of prediction $Y_o$ when the input graph is restricted to the explanatory graph $G_s$, as in the case of $I(Y_o;G_s) = 1$, knowing the explanatory graph $G_s$ gives us complete information about $Y_o$, and vice versa. Intuitively, if removing an edge $(i,j)$ changes the prediction of a GNN drastically, this edge is considered important and should therefore be included in $G_s$.
It is important to note that $H(Y_o)$ is only related to the DT with fixed parameters during the evaluation/explanation stage. This leads to our objective being equivalent to minimizing the conditional entropy $H(Y_o|G=G_s)$.

To optimize this function a relaxation is applied for the edges, since normally there would be $2^m$ candidates for $G_s$. The explanatory graph is henceforth assumed to be a Gilbert random graph, where the selections of edges from $G_o$ are conditionally independent to each other. However, the authors describe a random graph with each edge having its own probability, rather than a shared probability as described in \ref{sec:random-graphs}, as follows: Let $e_{ij}\in V \times V$ be the binary variable indicating whether the edge is selected, with $e_{ij} = 1$ if edge $(i,j)$ is selected to be in the graph, and 0 otherwise. For the random graph variable $G$ the probability of a graph $G$ can be factorized as 
\begin{equation}
    P(G) = \prod_{(i,j)\in E}P(e_{ij}).
\end{equation}
TODO: Inhomogeneous Erdos Renyi model? Mention that this is a generative model?
 $P(e_{ij})$ is instantiated with the Bernoulli distribution $e_{ij} \sim Bern(\theta_{ij})$, where $P(e_{ij} = 1) = \theta_{ij}$ is the probability that edge $(i,j)$ exists in $G$.
After this relaxation the learning objective becomes:
\begin{equation}
    \label{eq:init_learning_obj}
    \min_{G_s}H(Y_o|G = G_s) = \min_{G_s} \mathbb{E}_{G_s}[H(Y_o|G = G_s)] \approx \min_{\Theta} \mathbb{E}_{G_s \sim q(\Theta)}[H(Y_o|G = G_s)],
\end{equation}
where $q(\Theta)$ is the distribution of the explanatory graph that is parameterized by $\Theta$'s.

\subsection{Reparameterization Trick}
TODO: As described in section \ref{sec:perturbation-based_explainability}, a reparameterization trick can be utilized to relax discrete edge weights to continuous variables in the range $(0,1)$. This is used in PGExplainer to allow for efficiently optimizing the objective function with gradient-based methods. The sampling process $G_s \sim q(\Theta)$ is approximated with a determinant function that takes as input the parameters $\Omega$, a temperature $\tau$ to control the approximation and an independent random variable $\epsilon$: $G_s \approx \hat{G}=f_\Omega(G_o,\tau,\epsilon)$. The binary concrete distribution is utilized as an instantiation for the sampling, yielding the weight $\hat{e}_ij \in (0,1)$ for edge $(i,j)$ in $\hat{G}_s$, computed by:
\begin{equation}
    \label{eq:reparam_trick}
    \epsilon \sim \text{Uniform}(0,1), \qquad \hat{e}_{ij}=\sigma((\log \epsilon - \log(1-\epsilon)+\omega_{ij}/\tau),
\end{equation}
where $\sigma(\cdot)$ is the Sigmoid function and $\omega_{ij} \in \mathbb{R}$ is the parameter/explainer logit for the corresponding edge. When $\tau \rightarrow 0$, e.g. during the explanation stage, the weight $\hat{e}_{ij}$ is binarized with the sigmoid function $\lim_{\tau\rightarrow 0}P(\hat{e}_{ij} = 1) = \frac{\exp (\omega{ij})}{1+\exp (\omega{ij})}$. Since $P(e_{ij} = 1) = \theta_{ij}$, choosing $\omega_{ij} = \log\frac{\theta_{ij}}{1-\theta_{ij}}$ leads to $\lim_{\tau\rightarrow 0}\hat{G}_s = G_s$ and justifies the approximation of the Bernoulli distribution with the binary concrete distribution. During training, when $\tau > 0$, the objective function in \eqref{eq:init_learning_obj} is smoothed with a well-defined gradient $\frac{\partial\hat{e}_{ij}}{\partial\omega_{ij}}$ and becomes:
\begin{equation}
    \min_\Omega \mathbb{E}_{\epsilon \sim \text{Uniform}(0,1)}H(Y_o| G = \hat{G}_s)
\end{equation}

TODO: Slight modification of objective by replacing conditional entropy with cross entropy. Focus on correct prediction of class label

With the modification of the conditional entropy to cross-entropy $H(Y_o, \hat{Y}_s)$, where $\hat{Y}_s$ is the prediction of the DT when $\hat{G}_s$ is given as input, as well as adaption of Monte Carlo sampling, the learning objective becomes:
\begin{align}
    &\min_\Omega\mathbb{E}_{\epsilon\sim\text{Uniform}(0,1)}H(Y_o, \hat{Y}_s) \approx \min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P(Y_o = c) \log P(\hat{Y}_s = c) \\
    = &\min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P_\Phi (Y_o = c|G = G_o) \log P_\Phi(\hat{Y}_s = c|G=\hat{G}_s^{(k)}).
\end{align}
$\Phi$ denotes the parameters in the DT, $K$ is the number of total sampled graphs, $C$ is the number of class labels, and $\hat{G}_s^{(k)}$ denotes the $k$-th graph sampled with equation \ref{eq:reparam_trick}, parameterized by $\Omega$. Note that this objective is defined per explainable instance.

The approach in PGExplainer is a common approach in ML for simplifying objectives? FIND LITERATURE THAT EXPLAINS APPROXIMATION OF COND. ENTROPY WITH CROSS ENTROPY. Explanation as simple as one formula for one graph variable example, cross entropy applied to whole distribution? \bigskip

"We can modify the conditional entropy objective in Equation 4 with a cross entropy objective between the label class and the model prediction" (GNNExplainer)


\subsection{Global Explanations}

\subsection{Regularization Terms}

\section{Reimplementation}
TODO: Maybe separate the theory of PGE from our own work more strictly? E.g. 3. PGE theory, 4. PGE reimplementation and NeuroSAT application? \\
Implementation details:
\begin{itemize}
    \item Started by reimplementing the downstream tasks used in og paper for node and graph class.
    \item Node datasets taken from original and transformed to a "pyg format", to keep original structure and ground truths
    \item Graph: BA-2Motif from pyg library with self generated gt, MUTAG dataset taken from pyg and added gt-labels that were added in original dataset
    \item Created pytorch/pytorch geometric implementation of 3-layer graph conv networks
    \item architecture as described in paper: ReLu activation, Pooling for graph net
    \item Xavier initialization for all layers
    \item Same hyperparameters?(Adam, $1*10^-3$ lr, 1000 epochs)/experimental setup?
    \item ADDED Dropout(0.1) to improve performance/overfitting on node tasks
    \item Fully connected layer: torch geometric GraphConv to allow passing of edge weights(+ bipartite)!?
    \item Original references sageConv in paper, pyG impl. does not allow edge weights, verify!
    \item GATConv for Graph attention(referenced by original)
    \item Alternatives: GCNConv(edge weights, no bipartite graphs)
    \item 80/10/10 split
    \item => Similar accuracies achieved
\end{itemize}
 

 Explainer:
 This is irrelevant for paper, description of why reparametrization trick necessary
 - First approach tried passing a masked edge index to downstream task with edge weights > 0.5
 => Unable to learn with hard cut-off (bad for gradients). (Same with TopK probably?!)
 - Pass calculated edge weights to downstream task for learning. If no edge weights are passed(downstream task), all edge weights are initialized with one to represent all edges beeing relevant
 
%\section{Benchmarking}
%\section{Application on Bipartite Graphs?}#

\section{Application on NeuroSAT}
Reimplementation of NeuroSAT provided by Rodhi. As the code used for NeuroSAT can also be found in our Repository, we stress that only the changes described in the following chapter are part of our work. \\

What did we do? What did we change for NeuroSAT? What data was used? How did we adapt PGExplainer? \\

Only change in NeuroSAT: pass edge weights into adjacency matrix. Calculates .... \\
Generated batches of unsat problems that "turned" unsat because of last added clause. 10 literals per problem. Only unsat to test for unsat cores, that only apply for unsat problems. Calculated unsat cores with solver xy by adding negative assumption literals per clause and passing these as assumption for calulation. The edges of the clauses present in the unsat core were treated as ground truth. \\

Changes for explainer:
Edge embeddings calced by DS and passed to explainer. Calculated edge embeddings by concatenating node embeddings for connected nodes, similar to original. Embeddings fed into MLP, weights sampled with reparam. trick to get edge "probabilities", passed as "unbatched" sampled graph into NeuroSAT predictor. Visualization of SAT problems with edge weights, ands gts. \\
For quant. eval. adapted roc auc as metric as done in PGExplainer. Results seem "good" but qual. eval. shows different result. roc auc bad metric? \\
For qual. eval. topk(=number of edges in gt) edges of predictions were highlighted to be compared to gt edges. For quant. eval. the edge probabilites were compared to gt with 1s for edges in gt and 0s for rest. \\