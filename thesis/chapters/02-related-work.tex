\chapter{Related Work}

\textbf{GNNExplainer} \\
Ying et al. proposed the GNNExplainer\cite{ying2019gnnexplainer}: the first general, model-agnostic explainer for graph neural networks on any graph-based machine learning task. It is able to identify a concise subgraph structure and a subset of node features, that play a crucial role in the prediction of the underlying graph neural network. This is generally understood as an explanation. The work by Ying et al. serves as the main baseline for the PGExplainer, that seeks to improve its predecessor. Many concepts, experiments and specifications of PGExplainer were adapted from the GNNExplainer, which we seek to process in our work. \bigskip

\textbf{Parameterized Explainer for Graph Neural Networks} \\
The Parameterized Explainer for Graph Neural Networks (PGExplainer) by Lou et al.\cite{luo2020parameterized} is the main subject of our work. The method adopts a deep neural network to parameterize the generation process of explanations, thus allowing multiple instances to be explained collectively. Furthermore, it has better generalization ability and can explicitly be utilized in an inductive setting.

We reimplement the original work using PyTorch\cite{paszke2019pytorch} and PyTorch Geometric\cite{Fey/Lenssen/2019} instead of TensorFlow\cite{tensorflow2015-whitepaper}, while also emphasizing its application in an inductive setting, where test instances are unseen during training, as opposed to the original collective setting. A secondary study on the inductive performance was also performed by the authors, which we want to extend by applying it on a graph neural network with a slightly different architecture, testing whether the explainer proves to be model-agnostic. This extension is motivated by the need for explainability methods that generalize across architectures (TODO: SOURCE FOR THIS). The goal of this thesis is to apply the explainer model to a deep learning approach for solving a bipartite graph problem - specifically, the boolean satisfiability problem - and generate proofs for the deep model's predictions. \bigskip

\textbf{[Re] Parameterized Explainer for Graph Neural Network} \\
%We try own reimplementation that follows the original paper, as well as code and reimplementation for uncertainties + use a slightly different architecture in underlying GNN + Hyperparameter search of combination of parameters used in original and repication. Treated as additional baseline

Holdijk et al.\cite{holdijk2021re} performed a replication study on the PGExplainer that focuses on reimplementing the method in PyTorch, testing whether the claims with respect to the GNNExplainer hold and discussing whether the used evaluation method makes sense. They highlight a large discrepancy between the paper and codebase, making a replication that includes the evaluation method from the paper alone impossible. With help of the codebase, the authors are able to replicate the experiments and verify the main claims of the original paper. However, they express some concerns regarding the evaluation setup and note a large difference between the originally noted AUC scores and their results for most of the datasets. Additionally, they question the general approach for evaluating graph data with ground truths, as done in GNNExplainer and PGExplainer, which we will discuss in \ref{}. We use this work as an additional baseline for our approach, but note that the replication was also done in the collective setting. Therefore, the difference to our work lies mainly in the architecture used for the downstream model and the setting of the experiments. \bigskip

\textbf{Taxonomic Survey} \\
Yuan et al.\cite{yuan2022explainability} performed an extensive taxonomic survey on explainability in graph neural networks. We use this survey to discuss different approaches and motivate the selection of the PGExplainer in \ref{sec:gnn_explainability}, after important terminology has been introduced. The authors note that the PGExplainer "is not performing as promising as its original reported results"\cite{yuan2022explainability}, siding with Holdijk et al.\cite{holdijk2021re}. Nevertheless, we evaluate the explainer model with regard to its application in the inductive setting. \bigskip

%Propose only using PGExplainer for Node classification task.\bigskip

\textbf{NeuroSAT} \\
NeuroSAT by Selsam et al.\cite{selsam2018learning} is a machine learning approach for solving the boolean satisfiability problem using a message passing neural network. It is able to detect satisfying assignments, but lacks proofs of unsatisfiability. The authors performed a small study on the detection of unsat cores, revealing that NeuroUNSAT is able to detect UNSAT cores if the UNSAT problems contain a specific UNSAT core. However, this is expected to be due to the model memorizing the unsat cores, rather than generalizing to any unsat core. We want to test this by applying the PGExplainer in an inductive setting to the NeuroSAT model, and evaluate whether the generated explanations do align with unsat cores.