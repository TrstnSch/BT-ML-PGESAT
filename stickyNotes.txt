Node tasks no more stable and never smaller roc than 0.5 -> Good

Graph tasks now completely collapse!?








Original Loss and Paper Loss seem to achieve same result
DOWNSTREAM TASK MODELS WERE OVERFIT!??!?! -> Used last trained model instead of earlier model that achieved highest val/accuracy
BA-Community explainer now performs well for frst tested seed
BA-2Motif worse now, still achieves decent AUC if DT trained with 0.1 features and explainer uses ones features!








DOWNSTREAM TASK LAST TRAINED (10.04.):
GraphConv with BN after ⅔ hidden layers

BA-2Motif: 1.0, 1.0, 1.0
MUTAG (Dropout 0.1): 0.86, 0.86, 0.82

BA-Shapes: 1.0, 1.0, 0.97
BA-Community (Dropout 0.1): 0.92, 0.87, 0.89
Tree-Cycles: 1.0, 1.0, 0.99
Tree-Grid: 0.99, 0.99, 0.99









Use current version of NeuroSAT to sweep over multiple architectures, hyperparameters and maybe even loss?

Probably smart to store gt in data batch first -> Done
Think on adding the clause meaning after the sampling randomness?









Loss seems to be identical when calced at once instead of for each singular problem/graph









Fix edge probabilite values for NeuroSAT!
Look at mlp outputs/weights, check if negative?
Normalize ->Already normalized out of NeuroSAT

Lower lr improves edge probabilites









Use Binary cross entropy for NeuroSAT, since paper describes using cross entropy and probabilites in NeuroSAT are binary.

Seems comparable to our implemenation of described formula











Tried to overfit NeuroSAT, very difficult as AUC is not directly used in Explainer since it only calculates loss between predictions. Introduced evidence might be a problem here?

Implemented MUS calculation as gt instead of unsat cores, achieves very poor auc atm!








Lower lr for neurosat achieves :
Highest individual auc: 0.8164945919370697 







NOTE:
Embeddings for NeuroSAT seem to be between -1 and 1
Embeddings for e.g. Tree-Cycles seem to be between -2 and 25







CONFIRMED: NeuroSAT EVALUATION WITHOUT TRAINING RESULTS IN ROC_AUC OF 0.5!!!!








NeuroSAT Problem:
Unsat core always contains full clauses - for each clause all connected literals are part of graph/gt

Explanations however do not fulfill this and can only contain some of the literals of a clause, which does not make it a subset of the original clause.
Maybe introduce regularization that punishes selection of singular edge instead of all outgoing edges for a clause node?












Currently re running all quant experiments with configs from sweeps and train/val/test split. Inference time included for test data.

WHEN DONE:
COMPARE LIKE IT IS for Tree-Cycles with all motif nodes, for ba-2motif with ones as features?, 
Sweep BA-2Motif
Re run dt on different data splits to get accuracies

THEN:
Leave it mostly as is, maybe try to improve the tasks that fail if ideas there. If no ideas THEN

Continue with NeuroSAT:
Check if explanation from explainer satisfiable (probably not)
Try with MUS
Sweep
Expand input embeddings with intermediate embeddings.








Explainer loss should probably use argmax for original prediction, as discrete case?
-> No, Last loss equation in paper uses conditional probability of each class, so no argmax?!









Current State:
USE TRAIN SPLIT!
BA-Shapes: very robust, works well
BA-Community: Qualitative explanations suck
Tree-Cycles: Does not make sense anymore. AllMotif now also either very high or very low?
Tree-Grid: Okayish, not perfect

BA-2Motif: Works very well with 1s as features
MUTAG: Lowkey sucks, runs all increase but are incredibly high in variance






Cannot reproduce Tree-Cycles runs stored in wandb, maybe redo. Train split plus all nodes also generates either good or bad runs?





Reduced batch size for neuro sat, as well as number variables to constant 8. Different min/max problematic due to varying size having to be extracted from gt/problem individually.

Common edges better now, after removing detach()

Try with MUSs!







TRY for original task:
QUALITATIVE EVALUATION FOR BEST AUC MODELS!
Check values of edge weights!
TRY current effect of loss regularization terms?





Fixed calculation of MUS in pysat. NO ASSUMPTIONS NEEDED!
TODO: Convert MUS into gt







When taking topK lowest edges there is zero overlap between gt and topk, as expected








THERE MUST BE A PROBLEM WITH ROC_AUC OR VISUALIZATION! GT AND TOPK ONLY SHARE 1 OR FEW EDGES?!?!?!?
Same result when training on evaluation data.

For problem with highest and lowest auc, only 1 for highest and 3 for lowest edges are in GT AND TOPK! -> Does not seem to be detecting unsat core.
AUC seems to be increasing for bigger graphs?
Auc probably not a good metric?








Added L2Norm explainer getGraphEmbeddings -> Leads to decrease in AUC?!
Only during training: minimal decrease in auc compared to without, no effect on weights










NeuroSAT on GPU seems to be identical to CPU runs. Evaluation on multiple sub_problems implemented -> Currently using a very large validation set

TODO: Find a clean way to seperate the problem batches into train/val/test

Take high auc and low auc example and compare gt with topK by showing edges present both graphs!

Are calced unsat cores perfect? Able to calculate multiple ones?

Sweep

Node prediction? NeuroSAT not really meant for that?




------------------------------------------




TODO:
Compare GPU CPU runs to determine which datasets can be swept on cpu

Run train split version and compare? On Tree-Cycles very high in variance -> bad
If doing good, hyperparam search with this?

Play around with motif nodes for Tree-Grid? Get better results with only on node per motif that covers complete motif?









TODO: Display negative literals correctly in visualisation, index +-1?
Edge weights for gt too thick! -> Normalize gt and predictions to be between 0 and 1?

GTs vary in size, general problematic for Explainer?

Sweep NeuroSAT
 lr 0.03 leads to edge probabalities geeting rounded to 0
Edge probabilities get very close to 0 anyways -> normalize? Or only hyperparams?

Continue colab runs








First NeuroSAT run with gt:
Eval on one single sub_problem, roc seems to increas, but prediction of sampled graph changes!







Tree-Cycles with all cycle nodes does not work good, at least on „old“ config

BUT: clear observation, runs converge consantly to 0.5 -> random guessing. In original we have very high variance between runs/seeds!

Maybe try sweeping with all nodes?









Sweep BA-Shapes:

LR 0.0003 too low with 10 epochs, discarded fast.








Added Edge mask to calculate loss per sub_problem.
QUESTION: Passing batched data into mlp fine? Should be as it how it works for PGExplainer, right?


NeuroSAT problems: Code runs out of memory?
-> Maybe fixed by detaching NeuroSAT predictions and only calculating pOriginal once

Loss is nan -> Because of negative values passed into log, fixed wiht softmax

1 Epoch takes about 7 minutes atm

What does NeuroSAT predict? -> SAT/UNSAT for each sub_problem in batch








NeuroSAT:
Are problems batches of one big problem?! If so, explainer probably hast to unpack batches and view complete problem! -> Problem is batch of multiple sub_problems





Performance improvement: Instead of k graphs for loop scale edge_ij, predictions?




Differences GPU CPU training:

BA-Shapes: identical
Ba-Community: very slightly better on CPU?!
Tree-Cycles: identical
Tree-Grid: very slightly better on CPU?!
BA-2Motif: stable on GPU, instable on CPU
MUTAG: identical





How to batch node data? Possible?
K_hop_subgraph can be calculate batch wise, but prediction of downstream task has to be calced node wise?!





Current „Problems“:
BA-Shapes: Weird Loss
BA-Community, Tree-Grid: Look stable
Tree-Cycles: VERY high Variance in sampledEdgeSum and AUC       DID NOT USE PYTORCH CONFIGS
BA-2Motif: Scuffed Loss, high AUC variance, AUC decreasing!     DID NOT USE PYTORCH CONFIGS
MUTAG: Loss a bit jumpy                                         DID NOT USE PYTORCH CONFIGS






BA-2Motif needs ones as features!!!!!! nevermind!?!??!?! After significant changes the first run seems to be doing good, but then it gets worse?!






Tree-Grid experiment ran on 511, 800, 9 with 512,800, 9 eval.
Compare to original 511, 800, 1!

Original seems to be worse?









BA-2Motif problem:

Predictions of sampled graphs do not really make sense. The edges probabilities are fine and seem well distributed, but the predictions seem to stagnate

[[0.4585, 0.5415], [0.4604, 0.5396], [0.4527, 0.5473], [0.4566, 0.5434], [0.4617, 0.5383], [0.4666, 0.5334], [0.4573, 0.5427], [0.4607, 0.5393], [0.4527, 0.5473], [0.4574, 0.5426], [0.4583, 0.5417], [0.4540, 0.5460], [0.4603, 0.5397], [0.4609, 0.5391], [0.4614, 0.5386], [0.4579, 0.5421]] 










MUTAG seems to be doing good now? Try a few runs

BUT: AUC decreases over epochs(sometimes). Constant around 70-75%. Validate auc metric in general! GT seems to constantly be the highest weighted edges








TreeGrid seems to work
TreeCycles seems to work relatively good, between 60 and 80%?






Vermutung: Use normalize during training, but not during explainer training?!
-> This leads to bad accuracy outside of training!






BA2-Motif working!?
-No combine edge weights
-No normalization in ds task
-No argmax for Loss
-Loss from paper
-BA-2Motif latest model (no normalization!?)

BUT runs where AUC goes from 99 to 90% and back up to 95%









TRIED REMOVING COMBINE_EDGE_WEIGHTS

BA-2Motif:
Almost all weights tend to turn negative over time, between -0.5 and -0.08. At start all weights positive

Prediction of graph with edge_weights edge_ij is ALWAYS 0!

Using GCNConv instead of GraphConv gets at least some 1 class predictions. AUC at 27%










Normalize added to graph downstream task:
AUC decreases to 0 really fast. Learns the wrong way, but very accurate!!!!

Layer norm in Node Task(Tree-Cycles) leads to better performance in explainer, but also learns to predict minimal weights!
BatchNorm with bad validation values leads to auc of 0.5?! Second run 0.7 but decreases ti 0.5 during later epochs.
USE 2 DIFFERENT BN LAYERS! -> Leads to AUC of 0.05, but increases slightly. LEARNS LOWEST

Tree-Grid learns with same 2 bn layers architecture highest edge weights?! Nearly perfect
Was lucky/random? Second run only around 0.58
Validation quite well though, seems to work well overall










BA-2Motif:
Added weight norm intro forward of explainer. ROC increases to around 90%! Does not steadily increase though and goes up and down.
Cheating?
Houses are detected perfectly, but only gives 5 instead of 6 ndoes. Circle does not work too well, 2 nodes are detected outside?
Seems like that was coincidence ☹️

Weight norm Leads to horrible performance on Tree-Cycles
Init weights for explainer seems to be a bad idea!

Tree-Cycle model without weight norm and init went from AUC of 1 to AUC of 0.5











Added edge weights of one of none given to node downstream task. 
BA-Shapes only has auc problems when additional motif node in graph
BA-Community around 60%, same problem as BA-Shapes(AUC increases if using edge weights instead of topK to 80%. AUC does not improve over epochs). Relatively high probabilies?
Tree-Cycles: Stable around 60%, seems to be missing connectivity constraint! Predicts 4/6 edges correctly, but prefers other tri connected node edges over „base“ motif node edges. CONNECTIVITY CONSTRAINT NOT USED IN OG!?

BA-2Motif: Same as MUTAG, just inverted. All weights are around 0. AUC decreases a lot
MUTAG: Ground truth still messed up + all edge probabilites 1















Swapped pOriginal and pSample in Loss.

BA-Shapes works well
BA-Community is inefficient!
Tree-Cycles does not work, stuck around 50
Tree-Grid does not work, stuck around 50
 Works well if only trained on „middle“ nodes of  Motif (Apparently not anymore)
BA-2Motif decreases AUC during training?!
 From 60% down to 40%
-> PROBLEM WITH NEGATIVE WEIGHTS -> Highest probabilites are 0?!
(Maybe graph impl. problem)
MUTAG is bad, because of bugged ground truth!!













BIGGEST PROBLEM: topK does not perfectly identify topK edges, because of bidirectional edges!!! If same vale edge weights at threshold different edges might be selected instead of always two belonging ones.
To fix: Maybe extract one direction of edges from edge_index and edge weights accordingly before evaluating/processing?