To compare baseline methods GRAD, ATT, Gradient: reimplement as well or take values from paper => Don't reimplement, take from somewhere

Accuracys of Model from best accuracy over all epochs, mean or last epoch => Last epoch, or go back to earlier stage of model

How to create datasets on a singular graph => Simply use one data object(graph)

Use dropout => Sure, as long as eval() is used when not training




MLP learning lowest weights!! -> "Fixed" by setting ADAM to max or calculating reparam trick with -w_ij
 -> This is crucial, possible to "hotfix" with -1?

Performance on MUTAG?

How do I calc AUC? Different results for three different metrics

Apply constraints => size+entropy done
Budget and Connectivity only for node?

Mirror edge weights during training? Fix efficient method!

Graph order determines whether learns highest or lowest?!?!
First 10 epochs learns to maximize, after minimizes? Only on some architectures/hyperparams => TRY!

Triple bonds are detect as double bonds, predicted edge should be there?



WEIGHTS TEND TO BE WAY TOO HIGH FOR TREE-CYCLES. -> PROBABILITIES OF EDGES ARE ALL 1.
CAN BE REDUCED VIA L2NORM, ALTHOUGH APPARENTLY NOT USED IN ORIGINAL!
NORMALIZATION ERROR IN MY MODEL? WHERE? POSSIBLE CAUSES?

Take topK(motif) edges for node tasks as well! => Calculate AUC with topK set to 1, others set to 0 vs. GroundTruth



Background: How much into detail? Where to start? Neural Networks? Machine Learning? ...
Mathematical background -> Probably needed, how deep, where to start?
How to quote? Mathematical stuff/formulas quotes? General: Definitions from original works? Own "dictionary"? 

BA2Motif now struggling on 64 architecture. DOES NOT LEARN ANYMORE. Loss fluctuating!

Training of NodeExplainer: Train on motifs? Node splits of original do not really make sense?? Same for AUC evaluation. HOW CAN I GET EVERY MOTIF ONCE FROM MY DATASET??
    OR HOW CAN I VALIDATE ORIGINAL NODE SPLIT TO MAKE SENSE??
    Motfis I use to calculate AUC are Subragph edges where both connected nodes are of class 1.



plot ground truth from original
Train over motif nodes, with node labels of motif class(each computanional graph should contain a motfi to detect it!?).  Compare to training on ALL nodes. 



When training TreeGrid explainer on all nodes: "Detects" motfis as lowest weights?! Probably simply wrong, since model just learns to detect base nodes as important since usually represented?

BA-Community: 3-Hop graph contains multiple motifs. How to calculate topK/compare to gt?!
Order of pSample and pOriginal in Loss

\section{Linear Algebra} with Scalar, Vector, Matrix... needed? 


Weights for Graph classification too low/high -> All one or all zero. Where to normalize? In forward pass?



Switch back pOriginal and pSample in Loss. Log is apllied to the distribution we want to learn.

Think about creating a dataset for SAT

Presentation: Vorschlag no motfis in graph included prediction

Change graph convolution layer. Implement one according to original code? Replace? -> Exact layer should not really matter, downstream task is treated as blackbox

Train/Test split in Explainer? YES

Absolute values to normalization?! Disregards positively learned values as worst?



Try PGExplainer training for BA-2Motif with only class 1 or only class 2
Calculate downstream task accuracy after explainer training to validate, that downstream task is fixed!




Next presentation:
- quick recap

Generalisierung: Modell soll im voraus lernen wieviele Kanten zu Motif zählen bzw. ob Motif überhaupt vorhanden

Nächste Woche paar Folien Zeigen


How to calc stddev in WandB?!?!?!?
Loss.detach() on initialization??
What needs to be detached? What needs to require grad??


How far into detail? Only basics like in Introduction to GNNs or deeper? Graphics?

Embrace the follow up paper? Use configs from follow up that probably work well or use original and say not working well? Both?
If using their configs, what is own work so far?! How to I write about that?
Compare results to results from reimplementation?

Next steps: Try to play around so that BA-2Motif works??
Try on SAT anyways? Predict motif size??







For NeuroSAT:
Explainer MLP gets all embeddings batched, as it is done in Original. Should work like this, right?
Explicit extraction of sub_problems only for calculation of loss?

How to avoid predicted weights getting closer and closer to 0 instead of staying around the same dimension and varying? Normalization?

How to evaluate Sweeps? Trust Sweep importance scores across 2 seeds? Values obviously dependant on seed.

Writing: PGExplainer explanation in theoretical background and implementation/changes made etc. in main part or everything in main part?

Things done: Sweeps for 5/6 datasets, adapted NeuroSAT and evaluated first runs on 1 sub_problem, ran replication with train/test split

Compared CPU and GPU: Only for BA-2Motif big difference, 3 datasets identical -> Valid to use cpu for 5/6 datasets and gpu only for BA-2Motif? /Does not matter whether cpu/gpu used?

How to dislay standard deviation in wandb???

Replication TODO: Batching for node task to improve performance, validate early stopping and save respective model state for ds task training, Sweep BA-2Motit
NeuroSAT TODO: Evaluate on multiple sub_problems, implement train/test/val split?, Add normalization?, Size of gt passed into qual evaluation: necessary?, Add unsat core to data, Sweep, try different hiddem dim?, MUS?, try with satisfiable problems and backbones as gt?, try on GPU