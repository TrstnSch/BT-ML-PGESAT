To compare baseline methods GRAD, ATT, Gradient: reimplement as well or take values from paper => Don't reimplement, take from somewhere

Accuracys of Model from best accuracy over all epochs, mean or last epoch => Last epoch, or go back to earlier stage of model

How to create datasets on a singular graph => Simply use one data object(graph)

Use dropout => Sure, as long as eval() is used when not training




MLP learning lowest weights!! -> "Fixed" by setting ADAM to max or calculating reparam trick with -w_ij
 -> This is crucial, possible to "hotfix" with -1?

Performance on MUTAG?

How do I calc AUC? Different results for three different metrics

Apply constraints => size+entropy done
Budget and Connectivity only for node?

Mirror edge weights during training? Fix efficient method!

Graph order determines whether learns highest or lowest?!?!
First 10 epochs learns to maximize, after minimizes? Only on some architectures/hyperparams => TRY!

Triple bonds are detect as double bonds, predicted edge should be there?



WEIGHTS TEND TO BE WAY TOO HIGH FOR TREE-CYCLES. -> PROBABILITIES OF EDGES ARE ALL 1.
CAN BE REDUCED VIA L2NORM, ALTHOUGH APPARENTLY NOT USED IN ORIGINAL!
NORMALIZATION ERROR IN MY MODEL? WHERE? POSSIBLE CAUSES?

Take topK(motif) edges for node tasks as well! => Calculate AUC with topK set to 1, others set to 0 vs. GroundTruth



Background: How much into detail? Where to start? Neural Networks? Machine Learning? ...
Mathematical background -> Probably needed, how deep, where to start?
How to quote? Mathematical stuff/formulas quotes? General: Definitions from original works? Own "dictionary"? 

BA2Motif now struggling on 64 architecture. DOES NOT LEARN ANYMORE. Loss fluctuating!

Training of NodeExplainer: Train on motifs? Node splits of original do not really make sense?? Same for AUC evaluation. HOW CAN I GET EVERY MOTIF ONCE FROM MY DATASET??
    OR HOW CAN I VALIDATE ORIGINAL NODE SPLIT TO MAKE SENSE??
    Motfis I use to calculate AUC are Subragph edges where both connected nodes are of class 1.



plot ground truth from original
Train over motif nodes, with node labels of motif class(each computanional graph should contain a motfi to detect it!?).  Compare to training on ALL nodes. 



When training TreeGrid explainer on all nodes: "Detects" motfis as lowest weights?! Probably simply wrong, since model just learns to detect base nodes as important since usually represented?

BA-Community: 3-Hop graph contains multiple motifs. How to calculate topK/compare to gt?!
Order of pSample and pOriginal in Loss

\section{Linear Algebra} with Scalar, Vector, Matrix... needed? 


Weights for Graph classification too low/high -> All one or all zero. Where to normalize? In forward pass?



Switch back pOriginal and pSample in Loss. Log is apllied to the distribution we want to learn.

Think about creating a dataset for SAT

Presentation: Vorschlag no motfis in graph included prediction

Change graph convolution layer. Implement one according to original code? Replace? -> Exact layer should not really matter, downstream task is treated as blackbox

Train/Test split in Explainer? YES

Absolute values to normalization?! Disregards positively learned values as worst?



Try PGExplainer training for BA-2Motif with only class 1 or only class 2
Calculate downstream task accuracy after explainer training to validate, that downstream task is fixed!




Next presentation:
- quick recap

Generalisierung: Modell soll im voraus lernen wieviele Kanten zu Motif zählen bzw. ob Motif überhaupt vorhanden

Nächste Woche paar Folien Zeigen


How to calc stddev in WandB?!?!?!?
Loss.detach() on initialization??
What needs to be detached? What needs to require grad??


How far into detail? Only basics like in Introduction to GNNs or deeper? Graphics?

Embrace the follow up paper? Use configs from follow up that probably work well or use original and say not working well? Both?
If using their configs, what is own work so far?! How to I write about that?
Compare results to results from reimplementation? Yes

Next steps: Try to play around so that BA-2Motif works?? No
Try on SAT anyways? Predict motif size?? -> Try SAT







For NeuroSAT:
Explainer MLP gets all embeddings batched, as it is done in Original. Should work like this, right?
Explicit extraction of sub_problems only for calculation of loss?

How to avoid predicted weights getting closer and closer to 0 instead of staying around the same dimension and varying? Normalization?

How to evaluate Sweeps? Trust Sweep importance scores across 2 seeds? Values obviously dependant on seed.

Writing: PGExplainer explanation in theoretical background and implementation/changes made etc. in main part or everything in main part?

Things done: Sweeps for 5/6 datasets, adapted NeuroSAT and evaluated first runs on 1 sub_problem, ran replication with train/test split

Compared CPU and GPU: Only for BA-2Motif big difference, 3 datasets identical -> Valid to use cpu for 5/6 datasets and gpu only for BA-2Motif? /Does not matter whether cpu/gpu used?

How to dislay standard deviation in wandb???

Replication TODO: Batching for node task to improve performance, validate early stopping and save respective model state for ds task training, Sweep BA-2Motif
NeuroSAT TODO: Evaluate on multiple sub_problems, implement train/test/val split?, Add normalization?, Size of gt passed into qual evaluation: necessary?, Add unsat core to data, Sweep, try different hiddem dim?, MUS?, try with satisfiable problems and backbones as gt?, try on GPU

------------------------------

Logical conclusion of graph probability P(G) for graph generation?; Only direct dependency to Gilbert graph is principle of edge probabilites beeing drawn independently?

PGE formula 5 to 6: Replace conditional entropy with cross entropy?! Explanation in GNNExplainer: "We can modify the conditional entropy objective in Equation 4 with a cross entropy objective between the label class and the model prediction"

Convention citing: Seitenzahlen? Chapter?


random graphs !?! Gilbert: "every possible edge occurs independently with probability 0 < p < 1"
Is that what happens in PGE? edges occur independetly, but probabilities are learned/differ from edge to edge? Or only relevant for defintion; instantiation with bernoulli == what Gilbert definded?
IDEA: instantiation as bernoulli is "constant", not learned. Sampling process: ALWAYS draws from random(0,1), learned weight only added and therefore influences the outcome.
Eq. 3: At this point we only randomly sample a graph and define the goal as the probability that the label is the same for the sampled graph?
-> Graph probability == epsilon?
=> Random graphs/Gilbert model explanation satisfies PGE background, general probabilistic graphical model not needed?


Normalization already present in NeuroSAT, embeddings should be normalized. Additional normalization leads to slight decrease in AUC, does not lead to larger probabilites 


UNSAT core calculation assumptions literature o.ä.

Gilbert graph / Diestel definition: Definition with p is correct for different p per edge? "Fixed probability" for each experiment means a) fixed for all edges or b) fixed for edge e across all Graphs on V?

To PGE: "A straightforward instantiation of P (eij ) is the Bernoulli distribution eij ∼ Bern(θij )" What is Teta ij?

"In the random graph proposed by Gilbert, each potential edge is independently chosen from a Bernoulli distribution" -> Wrong!? Since Gilbert graph also uses same probability for each edge?


Explanations from Lucas:
Intuition: conditional entropy only for one concrete sampled graph G_s, cross entropy for two probability distributions and therefore over distribution of all sampled graphs

Gilbert random graph: PGExplainer uses base idea of Gilbert random graph, but adapts it for varying probabilites per edge -> Independent experiments with DIFFERENT probabalities


------------------------------


When to use detach()? detach() original predictions?

Writing: Deep Learning, GNNs

Code: 
- Difference of BA-2Motif features, compare to original code/dataset
- Opposite classification of Tree-Cycles

Research towards NeuroSAT, try try try
Concat embeddings from different 





------------------------------


NOTE:
Embeddings for NeuroSAT seem to be between -1 and 1
Embeddings for e.g. Tree-Cycles seem to be between -2 and 25

LR of 3*10^-4 instead of 3*10^-3 keeps auc score, leads to better edge probabilities between 0.0006 and 0.0199
    -> Try MUSs first? Formally probably good idea

ROC-AUC for untrained model is 0.5, so metric seems to notice something, even if qual. evaluation does not look the WAY


Relevant to monitor evaluation loss?


BA-2Motif explainer works almost perfect for trained BA-2Motif model, if dataset uses features of ones. BA-2Motif model was trained on 0.1 features though and achieves poor accuracy of 0.5 on data with ones features!!!
Trained BA-2Motif model with ones -> Explainer on same data performs relatively poor
    -> Explainer with 0.1 as features performs even worse (NOTE THAT THIS VERSION LEADS TO A SMOOTH LOSS!)

Trained on BA-2Motif with original 0.1 features and same data in explainer -> Starts out really well (0.9) but decreases to 0.65
    -> Explainer with ones as achieves outstanding auc (0.98) (Does not make sense, explains data that the DT did not learn) (NOTE THAT THIS VERSION LEADS TO A SMOOTH LOSS!)

Similar to the Batchnorm behaviour with different presence in training and evaluation in explainer


Try to get explainer to overfit with very small number of subproplems, scale up the network size etc...


Predict clauses instead of edges, adapt framework to carry clause weights on connected edges?


------------------------------

Evaluation with MUS instead of random UNSAT core

NeuroSAT added hard constraint
1. logits are meaned for edges of same clause
2. same logic used to draw randomness per clause instead per edge -> makes sense?

AUROC calculated for each sub_problem and meaned

- first small sweeps, no reliable results to increase performance -> Currently sitting slightly above 0.5
=> Draw conclusion that GNN does not rely on MUSs from this?

OVERFIT NEUROSAT! (HOW? MODEL DOES NOT LEARN WITH GT!)

Direct comparison of 3 embedding input and 1 embedding input on same setup for one seed: (NeuroSAT-HARD-CONSTRAINT-HARD-SAMPLING: winter-river-16(3embs) and devout-energy-15)

3 embedding input loss is smoother and steadily decreasing; AUROC not above 0.55, decreasing for both models for current config t oaround 0.52.
-> Both no good result

Same for network with 3emb and larger architecture? (grateful-microwave-18)
-> AUROC starts lower but quickly reaches and stays around .53?

=> For all runs: Highest auc qualitative analysis only contains one or two of the clauses that are part of the MUS






Downstream task training: Use first model that achieves 1.0 val accuracy or last model that also decreases the val loss?

For original:
Undirected graphs have two edge embeddings since treated as directed graph with edges in both directions.
Idea: Similar to NeuroSAT: Mean explainer logits for each edge pair? How to treat randomness?
Add one random value to each edge pair instead of to each edge? -> Seems to add slight improvement/stability


AUROC Evaluation:
- original skips tasks where gt only ones or zeros. Torch binaryAUROC return 0.5 for these cases. Skip or calc with 0.5?


Tree-Cycles
-> Many hyperparam configs that lead to AUROC of 0 or 1, "only" depending on seed -> GOOD OR BAD?
What does seed affect? Xavier init and randomness in sampling!?
=> Variance over seeds can only be affected by initalisation of layers??

-> Try weight decay or different optimizer

Observed effect on "best" parameters from sweep (Seed-effects: 25-27 and 31-33): 
- Good initalization leads to almost perfect instant auroc, stays there. sum_sampled_edges decreases
- Bad init leads to very slow start (Increase in AUROC only after 10 epochs) that converges to 0.5 instead of 1. sum_sampled_edges increases
=> Model muss sich fangen, da in lokalem maximum?? und kann dann allerdings nicht mehr lernen die kanten wieder zu minimieren, daher maximum 0.5???




Optional sampling with sample_bias and different loss: Name in Theory part of PGExplainer e.g. add additional formula for loss, or later in reimplementation or in attachments?


For litearture/PGExplainer theory: Active speech? -> "This is used in PGExplainer to allow for efficiently optimizing the objective function with gradient-based methods"

Verteidigung wann? 2-4 Wochen nach abgabe, gerne im Voraus absprechen

generative probabilistic model for graph data relevant in context of Gilbert random graph; Enough to mention random graph is a kind of generative probabilistic model for graph data or further explain?

How to evaluate wandb sweep?

To reparam trick:
$\omega_{ij} \in \mathbb{R}$ is the parameter/explainer logit for the corresponding edge (edge_ij == importance score?)


What does this mean?
Since $P(e_{ij} = 1) = \theta_{ij}$, choosing $\omega_{ij} = \log\frac{\theta_{ij}}{1-\theta_{ij}}$ leads to $\lim_{\tau\rightarrow 0}\hat{G}_s = G_s$


"K is the number of total sampled graphs" -> Does this mean all graphs in the training set and therefore the loss is defined for the complete training set?
Or is the loss/learning objective defined for each graph individually? -> Algorithm contains "k sampled graphs per each graph G(i)"

learning objective should be over all samples or per sample?


------------------------------

tikz graphs

How do I proceed: original PGExplainer was used in collective setting -> Trained on test data

Possible to compare their inductive setting results to their collective baseline?

We test in indictive setting, hard to compare. Newer Paper provides inductive results in the form of plots

"This can be understood as an extension of the theoretical background? Since the paper differs from the codebase and is imprecise about certain descriptions (see Holdijk), we aim to give a thorough introduction that regards everything."


Since sweeps take ages:
Correct approach to use general settings of PGE with different GNN architecture inductive.
-> SWEEP THIS -> BASELINE Best AUROC accuracies over 10 seeds for all datasets (roughly comparable to new PGE)

THEN evaluate effects of different selected nodes etc. on same parameters for comparison?
Comparisons have to be made on same set of hyperparams with only one thing changed!?

IF Downstream task changes -> Explainer has to be swept again, correct? Since internal calculations change/scale

=>

------------------------------

Rosenblatt for MLP idea?

TODO: WE ONLY CONSIDER UNDIRECTED GRAPHS IN THIS WORK, THEREFORE ADAPT GRAPH DEFINITIONS?!

e_{i,j} or e_{ij}?

$\hat{E}_c$. or $\mu_{\hat{e}_c}$