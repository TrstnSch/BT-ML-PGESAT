{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NeuroSAT\n",
    "import torch\n",
    "import pickle\n",
    "import explainer_NeuroSAT\n",
    "\n",
    "from pysat.solvers import Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opts = {\n",
    "        'out_dir': '/Users/trist/Documents/Bachelor-Thesis/NeuroSAT/test/files/data/dataset_train_10',\n",
    "        'logging': '/Users/trist/Documents/Bachelor-Thesis/NeuroSAT/test/files/log/dataset_train_10.log',\n",
    "        'n_pairs': 100,  # Anzahl der zu generierenden Paare\n",
    "        'min_n': 8,\n",
    "        'max_n': 8,\n",
    "        'p_k_2': 0.3,\n",
    "        'p_geo': 0.4,\n",
    "        'max_nodes_per_batch': 4000,\n",
    "        'one_pair': False,\n",
    "        'emb_dim': 128,\n",
    "        'iterations': 26,\n",
    "    }\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(opts['out_dir'], 'rb') as file:\n",
    "    data = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sub_problem 1: 53\n",
      "Sub_problem 1 clauses: [[-42, 47, -48, 46], [42, -47, 46, 43, -45], [-44, 42, -47, -41], [41, 42, 45, 48], [47, 43, -44, 42, 41], [48, -44, 46, 41], [-48, 42, 47], [-44, 41, -43], [42, 44, -48, -46, 43, 41, 47], [-47, -44, 45], [46, -42, 48, 43], [46, -43, 42, -48], [-44, -46, 41, 43, -42], [42, -44, -41, -47, -45], [46, -45, -41, -48], [-42, -46, 47], [-48, -45], [-46, -45, 44], [46, -48, -42, -43, 45, -44, 47, 41], [44, 48, 41], [47, 41, -46, -43, 42], [44, 46, 43, 47], [-46, -47, -45, 44, 42], [-41, -46, 48, -43], [48, -45, 44, -41, -47], [43, 44, -46, -42], [-47, -43, -46, -42, -44], [44, 46, 47], [-41, -42, 44, 45, 46, 43], [44, -41, -43, -47], [47, 42, -41], [-41, 48, 44], [-42, -46], [-44, 45, -43], [-44, -41], [-43, -44], [41, 42, 48, 43, 45, 47, -46, 44], [46, -42, -44, -41, 47, 48], [48, 44, -42, -45, -46, -41, -43], [-45, 41, 46], [43, 42, -41, 47], [-44, 46], [-43, 48, -45, 44, -47], [43, -45, 47, -46, -42], [47, 45, 41, 43, 48, -44, 42, 46], [46, 47, -44], [-43, -47, 48, 46, -44, 45, 41, -42], [43, 42, -41, -44, -46, -48, -47, 45], [42, 43, 45, 41], [42, 46, 45], [-44, -45, 41, 42], [-41, 47, -44, 46, -48], [-48, 44]]\n",
      "[-42, 47, -48, 46]\n",
      "[42, -47, 46, 43, -45]\n",
      "[-44, 42, -47, -41]\n",
      "[41, 42, 45, 48]\n",
      "[47, 43, -44, 42, 41]\n",
      "[48, -44, 46, 41]\n",
      "[-48, 42, 47]\n",
      "[-44, 41, -43]\n",
      "[42, 44, -48, -46, 43, 41, 47]\n",
      "[-47, -44, 45]\n",
      "[46, -42, 48, 43]\n",
      "[46, -43, 42, -48]\n",
      "[-44, -46, 41, 43, -42]\n",
      "[42, -44, -41, -47, -45]\n",
      "[46, -45, -41, -48]\n",
      "[-42, -46, 47]\n",
      "[-48, -45]\n",
      "[-46, -45, 44]\n",
      "[46, -48, -42, -43, 45, -44, 47, 41]\n",
      "[44, 48, 41]\n",
      "[47, 41, -46, -43, 42]\n",
      "[44, 46, 43, 47]\n",
      "[-46, -47, -45, 44, 42]\n",
      "[-41, -46, 48, -43]\n",
      "[48, -45, 44, -41, -47]\n",
      "[43, 44, -46, -42]\n",
      "[-47, -43, -46, -42, -44]\n",
      "[44, 46, 47]\n",
      "[-41, -42, 44, 45, 46, 43]\n",
      "[44, -41, -43, -47]\n",
      "[47, 42, -41]\n",
      "[-41, 48, 44]\n",
      "[-42, -46]\n",
      "[-44, 45, -43]\n",
      "[-44, -41]\n",
      "[-43, -44]\n",
      "[41, 42, 48, 43, 45, 47, -46, 44]\n",
      "[46, -42, -44, -41, 47, 48]\n",
      "[48, 44, -42, -45, -46, -41, -43]\n",
      "[-45, 41, 46]\n",
      "[43, 42, -41, 47]\n",
      "[-44, 46]\n",
      "[-43, 48, -45, 44, -47]\n",
      "[43, -45, 47, -46, -42]\n",
      "[47, 45, 41, 43, 48, -44, 42, 46]\n",
      "[46, 47, -44]\n",
      "[-43, -47, 48, 46, -44, 45, 41, -42]\n",
      "[43, 42, -41, -44, -46, -48, -47, 45]\n",
      "[42, 43, 45, 41]\n",
      "[42, 46, 45]\n",
      "[-44, -45, 41, 42]\n",
      "[-41, 47, -44, 46, -48]\n",
      "[-48, 44]\n",
      "Computed MUS:\n",
      "c MUS approx: 4 7 10 13 16 17 20 28 31 32 35 36 42 51 53 0\n",
      "------------------------------------\n",
      "lenght of computed MUS: 11\n",
      "c MUS approx: 4 17 29 32 38 40 49 65 72 79 0\n",
      "[4, 17, 29, 32, 38, 40, 49, 72, 79]\n",
      "79\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nprint(is_sat)\\n#print(stats)\\nprint(f\"Computed unsat core length: {len(unsat_core)}\")\\nprint(f\"Computed unsat core: {unsat_core}\")\\nprint(\"------------------------------------\")\\nprint(core_clauses)\\n\\n\\nutils.visualize_cnf_interactive(clauses_problem_0)\\n\\n\\n\\n# This is shifted, literals are indexed 10-19 instead of 11-20!!!!!\\n# positive literals match! First clause (71/70 in batch_edges) is [17, 12, -14, 20]. In batched batch_edges: 16-70, 11-70, 19-70 matches 17, 12, 20\\n# negative ones do not match!!!!!\\nprint(data[0].batch_edges)\\n\\nmask = utils.get_batch_mask(torch.tensor(data[0].batch_edges), batch_idx=current_batch_num, batch_size=10, n_variables=problemBatch.n_variables)\\n\\nmasked_batch_edges = data[0].batch_edges[mask]\\nprint(masked_batch_edges)\\n\\nprint(problemBatch.n_variables)\\nprint(problemBatch.n_literals)\\n\\n\\n\\n\\n# TODO: Create gt in shape of mask with 0 if edge pair not in core, 1 if edge pair in core!\\n# -> First clause in sub_problem = 70: 17,12,-14,20 in indexes: 16,11,1413,19 has edges 16-17, 11-70, 1413-70 and 19-70\\n# -> First clause in gt/core = 100 (subtract 100 because of assumption -> 0, first clause(70)) with content [17, 12, -14, 20]. Convert to edge indexes and map!\\n\\nunsat_core_edges = reversed_core - offset\\n# connect first element in unsat_core_edges to literals of first core_clauses\\n\\ngt = []\\nfor i, idx in enumerate(unsat_core_edges):\\n    literals = core_clauses[i]\\n    \\n    # TODO: Swap clauses_problem_0  with current problem?\\n    # This only works if all sub_problems have the same amount of clauses!! WRONG!\\n    clause = len(clauses_problem_0)*current_batch_num + unsat_core_edges[i]\\n    \\n    # Sum of problemBatch.n_clauses_per_batch[] before current_batch_num? Or count while calculating gt for data?\\n    #clause = problemBatch.n_clauses_per_batch[current_batch_num] + unsat_core_edges[i]\\n    \\n    for value in literals:\\n        value = value -1 if value >= 1 else problemBatch.n_variables - (value + 1)\\n        gt.append([value, clause])\\n        \\nmotif_size = len(gt)\\n\\nprint(torch.tensor(gt))\\n\\nprint(problemBatch.n_clauses_per_batch)\\n\\nempty_gt = torch.zeros(len(masked_batch_edges))\\n\\nprint(empty_gt.shape)\\n\\ntest = torch.isin(torch.tensor(masked_batch_edges), torch.tensor(gt))\\n\\n# We only need the right column of the isin tensor\\nclausesTest = test[:,1].int()\\nprint(clausesTest)\\n\\n\\nprint(problemBatch.n_variables)\\n\\n\\ngt_mask = []\\nfor i, content in enumerate(masked_batch_edges):\\n    if masked_batch_edges[i] in unsat_core_edes:\\n        gt_mask.append(1)\\n    else:\\n        gt_mask.append(0)\\n        \\ntest2 = torch.isin(torch.tensor(data[0].batch_edges), torch.tensor(gt))\\n\\n# We only need the right column of the isin tensor\\nallClausesTestgt = test2[:,1].int()\\n        \\n        \\npos = utils.visualize_edge_index_interactive(masked_batch_edges, clausesTest)\\npos2 = utils.visualize_edge_index_interactive(masked_batch_edges, clausesTest, \"test\", pos, motif_size)'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pysat.formula import WCNF\n",
    "from pysat.formula import CNF\n",
    "from pysat.examples.musx import MUSX\n",
    "from pysat.examples.optux import OptUx\n",
    "import utils\n",
    "\n",
    "problemBatch = data[0]\n",
    "\n",
    "current_batch_num = 5\n",
    "\n",
    "clauses_problem_0 = problemBatch.get_clauses_for_problem(current_batch_num)\n",
    "\n",
    "print(f\"Length of sub_problem 1: {len(clauses_problem_0)}\")\n",
    "# Sub problem 1 Contains literals 11-20, ...\n",
    "print(f\"Sub_problem 1 clauses: {clauses_problem_0}\")\n",
    "\n",
    "solver = Solver(name='m22')\n",
    "\n",
    "#for clause in clauses_problem_0:\n",
    "#    solver.add_clause(clause)\n",
    "\n",
    "# TODO: Change this according to length of literals\n",
    "offset = problemBatch.n_literals + 1\n",
    "\n",
    "# TODO: Assumptions must be unique! + 100 will not work in batch as there exists clause 100...\n",
    "assumptions = [i + offset for i in range(len(clauses_problem_0))]\n",
    "\n",
    "# Add the clauses with selector literals\n",
    "for i, clause in enumerate(clauses_problem_0):\n",
    "    solver.add_clause(clause + [-assumptions[i]])  # Each clause gets a unique assumption\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "is_sat = solver.solve(assumptions=assumptions)\n",
    "\n",
    "\n",
    "# TODO: THIS UNSAT CORE REQUIRES AN ASSUMPTION!!! THEREFORE NOT SUITED FOR PGEXPLAINER?! -> Bypass with additional assumption per clause?\n",
    "unsat_core = solver.get_core()\n",
    "\n",
    "reversed_core = torch.tensor(unsat_core[::-1])\n",
    "\n",
    "# TODO: looks like unsat_core is in reverse order, flip?\n",
    "# Last element of unsat core is first clause in core_clauses\n",
    "\n",
    "stats = solver.accum_stats()\n",
    "    \n",
    "# Map back to original clauses\n",
    "core_clauses = [clauses_problem_0[i] for i in range(len(clauses_problem_0)) if assumptions[i] in unsat_core]\n",
    "    \n",
    "    \n",
    "cnf = CNF()\n",
    "for i, clause in enumerate(clauses_problem_0):\n",
    "    print(clause)\n",
    "    cnf.append(clause)\n",
    "\n",
    "print(\"Computed MUS:\")\n",
    "#print(cnf.to_dimacs())\n",
    "# Compute Minimally Unsatisfiable Subformulas instead of UNSAT cores? No assumptions needed?\n",
    "musx = MUSX(cnf)\n",
    "mus = musx.compute()\n",
    "print(\"------------------------------------\")\n",
    "print(f\"lenght of computed MUS: {len(mus)}\")\n",
    "\n",
    "\"\"\"with OptUx(cnf) as optux:\n",
    "    for mus in optux.enumerate():\n",
    "        print('mus {0} has cost {1}'.format(mus, optux.cost))\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# TODO: USE MUS INSTEAD OF UNSAT CORE\n",
    "# Computed mus contains the relative clause numbers for the current problem -> Need to be mapped to edges of original clause\n",
    "\n",
    "eval_problem = data[0]\n",
    "sub_problem_start_clause = 0\n",
    "\n",
    "current_batch_num = 12\n",
    "\n",
    "clauses_problem_i = problemBatch.get_clauses_for_problem(current_batch_num)\n",
    "\n",
    "cnf = CNF()\n",
    "for i, clause in enumerate(clauses_problem_i):\n",
    "    #print(clause)\n",
    "    cnf.append(clause)\n",
    "\n",
    "musx = MUSX(cnf)\n",
    "# mus contains list of relative clause numbers for the current problem e.g. [4, 7, ..., 53], starting at 1\n",
    "mus = musx.compute()\n",
    "\n",
    "# Map mus list to \n",
    "eval_batch_mask = utils.get_batch_mask(torch.tensor(eval_problem.batch_edges), batch_idx=current_batch_num, batch_size=opts['min_n'], n_variables=eval_problem.n_variables)\n",
    "\n",
    "masked_batch_edges = eval_problem.batch_edges[eval_batch_mask]\n",
    "\n",
    "\n",
    "print(mus)\n",
    "print(len(torch.unique(torch.tensor(masked_batch_edges[:, 1]))))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "solver.delete()\n",
    "\"\"\"\n",
    "print(is_sat)\n",
    "#print(stats)\n",
    "print(f\"Computed unsat core length: {len(unsat_core)}\")\n",
    "print(f\"Computed unsat core: {unsat_core}\")\n",
    "print(\"------------------------------------\")\n",
    "print(core_clauses)\n",
    "\n",
    "\n",
    "utils.visualize_cnf_interactive(clauses_problem_0)\n",
    "\n",
    "\n",
    "\n",
    "# This is shifted, literals are indexed 10-19 instead of 11-20!!!!!\n",
    "# positive literals match! First clause (71/70 in batch_edges) is [17, 12, -14, 20]. In batched batch_edges: 16-70, 11-70, 19-70 matches 17, 12, 20\n",
    "# negative ones do not match!!!!!\n",
    "print(data[0].batch_edges)\n",
    "\n",
    "mask = utils.get_batch_mask(torch.tensor(data[0].batch_edges), batch_idx=current_batch_num, batch_size=10, n_variables=problemBatch.n_variables)\n",
    "\n",
    "masked_batch_edges = data[0].batch_edges[mask]\n",
    "print(masked_batch_edges)\n",
    "\n",
    "print(problemBatch.n_variables)\n",
    "print(problemBatch.n_literals)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# TODO: Create gt in shape of mask with 0 if edge pair not in core, 1 if edge pair in core!\n",
    "# -> First clause in sub_problem = 70: 17,12,-14,20 in indexes: 16,11,1413,19 has edges 16-17, 11-70, 1413-70 and 19-70\n",
    "# -> First clause in gt/core = 100 (subtract 100 because of assumption -> 0, first clause(70)) with content [17, 12, -14, 20]. Convert to edge indexes and map!\n",
    "\n",
    "unsat_core_edges = reversed_core - offset\n",
    "# connect first element in unsat_core_edges to literals of first core_clauses\n",
    "\n",
    "gt = []\n",
    "for i, idx in enumerate(unsat_core_edges):\n",
    "    literals = core_clauses[i]\n",
    "    \n",
    "    # TODO: Swap clauses_problem_0  with current problem?\n",
    "    # This only works if all sub_problems have the same amount of clauses!! WRONG!\n",
    "    clause = len(clauses_problem_0)*current_batch_num + unsat_core_edges[i]\n",
    "    \n",
    "    # Sum of problemBatch.n_clauses_per_batch[] before current_batch_num? Or count while calculating gt for data?\n",
    "    #clause = problemBatch.n_clauses_per_batch[current_batch_num] + unsat_core_edges[i]\n",
    "    \n",
    "    for value in literals:\n",
    "        value = value -1 if value >= 1 else problemBatch.n_variables - (value + 1)\n",
    "        gt.append([value, clause])\n",
    "        \n",
    "motif_size = len(gt)\n",
    "\n",
    "print(torch.tensor(gt))\n",
    "\n",
    "print(problemBatch.n_clauses_per_batch)\n",
    "\n",
    "empty_gt = torch.zeros(len(masked_batch_edges))\n",
    "\n",
    "print(empty_gt.shape)\n",
    "\n",
    "test = torch.isin(torch.tensor(masked_batch_edges), torch.tensor(gt))\n",
    "\n",
    "# We only need the right column of the isin tensor\n",
    "clausesTest = test[:,1].int()\n",
    "print(clausesTest)\n",
    "\n",
    "\n",
    "print(problemBatch.n_variables)\n",
    "\n",
    "\n",
    "gt_mask = []\n",
    "for i, content in enumerate(masked_batch_edges):\n",
    "    if masked_batch_edges[i] in unsat_core_edes:\n",
    "        gt_mask.append(1)\n",
    "    else:\n",
    "        gt_mask.append(0)\n",
    "        \n",
    "test2 = torch.isin(torch.tensor(data[0].batch_edges), torch.tensor(gt))\n",
    "\n",
    "# We only need the right column of the isin tensor\n",
    "allClausesTestgt = test2[:,1].int()\n",
    "        \n",
    "        \n",
    "pos = utils.visualize_edge_index_interactive(masked_batch_edges, clausesTest)\n",
    "pos2 = utils.visualize_edge_index_interactive(masked_batch_edges, clausesTest, \"test\", pos, motif_size)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36846, 2)\n",
      "1440\n",
      "tensor([ True,  True,  True,  ..., False, False, False])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'for i in data[0].batch_edges:\\n    if i[0] in []'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data[0].batch_edges.shape)\n",
    "\n",
    "print(data[0].n_variables)\n",
    "\n",
    "batch = []\n",
    "currentBatch = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "problem = data[0]\n",
    "n_variables = problem.n_variables\n",
    "\n",
    "batch_edges = torch.tensor(problem.batch_edges)           # Shape: (num_edges, 2)\n",
    "\n",
    "batch_literals = torch.cat([\n",
    "    torch.arange(0, 40),                   # Positive literals\n",
    "    torch.arange(n_variables, n_variables + 40)  # Negative literals\n",
    "])\n",
    "\n",
    "batch_mask = torch.isin(batch_edges[:, 0], batch_literals)\n",
    "\n",
    "# Apply the mask\n",
    "batch_edges_filtered = batch_edges[batch_mask]\n",
    "\n",
    "\n",
    "print(batch_mask)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"for i in data[0].batch_edges:\n",
    "    if i[0] in []\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(36846,)\n",
      "torch.Size([36846])\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "batch_idx = 2  # Get mask for the 3rd batch (0-based index)\n",
    "batch_mask = get_batch_mask(torch.tensor(problem.batch_edges), batch_idx)\n",
    "\n",
    "# Apply the mask to filter edges\n",
    "batch_edges_filtered = problem.batch_edges[batch_mask]\n",
    "\n",
    "print(problem.batch_edges[:,0].shape)\n",
    "print(batch_mask.shape)\n",
    "#print(torch.tensor(batch_edges_filtered))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[[-1, -14, 31], [-35, 19, -3, 37], [19, 22, -6, -2, 38, -31, -37, -32], [7, 28, 32], [2, -29], [-14, 4, 2, -31], [19, 17, 13, 10, 15], [-21, 10], [-35, -27, 17, 12, -6], [-35, 26], [27, -30, -15], [26, 10, -15, 18, 30, 13], [-30, -7, -1, 3, 28], [5, 1, 39, -14, 40], [35, 18, -6], [-31, -28, 9, -2, -20, 35, -14, 10, 5], [4, -40, 27], [19, 10], [35, 20, -10, 14, -18, 39, -30, 1, -27], [-19, 38, 31, -20], [20, -22, -23, -7], [-14, 4, 22], [14, 19, 17, 15, 21], [-30, -26, 38], [21, 24, -23], [-22, 8, 20, -3, -31, 36, -29, -34, 19], [-30, 5, -37, 13, -12, -11, 40, 33, 19, 10], [-34, -40], [33, 29, -40, 28, -10, -14, 35, -32], [20, 12, 38, -13, 31], [19, 7, -12, 13, -34, 8], [1, 19], [7, 5, -39, 35, 23, 11, -27], [5, 8, 31, -17, -39, -25], [34, -32, -21, 4, -24, 35, 19, -5, 8, -28], [27, 15, -25, -23, -16, -38, -13], [37, -33, 15, -18, 28, -38], [-4, 34, -7, -40], [14, -33], [-6, 2, -19, -16], [32, -34, -33, -9], [-20, -31, 16, 18, 34], [9, 20, 4], [-18, -9, -30], [11, 24, 34], [23, 3, -2, -12], [-8, -24, 7, 25, 33, 37], [-22, -12, -38], [-14, -39, -32, -7, 21, 40], [4, -5, 25], [11, -13, -29], [-34, -37, 32], [40, 28, -14, 34, -8, 31, -15], [-24, -22, 20, 12, 25, -14, 17, 6, -36], [-5, 24, -33, -11, -10, 34], [36, -16, -13, 23], [14, -3, -18, -35], [6, -30, 28, -35, -12], [38, -11, -12, -8], [29, -9, -30], [-19, 22, -3, -38], [29, -35, -33, -9, -32, 24, 15], [20, 38, -34], [27, 12, 37, 29, -8, -15], [31, 37, -19, 29, 30, 17, 2, -25], [-20, -35], [3, 17, 18, 4, -1, -36, -21, -19, -20, -25, -33, -34, 24, 15, -39, -11, 5], [8, 30, 2], [19, 35, 27, 7, 3, 32, 39, 31, 21], [25, -18, 28], [-28, 18, -32, 11, 29, -8, 27], [5, -33, -12, 19, 20, -35], [2, -20, 19], [20, 27, 10, -28, 33], [-14, 4, 3, 40, -33], [16, -27, 9, -19, 24], [18, 36, 37, -31, 34, 29, -33], [-39, 12, 1, 11, -29, -22], [-14, -24, 3, -11], [-12, 14, -38], [9, -22, -32, 37, 10, -24], [-7, 18, 12], [-6, 2, 16, 25], [7, -10], [8, 39, 1, -29, 21], [-20, 11, -3, -27, -30, -33], [-24, 15, -8], [-30, 2, -7, 35, 39], [-11, -1], [-14, 9, 32, 15], [-12, -23, 34], [35, 12, -14], [35, 10, -12, 39], [-14, 37, 9], [-11, -39, 7, 19], [-25, -40, 23], [16, 17, -2, 36, -11, -35, -30, -13, -40, 22, 34, -7, -26, 20], [-29, -16], [33, -18, 14, 26], [-20, -17, 31], [-6, -17, -19, 10, -35, 34, 16], [-27, -23, -9, 8, 20, 29], [-6, -18], [2, 9, -40, -10, 36, -38], [4, 25, 12], [2, -37, -22], [32, -11], [38, 26, 36, 29, 15, -25], [-36, 22], [-17, 21, 40], [-26, -12, 35], [18, -1, 30], [-12, 17], [22, 7, 40], [37, 31, 35, 33], [32, -10], [38, -28, -33, 18, 3], [12, 20, 14, 34, -31, -2], [-1, 8, -40], [21, -40, -4, -12], [33, 31, -5, 38, 1], [-13, 22, -1], [-21, 30, 4], [-15, -13, 30, -27, 29, 21], [36, 29, 26, -21], [-28, 35, 27, 21], [-7, 39, 34, -4], [5, -9, 25, 40], [31, -15, 24], [16, -37, -21], [17, 23, 33, -40, -8], [-37, 26, -13, 35], [30, 16, -37], [26, -27, -31], [-39, 24], [-12, 22, -25, 37], [40, -34, -10, -39], [-11, 12, 8], [15, 1, -23, 4, -26, 11], [7, 23, -12, 17, 34], [18, -2, 32, -14, -6, 25, 10, -37], [17, -2, 16, 26, -27, 10, -37, -21, -24, 20, 40, 31], [-19, -25, -6], [-24, -35, 21, -30, -7, 20, -19], [4, -25, -28], [-30, 12, -39], [7, 23, -8], [-16, 10, 29, 25, -15, -36], [5, -28, -16], [17, -27, 9], [-32, -16, -33, -13, -14], [19, 25, 30], [-30, 29, 22, 31, 28, -3], [-4, 32, 25, -19], [40, 14, 13, -27, -32], [-28, 16, -9], [-10, -16, 32, 19], [-35, -30, -10, 23], [3, -32, -11, 23, 34, -30, 7, -9], [-13, -8, -24], [16, -15, 23, -1, -40], [28, -20, -31, 1, -4, -11], [3, 2, 7], [39, 17, 18], [27, 4, -15, -23], [-38, 36, -6, -2, -19, 24], [38, 29, 25], [-9, -29, -37], [25, -27, 21, -1, -10], [21, -33, -32, -6, -31, 14, -29], [32, 19, -37, 15], [-7, -38, 22, -29, 20], [-14, 6], [-15, -14, -25], [-34, -6, -39], [-11, 27, -7, -29], [-6, -21, -16, 20, -39, -35, -38], [-34, -30], [3, 39, -12, -27], [31, -3, 7, 29, 28], [9, -30, -17, -27, 31], [13, 35, -12, 28, -27, 16, -34, -7], [33, -9, 29, -3, -15, 25], [-15, -17, -24], [22, -20], [-30, 15, -9], [27, 18, 24, 29, -14, 5], [16, -10, 35, -7], [-4, -3, 2], [-5, -9, -18], [-14, -6], [-29, 1, -8, -10], [31, 39, -20, 36, 9, 17], [-10, -30, 25], [35, 13, 16], [-31, -39, -11], [-23, -14, 3], [1, -5, -40], [-15, 8, -13, 35], [-8, 27, -32, 9, 4], [12, 36], [30, 1, -40, 16], [12, -11], [-23, -25, 10, -35, -8], [-12, 10, -23], [32, 22, -11], [-20, 38], [-16, 5], [-24, -35, 11, -36, 6, 21, 40], [17, 1, 2], [-39, 23], [-39, -22, -9, -24], [-6, -23, -33, -3], [1, 30, -4, 35], [-23, -31], [-5, 27, 40], [3, -26, 31, 30], [-3, 15, 39, 6, -5, 21], [-3, -14, -2], [14, -9, -31, 4, -40], [37, -28, -29], [11, -22, -9, -32], [-27, 40, 24, -32, -12], [-9, 2, -13, 40, 26, -23, -20, 27, -38, 25], [9, 21, 32], [-3, 28, -18, 2], [17, -36, -8], [4, 19, -31], [31, -16, 39], [7, -19, 29, -30], [-15, -5, 25], [-20, 24, -29, -5, 40, -36, 39, -23, 26], [38, 20], [-29, -33, -2, 6, 10, -11], [-35, 7], [-30, -14, -1], [10, -7, 20], [-40, 3, -23, 33, 25], [17, -25, 37], [-4, -37, 15], [31, 26, 8, 1, -35], [30, 21, -20], [31, -34, 17], [-39, 4, -7], [3, 34, -33, -37], [-40, -33, 36], [-7, 2, -1, 26, -39, 33], [20, -40, -18, -35, 13, 25, 23, 27], [-24, -40, 14], [23, -2, -27, -26, -14, 3, 32, -34, -4], [-18, 4, 23], [-21, -5, 15, -18], [-26, 5, 12], [-26, 20, -36, -16, -27, -9, -1], [-12, 24, -34], [-41, -54, 71], [-75, 59, -43, 77], [59, 62, -46, -42, 78, -71, -77, -72], [47, 68, 72], [42, -69], [-54, 44, 42, -71], [59, 57, 53, 50, 55], [-61, 50], [-75, -67, 57, 52, -46], [-75, 66], [67, -70, -55], [66, 50, -55, 58, 70, 53], [-70, -47, -41, 43, 68], [45, 41, 79, -54, 80], [75, 58, -46], [-71, -68, 49, -42, -60, 75, -54, 50, 45], [44, -80, 67], [59, 50], [75, 60, -50, 54, -58, 79, -70, 41, -67], [-59, 78, 71, -60], [60, -62, -63, -47], [-54, 44, 62], [54, 59, 57, 55, 61], [-70, -66, 78], [61, 64, -63], [-62, 48, 60, -43, -71, 76, -69, -74, 59], [-70, 45, -77, 53, -52, -51, 80, 73, 59, 50], [-74, -80], [73, 69, -80, 68, -50, -54, 75, -72], [60, 52, 78, -53, 71], [59, 47, -52, 53, -74, 48], [41, 59], [47, 45, -79, 75, 63, 51, -67], [45, 48, 71, -57, -79, -65], [74, -72, -61, 44, -64, 75, 59, -45, 48, -68], [67, 55, -65, -63, -56, -78, -53], [77, -73, 55, -58, 68, -78], [-44, 74, -47, -80], [54, -73], [-46, 42, -59, -56], [72, -74, -73, -49], [-60, -71, 56, 58, 74], [49, 60, 44], [-58, -49, -70], [51, 64, 74], [63, 43, -42, -52], [-48, -64, 47, 65, 73, 77], [-62, -52, -78], [-54, -79, -72, -47, 61, 80], [44, -45, 65], [51, -53, -69], [-74, -77, 72], [80, 68, -54, 74, -48, 71, -55], [-64, -62, 60, 52, 65, -54, 57, 46, -76], [-45, 64, -73, -51, -50, 74], [76, -56, -53, 63], [54, -43, -58, -75], [46, -70, 68, -75, -52], [78, -51, -52, -48], [69, -49, -70], [-59, 62, -43, -78], [69, -75, -73, -49, -72, 64, 55], [60, 78, -74], [67, 52, 77, 69, -48, -55], [71, 77, -59, 69, 70, 57, 42, -65], [-60, -75], [43, 57, 58, 44, -41, -76, -61, -59, -60, -65, -73, -74, 64, 55, -79, -51, 45], [48, 70, 42], [59, 75, 67, 47, 43, 72, 79, 71, 61], [65, -58, 68], [-68, 58, -72, 51, 69, -48, 67], [45, -73, -52, 59, 60, -75], [42, -60, 59], [60, 67, 50, -68, 73], [-54, 44, 43, 80, -73], [56, -67, 49, -59, 64], [58, 76, 77, -71, 74, 69, -73], [-79, 52, 41, 51, -69, -62], [-54, -64, 43, -51], [-52, 54, -78], [49, -62, -72, 77, 50, -64], [-47, 58, 52], [-46, 42, 56, 65], [47, -50], [48, 79, 41, -69, 61], [-60, 51, -43, -67, -70, -73], [-64, 55, -48], [-70, 42, -47, 75, 79], [-51, -41], [-54, 49, 72, 55], [-52, -63, 74], [75, 52, -54], [75, 50, -52, 79], [-54, 77, 49], [-51, -79, 47, 59], [-65, -80, 63], [56, 57, -42, 76, -51, -75, -70, -53, -80, 62, 74, -47, -66, 60], [-69, -56], [73, -58, 54, 66], [-60, -57, 71], [-46, -57, -59, 50, -75, 74, 56], [-67, -63, -49, 48, 60, 69], [-46, -58], [42, 49, -80, -50, 76, -78], [44, 65, 52], [42, -77, -62], [72, -51], [78, 66, 76, 69, 55, -65], [-76, 62], [-57, 61, 80], [-66, -52, 75], [58, -41, 70], [-52, 57], [62, 47, 80], [77, 71, 75, 73], [72, -50], [78, -68, -73, 58, 43], [52, 60, 54, 74, -71, -42], [-41, 48, -80], [61, -80, -44, -52], [73, 71, -45, 78, 41], [-53, 62, -41], [-61, 70, 44], [-55, -53, 70, -67, 69, 61], [76, 69, 66, -61], [-68, 75, 67, 61], [-47, 79, 74, -44], [45, -49, 65, 80], [71, -55, 64], [56, -77, -61], [57, 63, 73, -80, -48], [-77, 66, -53, 75], [70, 56, -77], [66, -67, -71], [-79, 64], [-52, 62, -65, 77], [80, -74, -50, -79], [-51, 52, 48], [55, 41, -63, 44, -66, 51], [47, 63, -52, 57, 74], [58, -42, 72, -54, -46, 65, 50, -77], [57, -42, 56, 66, -67, 50, -77, -61, -64, 60, 80, 71], [-59, -65, -46], [-64, -75, 61, -70, -47, 60, -59], [44, -65, -68], [-70, 52, -79], [47, 63, -48], [-56, 50, 69, 65, -55, -76], [45, -68, -56], [57, -67, 49], [-72, -56, -73, -53, -54], [59, 65, 70], [-70, 69, 62, 71, 68, -43], [-44, 72, 65, -59], [80, 54, 53, -67, -72], [-68, 56, -49], [-50, -56, 72, 59], [-75, -70, -50, 63], [43, -72, -51, 63, 74, -70, 47, -49], [-53, -48, -64], [56, -55, 63, -41, -80], [68, -60, -71, 41, -44, -51], [43, 42, 47], [79, 57, 58], [67, 44, -55, -63], [-78, 76, -46, -42, -59, 64], [78, 69, 65], [-49, -69, -77], [65, -67, 61, -41, -50], [61, -73, -72, -46, -71, 54, -69], [72, 59, -77, 55], [-47, -78, 62, -69, 60], [-54, 46], [-55, -54, -65], [-74, -46, -79], [-51, 67, -47, -69], [-46, -61, -56, 60, -79, -75, -78], [-74, -70], [43, 79, -52, -67], [71, -43, 47, 69, 68], [49, -70, -57, -67, 71], [53, 75, -52, 68, -67, 56, -74, -47], [73, -49, 69, -43, -55, 65], [-55, -57, -64], [62, -60], [-70, 55, -49], [67, 58, 64, 69, -54, 45], [56, -50, 75, -47], [-44, -43, 42], [-45, -49, -58], [-54, -46], [-69, 41, -48, -50], [71, 79, -60, 76, 49, 57], [-50, -70, 65], [75, 53, 56], [-71, -79, -51], [-63, -54, 43], [41, -45, -80], [-55, 48, -53, 75], [-48, 67, -72, 49, 44], [52, 76], [70, 41, -80, 56], [52, -51], [-63, -65, 50, -75, -48], [-52, 50, -63], [72, 62, -51], [-60, 78], [-56, 45], [-64, -75, 51, -76, 46, 61, 80], [57, 41, 42], [-79, 63], [-79, -62, -49, -64], [-46, -63, -73, -43], [41, 70, -44, 75], [-63, -71], [-45, 67, 80], [43, -66, 71, 70], [-43, 55, 79, 46, -45, 61], [-43, -54, -42], [54, -49, -71, 44, -80], [77, -68, -69], [51, -62, -49, -72], [-67, 80, 64, -72, -52], [-49, 42, -53, 80, 66, -63, -60, 67, -78, 65], [49, 61, 72], [-43, 68, -58, 42], [57, -76, -48], [44, 59, -71], [71, -56, 79], [47, -59, 69, -70], [-55, -45, 65], [-60, 64, -69, -45, 80, -76, 79, -63, 66], [78, 60], [-69, -73, -42, 46, 50, -51], [-75, 47], [-70, -54, -41], [50, -47, 60], [-80, 43, -63, 73, 65], [57, -65, 77], [-44, -77, 55], [71, 66, 48, 41, -75], [70, 61, -60], [71, -74, 57], [-79, 44, -47], [43, 74, -73, -77], [-80, -73, 76], [-47, 42, -41, 66, -79, 73], [60, -80, -58, -75, 53, 65, 63, 67], [-64, -80, 54], [63, -42, -67, -66, -54, 43, 72, -74, -44], [-58, 44, 63], [-61, -45, 55, -58], [-66, 45, 52], [-66, 60, -76, -56, -67, -49, -41], [52, 64, -74], [107, -120, -93], [-93, 86, -104, -95, -117, -99, -81, -82, -113], [-115, 87, -105, -100], [-111, -88, -96], [-109, -104, 84, 88, 116], [-117, -85, -96, -106], [-87, 83, -115, -101, 118, -119], [120, 104, -107], [-106, -110, -98, -120], [119, -113], [-98, -112, -107, -119, 83], [-84, -104, 83], [117, -120], [105, -82, -114], [-111, -104, 91], [-95, -84, -89], [-115, -118, -83, 109, -114, -82, 95, 119, -89], [-108, 88], [-112, 117], [-118, -86, -115, 92, -111, -106], [-83, 110, -118, -98, -105], [-112, -96, 90], [-100, 88, -119, 114, -111, 90, 118, -85, 102], [-109, -117, 98, 95, -87, -106, -91, 114, 84], [-87, -116, 85, -94], [-88, -91, -84, -92, 113], [84, 113, 117], [-91, 97], [103, 107, 108, 83], [92, 90, 91], [86, 111], [81, 88, -83, 104, -100, 103, -119, -102, -93, -91], [88, -95, 89, -85], [86, 82, 120, -87], [93, 108, 109], [81, 115], [85, -96, -90], [96, -104, 110], [82, 97, 89, -114, -119], [97, 92, -95], [-96, 103, 117, -84, -100], [94, -85, -103], [108, 86, 99], [89, -115, -96, 100], [81, -89, -103], [-91, 114, 104, 101, -112], [93, 95, 118, -116, 107, 105, -82, -98], [-117, 103, 89, 105], [120, -119, 84, -90], [-102, 83, 117, -85, 100], [-89, 117, 95, 108, -107, 91, 109], [-109, -106, 96, -95, 93], [-118, 112, 103, 86], [115, -96, -119], [-97, 108, 81, -83, -93], [-84, 86, 113, 98, 109], [-95, 118, 81, 116], [-103, -101], [98, 116, -103], [-92, 110, 113, 97], [-110, 81, -118], [-81, -115, 101, 82, 119], [-99, -120, 83, -108], [99, -101, -109, 98], [-91, -114, 115, -89, 82, 81, 87, -96, 90], [119, -96, 93], [-110, 114, 92, 115, -119, -84, 88], [-107, 102, 100, 101, -109, -92, 104, -88, -115, -120], [114, -119], [-95, 84, 117], [91, 89, -114, 119], [-82, -89, 113, 96], [-101, 107, -113, 83, 112, -91, 118], [-96, 112, 98, 93], [-84, 89, 88, -100, -101, 98, -95, 107], [-100, -101, 96], [-100, -85, -98, 94, 97], [83, -115, 92, -90, -100, -109], [82, -114, 115], [82, 86, 96], [-104, 81, -87, -116, 102, -100, -96, 103, -85, -110, -101], [-115, 83], [107, -85, -112, 97, 100], [-109, -101, -115, -94, 84], [-115, 87], [92, 108, 89, -98], [94, 119, -101, -82, 93, -86, 104], [-86, -98, -87, -105], [116, 107, -100], [109, -89, 82], [87, 98, -86], [94, 102, -90, 84, 100, 86, -104, -88, -115, -96, -92], [87, 114, -89, 99, 82], [-97, -118, -114, -87, 100, 116], [-105, 110, 94], [93, -94, -92], [83, 90, -101, 107, -84, 117, -103, -113, 82], [107, 111, -119], [120, -114, -93, -107], [-97, -107, -105], [-101, 95, 110], [98, 103, -97], [97, -105, -93, -96, 120, 102, -119, 98, -86, 108], [105, -83, -103], [-84, 103, -117, -96, -106], [107, 94, 99, -113, -112, -90, 116, 91, 81, -101], [115, 109, 92, -85], [103, -116, -115], [-88, -85, -89], [-87, -84], [101, 95, 107], [-96, 119], [-95, -90], [-84, -87, 82, -100], [86, -93, 104, -89], [120, 117, -101, -102], [-103, -97, -96, 95], [-96, -100, 90, 84, 119], [-106, -81, 112], [99, -120, 93], [-95, -116, 103, 85, -108, 118, -114, 98, -100], [-87, 93], [103, 117, 102, 95, 85, 81, -86], [100, -118, 116], [-109, 119, 86, 116, -89, 120], [-104, -110, -87], [-86, -103, -99], [-115, 111, 108], [114, -96, 103, -106], [91, 111], [-120, 103, -118, -97, -95], [117, -91, 84], [-95, -99, -91, 90, 83, 81, 115], [106, -108, -111], [-89, -98, -116, -102, 94, -97, -118, -113, 103, -88], [89, 88], [-87, 104, -113, -96, 120, 94, -111], [-92, 115, -118, 87], [91, 104, 105], [-89, -107, -96, -94, -93], [-111, -103, -119], [83, -118, 94], [-114, 120, 93, -95], [-109, -120, -88], [-120, -115], [109, 100], [-111, -105, 108, 89], [-120, -95, -117], [116, 112, 88], [116, -106, -102], [119, -118], [83, -104, -103], [82, -83], [-97, -88, 86], [-95, 93, -117], [85, -107, -96], [-101, -94, 88], [91, 88], [-88, 106, 114, 95], [-93, 120, 89], [-108, -81], [-95, 88, -107, 83, 91, -99, -115], [-118, 81, 111], [-94, 110, 106, 96], [-120, 84, 105, 117], [-116, 97, 98], [92, 110, -99], [105, 107, 86, -89, -113, -100], [-88, -90, -116], [120, -108, 109, -116, 107], [-97, -113, -92], [95, -111, 83, -106], [-98, -94, -81, -113, -110], [-86, 90, -115], [108, -112, -107, 86, -114], [87, -97, 90], [102, -112, 99], [82, -83, 111, 117, -113, 110], [-117, -106, 110], [-102, -112, 103, 90, -104], [-115, 91, -97, 83, -84, -90, -81, 87], [-100, -85, 117], [-89, -97, 112, 115, -105], [112, -107, 83, 115], [-104, 106, 101, 89], [108, 115, -96, 94, 103, 98, 114, -95, 83], [-110, -83, 99], [-111, -96, -114, 117], [-115, -94, -82, 83, 113], [-100, -115, 82], [-82, -83, -92], [-114, 110, 119, 85, 84, -87], [82, 84, 101, 81], [-102, 85, 84, -83], [-105, -120, -102, 106, -113], [95, 105, 117, 94, -99], [-113, -116, 118, 120], [114, 94, 98], [95, -115, 106, -120], [92, -119, -82], [-82, -81, -87, -85, 86, 114, -83, 106, 113], [83, 112, -115, 114, -89, 101, -97, -109], [95, -83, 105, -107, -110], [98, -100, 84, -111, -112, 101, 108, -83], [-119, 116, -118], [-98, 86, 103, -112], [-112, -94, -90, -103], [100, 105, -109, 81, -96, -107, -94], [-88, 85, 98], [104, -115, -108, 106, 97], [84, -119, -97], [-101, 83, -115], [-100, 114], [-86, 82, 104], [-90, -100, -106], [90, 101, -107, -83, 119], [-119, 88], [87, 93, 106], [-91, 86, 108], [-84, 92, -118, 104, 113, -89, 81, 115, 105], [91, 118, -108, -94, 115, 93], [-113, -103, 101], [-113, -115, -114], [-89, 102, 97], [82, 118, -113, 104], [-88, 83, 92, -89], [85, -110], [85, -101], [97, -91, -96, 86, -84, 113, -105, -108], [91, -90, -111, 83, 109, -92], [102, 87, -90, -108], [106, 105], [-118, 83, -113, 89, -111, 112, -115], [101, 109, -82, -88, -93, 104], [-98, -100, 108, 107], [99, -89, 98], [117, 114], [147, -160, -133], [-133, 126, -144, -135, -157, -139, -121, -122, -153], [-155, 127, -145, -140], [-151, -128, -136], [-149, -144, 124, 128, 156], [-157, -125, -136, -146], [-127, 123, -155, -141, 158, -159], [160, 144, -147], [-146, -150, -138, -160], [159, -153], [-138, -152, -147, -159, 123], [-124, -144, 123], [157, -160], [145, -122, -154], [-151, -144, 131], [-135, -124, -129], [-155, -158, -123, 149, -154, -122, 135, 159, -129], [-148, 128], [-152, 157], [-158, -126, -155, 132, -151, -146], [-123, 150, -158, -138, -145], [-152, -136, 130], [-140, 128, -159, 154, -151, 130, 158, -125, 142], [-149, -157, 138, 135, -127, -146, -131, 154, 124], [-127, -156, 125, -134], [-128, -131, -124, -132, 153], [124, 153, 157], [-131, 137], [143, 147, 148, 123], [132, 130, 131], [126, 151], [121, 128, -123, 144, -140, 143, -159, -142, -133, -131], [128, -135, 129, -125], [126, 122, 160, -127], [133, 148, 149], [121, 155], [125, -136, -130], [136, -144, 150], [122, 137, 129, -154, -159], [137, 132, -135], [-136, 143, 157, -124, -140], [134, -125, -143], [148, 126, 139], [129, -155, -136, 140], [121, -129, -143], [-131, 154, 144, 141, -152], [133, 135, 158, -156, 147, 145, -122, -138], [-157, 143, 129, 145], [160, -159, 124, -130], [-142, 123, 157, -125, 140], [-129, 157, 135, 148, -147, 131, 149], [-149, -146, 136, -135, 133], [-158, 152, 143, 126], [155, -136, -159], [-137, 148, 121, -123, -133], [-124, 126, 153, 138, 149], [-135, 158, 121, 156], [-143, -141], [138, 156, -143], [-132, 150, 153, 137], [-150, 121, -158], [-121, -155, 141, 122, 159], [-139, -160, 123, -148], [139, -141, -149, 138], [-131, -154, 155, -129, 122, 121, 127, -136, 130], [159, -136, 133], [-150, 154, 132, 155, -159, -124, 128], [-147, 142, 140, 141, -149, -132, 144, -128, -155, -160], [154, -159], [-135, 124, 157], [131, 129, -154, 159], [-122, -129, 153, 136], [-141, 147, -153, 123, 152, -131, 158], [-136, 152, 138, 133], [-124, 129, 128, -140, -141, 138, -135, 147], [-140, -141, 136], [-140, -125, -138, 134, 137], [123, -155, 132, -130, -140, -149], [122, -154, 155], [122, 126, 136], [-144, 121, -127, -156, 142, -140, -136, 143, -125, -150, -141], [-155, 123], [147, -125, -152, 137, 140], [-149, -141, -155, -134, 124], [-155, 127], [132, 148, 129, -138], [134, 159, -141, -122, 133, -126, 144], [-126, -138, -127, -145], [156, 147, -140], [149, -129, 122], [127, 138, -126], [134, 142, -130, 124, 140, 126, -144, -128, -155, -136, -132], [127, 154, -129, 139, 122], [-137, -158, -154, -127, 140, 156], [-145, 150, 134], [133, -134, -132], [123, 130, -141, 147, -124, 157, -143, -153, 122], [147, 151, -159], [160, -154, -133, -147], [-137, -147, -145], [-141, 135, 150], [138, 143, -137], [137, -145, -133, -136, 160, 142, -159, 138, -126, 148], [145, -123, -143], [-124, 143, -157, -136, -146], [147, 134, 139, -153, -152, -130, 156, 131, 121, -141], [155, 149, 132, -125], [143, -156, -155], [-128, -125, -129], [-127, -124], [141, 135, 147], [-136, 159], [-135, -130], [-124, -127, 122, -140], [126, -133, 144, -129], [160, 157, -141, -142], [-143, -137, -136, 135], [-136, -140, 130, 124, 159], [-146, -121, 152], [139, -160, 133], [-135, -156, 143, 125, -148, 158, -154, 138, -140], [-127, 133], [143, 157, 142, 135, 125, 121, -126], [140, -158, 156], [-149, 159, 126, 156, -129, 160], [-144, -150, -127], [-126, -143, -139], [-155, 151, 148], [154, -136, 143, -146], [131, 151], [-160, 143, -158, -137, -135], [157, -131, 124], [-135, -139, -131, 130, 123, 121, 155], [146, -148, -151], [-129, -138, -156, -142, 134, -137, -158, -153, 143, -128], [129, 128], [-127, 144, -153, -136, 160, 134, -151], [-132, 155, -158, 127], [131, 144, 145], [-129, -147, -136, -134, -133], [-151, -143, -159], [123, -158, 134], [-154, 160, 133, -135], [-149, -160, -128], [-160, -155], [149, 140], [-151, -145, 148, 129], [-160, -135, -157], [156, 152, 128], [156, -146, -142], [159, -158], [123, -144, -143], [122, -123], [-137, -128, 126], [-135, 133, -157], [125, -147, -136], [-141, -134, 128], [131, 128], [-128, 146, 154, 135], [-133, 160, 129], [-148, -121], [-135, 128, -147, 123, 131, -139, -155], [-158, 121, 151], [-134, 150, 146, 136], [-160, 124, 145, 157], [-156, 137, 138], [132, 150, -139], [145, 147, 126, -129, -153, -140], [-128, -130, -156], [160, -148, 149, -156, 147], [-137, -153, -132], [135, -151, 123, -146], [-138, -134, -121, -153, -150], [-126, 130, -155], [148, -152, -147, 126, -154], [127, -137, 130], [142, -152, 139], [122, -123, 151, 157, -153, 150], [-157, -146, 150], [-142, -152, 143, 130, -144], [-155, 131, -137, 123, -124, -130, -121, 127], [-140, -125, 157], [-129, -137, 152, 155, -145], [152, -147, 123, 155], [-144, 146, 141, 129], [148, 155, -136, 134, 143, 138, 154, -135, 123], [-150, -123, 139], [-151, -136, -154, 157], [-155, -134, -122, 123, 153], [-140, -155, 122], [-122, -123, -132], [-154, 150, 159, 125, 124, -127], [122, 124, 141, 121], [-142, 125, 124, -123], [-145, -160, -142, 146, -153], [135, 145, 157, 134, -139], [-153, -156, 158, 160], [154, 134, 138], [135, -155, 146, -160], [132, -159, -122], [-122, -121, -127, -125, 126, 154, -123, 146, 153], [123, 152, -155, 154, -129, 141, -137, -149], [135, -123, 145, -147, -150], [138, -140, 124, -151, -152, 141, 148, -123], [-159, 156, -158], [-138, 126, 143, -152], [-152, -134, -130, -143], [140, 145, -149, 121, -136, -147, -134], [-128, 125, 138], [144, -155, -148, 146, 137], [124, -159, -137], [-141, 123, -155], [-140, 154], [-126, 122, 144], [-130, -140, -146], [130, 141, -147, -123, 159], [-159, 128], [127, 133, 146], [-131, 126, 148], [-124, 132, -158, 144, 153, -129, 121, 155, 145], [131, 158, -148, -134, 155, 133], [-153, -143, 141], [-153, -155, -154], [-129, 142, 137], [122, 158, -153, 144], [-128, 123, 132, -129], [125, -150], [125, -141], [137, -131, -136, 126, -124, 153, -145, -148], [131, -130, -151, 123, 149, -132], [142, 127, -130, -148], [146, 145], [-158, 123, -153, 129, -151, 152, -155], [141, 149, -122, -128, -133, 144], [-138, -140, 148, 147], [139, -129, 138], [-157, 154], [-199, 194], [199, 193], [-181, -199, 180], [172, -200], [168, 175, -176, 179, 177], [-163, 178], [-167, 175, -166], [194, 195, 191, -172], [-184, -178, 170], [-180, 195, -182], [171, -197, -178, 187, 170, -168, -190, -173, 182], [183, 179, 174], [183, 194, -184, -172], [178, 197, -169], [-195, -173, 171, -188, 199], [-198, -184, 161, -196, -163, -167], [-163, 193, 196, 161, 181, -192, -184], [-163, 179, -188], [182, 180], [200, 176, 198], [-196, -173, -182], [-175, -171, 195, 197, 192], [170, -178, -175], [-177, -170, -180, 165, 188], [-181, 169, 178, -162], [197, -173, -166], [180, 172, 194], [179, -177], [-193, 177, 195, -194], [183, 186, -192], [-179, -163, 187, 195], [-196, -185, 173, -199], [179, -197], [-176, 180, 189, 185], [-166, 188, 194], [-187, -176], [-164, -175], [-169, -168, 194, -165, 197, -174, 185, 164, -190, 198, 171], [191, 198], [-199, 184, -173, 183], [-176, -188, -195], [167, -185], [-161, 176], [-184, 176], [164, 193, 173, -191], [-170, -167, 172, 198, -177], [-185, 195, 179], [199, 196, -161, -176, -170, 164, -190, 169, 165, -178], [172, -175, 195, -164], [167, 162, -179, 170], [193, -190, 185, -186, 161, 195, -174], [-185, 181, -164, 192, -197, 165], [184, -198, -195, 174, -192, 194], [165, 182, 195, -189, 164, -163], [-181, -177, 189], [-168, -171, 181], [-181, -162, -161, 184], [173, 191, 199, -197, 182, -162, -190, -183, 165, 195, 188, -185], [-179, -199, 174, -185], [-163, -166, 198, -174, 189, -173, 162, 187, -196], [-185, -181, -173, -200], [-179, 169, -190], [-177, 162, 174, -182], [191, -174, -178], [-181, -167, 182], [184, -163, 196], [196, -177, -171, -190, 199, 173, -188, -176, 163, 191, -198, 170, -162, 179], [171, -199, 184, 166, 161, -198, 176], [185, 169, -190], [175, 194], [163, -185, 173, 165], [-179, 182, 180, 189, 196, 175], [198, 189, 174, 169, 168, -190, 167, -164, -196, 166, 176, -188, -161], [-194, -165, 172, -177, -161], [-197, -199, -171, -175, 191, -178], [-169, -172, -182, -177], [-161, -199, -187, -169, -180, 181, -195], [-181, -183], [199, 167, 195], [-164, -196, -170, 194, -181, -197], [-196, 184, 195], [-185, 172, -176, -161], [-200, -194, -180, -173, 189, 165, -181, 183, 196], [172, 187, 181, 198], [-166, -190, 198, -167, -193, -165, -189, 162], [-195, 165, 176], [165, 190, 174], [180, 191, -195, 169], [-168, -192, 180, -194, 188, -173, -186, -178, -182, 166], [165, 181, -174, -186], [179, -172, 190, 187, -166, -170], [174, 171, -184, 182], [-175, 165, 181, -187], [-168, 167], [-177, -168, -193, -191, -182, 173], [171, 163, -180], [-170, -174, 198, -164], [189, -198, -179], [-168, 183, -164, -188], [200, -189, -175, -164, -165, 198], [-199, -170, 161], [-180, -200, 196, -165], [178, -172, 182, -198, -173], [200, 163, -186, -180, -182], [-188, -183, 173, 198, 164, 200, -178], [162, -175], [-198, -171, 162, -163, -192, 178], [-161, 173, -179, -188], [177, 198, 172, 185, 187], [-173, 189, 169], [-168, -182], [162, -195, -161, 174, -176, 187, -188, -197, 172], [174, 163], [163, -200, 164, 195], [175, -162], [-198, -199, -161, 190], [182, 188, -164, 174, -186], [193, 187], [176, 169, 184], [-180, 168, 165, 171], [179, -187, 184, 194, 173, 168], [196, 163, -164, 191, -171, -169], [-199, -183, 200], [-200, 196, 193], [-185, 171, -194], [-193, -190, 164, 184, -198, 197], [180, 185], [-177, -175, 199, -164, -182, -170, 161, 200, 197], [-177, 171, -182, 169], [167, 192], [-162, 177, -174, -166], [191, 184], [178, 196, -164], [-178, 182, 168], [178, 179], [190, -199, 178, 177, 180], [176, -190, 184, 164], [173, 176, -172, 193, 162, -188, -164, -181], [-192, 175, 194], [184, 197, -178, -174, 186, -171, 196, 164, 200], [-187, -166], [187, 195, -175, 189], [179, -178, 168], [164, -186, 198], [-194, -168, -191], [178, 163, 185, 166, -172], [189, -196, -188, 195], [-197, -164, 173, -189], [-193, 200, -187, -181, -172, 199, 178], [-173, -175, 186, 190, 195], [176, -179, 193, 161, 189], [-161, 190, -185, 188], [-182, -197, 184, 165, -180, -192, 187, -186, 162], [-165, -168, 162, -197, 166, -161], [198, 195, -180, -197], [170, 179, -164, -165, -200, -169], [166, -176, -190], [166, 193, -161, -177, 187, -198, 188, -168, 186, 183], [173, 183, 190, 162, 195], [-196, -164, 199, -176], [-197, -166, 179, -198], [167, -162, 193, 168], [176, -185, 166], [199, -163, -178, -168], [-170, 197, 184, 200], [177, -190, -180, -200], [169, -170, 197, 192], [177, -186], [196, -192, 182, 163, 161, -194, -193, -173, 178, 184, 180], [191, -172], [-190, 188], [178, -193, 181, 166, -168, -187, 184, -183, 195, -194, 173, -174, -162, -167, 188, 180, 182], [200, -187, 173], [198, 197, 200], [-167, 194, -189, -190, -185], [184, -166, -162, -161], [-197, 171, -179, 198, 173], [174, 173, 170], [-173, -198, 168, 171, 193, -182, 192, 177, -183, -199, -190, -187, 169, 172, 188, -197, -191, 180], [163, -168, -193], [-182, -174, 183, -191, -167, -172], [-194, 164, 199], [198, 181, 172, -191], [-199, -188, -195, 162, 186, 196, 192, -169, 183], [181, 180, 191], [178, 187, 167, -199, 164, -196], [-182, -197, -165, -189], [-178, -171, -179, 193, -187, 185, -166], [195, 166, 162, 168, -181], [-199, -163, -179, 180], [-198, -171, -191, -195, -177], [178, -187, 168], [-194, 170, -188, -199], [188, -193, 196, 164], [-179, 162, -192], [178, 188, -195], [-200, -187, -192], [-167, -182, 191, 185], [-178, -174, 187, -167, 188, -180, 191], [-184, 185, -192, 188, -197, 174], [-180, -190, -170, -164], [-174, -163, 173, -165, 188, -189, -180, -175], [-184, 164, 162], [-185, 183, 186, -164, 165, -194], [-190, -168, -172], [176, 181, -163], [-198, 163, -190, 171, 185], [175, -183, -177, 162, -189, 182], [184, 182, -181], [-183, 191, -162, 161], [-175, 171, 190, -169], [-189, -170, 166, -178, -180, -195, 190], [-162, -163], [179, 165, 186], [-188, -163, 200], [-163, 161, 171, 200, -184], [180, 185, 170], [-190, 196, 167, -200, 165], [-184, -166, 172, 194], [175, -173, -165, 184], [-170, -177], [190, -199, 194], [188, 167, -176, 163, 200, 168], [164, -169, 173], [-163, 184, 187], [-183, 190, -187, -184], [200, 165, -169, -192, 184, -196, -173], [171, 200, -197, -196], [197, 162, -190, 179, 174], [165, -181], [-172, 189, 170, -173, 166, -194], [173, 186, 187, 199, 163, 190, 182, -166, 194, 191], [-196, -197, -180], [182, 177, 190, 195, 198], [-180, 169, -185], [171, -166, 173, -198, -189], [-184, -175, 166, -176, 199, 200], [-196, 184, 179], [-199, -163, -189], [192, 165, -166, 178, 173, -175, -199], [-168, 198, -185, 186, -169], [-186, 190, -175, 171, -193, 177, -184, -200, -183, 170], [186, 185, 169, 198], [163, -197], [186, -188, 182], [189, 196, -162], [168, -182], [-187, -183, 168, 191, -184, 175], [-182, -171], [-239, 234], [239, 233], [-221, -239, 220], [212, -240], [208, 215, -216, 219, 217], [-203, 218], [-207, 215, -206], [234, 235, 231, -212], [-224, -218, 210], [-220, 235, -222], [211, -237, -218, 227, 210, -208, -230, -213, 222], [223, 219, 214], [223, 234, -224, -212], [218, 237, -209], [-235, -213, 211, -228, 239], [-238, -224, 201, -236, -203, -207], [-203, 233, 236, 201, 221, -232, -224], [-203, 219, -228], [222, 220], [240, 216, 238], [-236, -213, -222], [-215, -211, 235, 237, 232], [210, -218, -215], [-217, -210, -220, 205, 228], [-221, 209, 218, -202], [237, -213, -206], [220, 212, 234], [219, -217], [-233, 217, 235, -234], [223, 226, -232], [-219, -203, 227, 235], [-236, -225, 213, -239], [219, -237], [-216, 220, 229, 225], [-206, 228, 234], [-227, -216], [-204, -215], [-209, -208, 234, -205, 237, -214, 225, 204, -230, 238, 211], [231, 238], [-239, 224, -213, 223], [-216, -228, -235], [207, -225], [-201, 216], [-224, 216], [204, 233, 213, -231], [-210, -207, 212, 238, -217], [-225, 235, 219], [239, 236, -201, -216, -210, 204, -230, 209, 205, -218], [212, -215, 235, -204], [207, 202, -219, 210], [233, -230, 225, -226, 201, 235, -214], [-225, 221, -204, 232, -237, 205], [224, -238, -235, 214, -232, 234], [205, 222, 235, -229, 204, -203], [-221, -217, 229], [-208, -211, 221], [-221, -202, -201, 224], [213, 231, 239, -237, 222, -202, -230, -223, 205, 235, 228, -225], [-219, -239, 214, -225], [-203, -206, 238, -214, 229, -213, 202, 227, -236], [-225, -221, -213, -240], [-219, 209, -230], [-217, 202, 214, -222], [231, -214, -218], [-221, -207, 222], [224, -203, 236], [236, -217, -211, -230, 239, 213, -228, -216, 203, 231, -238, 210, -202, 219], [211, -239, 224, 206, 201, -238, 216], [225, 209, -230], [215, 234], [203, -225, 213, 205], [-219, 222, 220, 229, 236, 215], [238, 229, 214, 209, 208, -230, 207, -204, -236, 206, 216, -228, -201], [-234, -205, 212, -217, -201], [-237, -239, -211, -215, 231, -218], [-209, -212, -222, -217], [-201, -239, -227, -209, -220, 221, -235], [-221, -223], [239, 207, 235], [-204, -236, -210, 234, -221, -237], [-236, 224, 235], [-225, 212, -216, -201], [-240, -234, -220, -213, 229, 205, -221, 223, 236], [212, 227, 221, 238], [-206, -230, 238, -207, -233, -205, -229, 202], [-235, 205, 216], [205, 230, 214], [220, 231, -235, 209], [-208, -232, 220, -234, 228, -213, -226, -218, -222, 206], [205, 221, -214, -226], [219, -212, 230, 227, -206, -210], [214, 211, -224, 222], [-215, 205, 221, -227], [-208, 207], [-217, -208, -233, -231, -222, 213], [211, 203, -220], [-210, -214, 238, -204], [229, -238, -219], [-208, 223, -204, -228], [240, -229, -215, -204, -205, 238], [-239, -210, 201], [-220, -240, 236, -205], [218, -212, 222, -238, -213], [240, 203, -226, -220, -222], [-228, -223, 213, 238, 204, 240, -218], [202, -215], [-238, -211, 202, -203, -232, 218], [-201, 213, -219, -228], [217, 238, 212, 225, 227], [-213, 229, 209], [-208, -222], [202, -235, -201, 214, -216, 227, -228, -237, 212], [214, 203], [203, -240, 204, 235], [215, -202], [-238, -239, -201, 230], [222, 228, -204, 214, -226], [233, 227], [216, 209, 224], [-220, 208, 205, 211], [219, -227, 224, 234, 213, 208], [236, 203, -204, 231, -211, -209], [-239, -223, 240], [-240, 236, 233], [-225, 211, -234], [-233, -230, 204, 224, -238, 237], [220, 225], [-217, -215, 239, -204, -222, -210, 201, 240, 237], [-217, 211, -222, 209], [207, 232], [-202, 217, -214, -206], [231, 224], [218, 236, -204], [-218, 222, 208], [218, 219], [230, -239, 218, 217, 220], [216, -230, 224, 204], [213, 216, -212, 233, 202, -228, -204, -221], [-232, 215, 234], [224, 237, -218, -214, 226, -211, 236, 204, 240], [-227, -206], [227, 235, -215, 229], [219, -218, 208], [204, -226, 238], [-234, -208, -231], [218, 203, 225, 206, -212], [229, -236, -228, 235], [-237, -204, 213, -229], [-233, 240, -227, -221, -212, 239, 218], [-213, -215, 226, 230, 235], [216, -219, 233, 201, 229], [-201, 230, -225, 228], [-222, -237, 224, 205, -220, -232, 227, -226, 202], [-205, -208, 202, -237, 206, -201], [238, 235, -220, -237], [210, 219, -204, -205, -240, -209], [206, -216, -230], [206, 233, -201, -217, 227, -238, 228, -208, 226, 223], [213, 223, 230, 202, 235], [-236, -204, 239, -216], [-237, -206, 219, -238], [207, -202, 233, 208], [216, -225, 206], [239, -203, -218, -208], [-210, 237, 224, 240], [217, -230, -220, -240], [209, -210, 237, 232], [217, -226], [236, -232, 222, 203, 201, -234, -233, -213, 218, 224, 220], [231, -212], [-230, 228], [218, -233, 221, 206, -208, -227, 224, -223, 235, -234, 213, -214, -202, -207, 228, 220, 222], [240, -227, 213], [238, 237, 240], [-207, 234, -229, -230, -225], [224, -206, -202, -201], [-237, 211, -219, 238, 213], [214, 213, 210], [-213, -238, 208, 211, 233, -222, 232, 217, -223, -239, -230, -227, 209, 212, 228, -237, -231, 220], [203, -208, -233], [-222, -214, 223, -231, -207, -212], [-234, 204, 239], [238, 221, 212, -231], [-239, -228, -235, 202, 226, 236, 232, -209, 223], [221, 220, 231], [218, 227, 207, -239, 204, -236], [-222, -237, -205, -229], [-218, -211, -219, 233, -227, 225, -206], [235, 206, 202, 208, -221], [-239, -203, -219, 220], [-238, -211, -231, -235, -217], [218, -227, 208], [-234, 210, -228, -239], [228, -233, 236, 204], [-219, 202, -232], [218, 228, -235], [-240, -227, -232], [-207, -222, 231, 225], [-218, -214, 227, -207, 228, -220, 231], [-224, 225, -232, 228, -237, 214], [-220, -230, -210, -204], [-214, -203, 213, -205, 228, -229, -220, -215], [-224, 204, 202], [-225, 223, 226, -204, 205, -234], [-230, -208, -212], [216, 221, -203], [-238, 203, -230, 211, 225], [215, -223, -217, 202, -229, 222], [224, 222, -221], [-223, 231, -202, 201], [-215, 211, 230, -209], [-229, -210, 206, -218, -220, -235, 230], [-202, -203], [219, 205, 226], [-228, -203, 240], [-203, 201, 211, 240, -224], [220, 225, 210], [-230, 236, 207, -240, 205], [-224, -206, 212, 234], [215, -213, -205, 224], [-210, -217], [230, -239, 234], [228, 207, -216, 203, 240, 208], [204, -209, 213], [-203, 224, 227], [-223, 230, -227, -224], [240, 205, -209, -232, 224, -236, -213], [211, 240, -237, -236], [237, 202, -230, 219, 214], [205, -221], [-212, 229, 210, -213, 206, -234], [213, 226, 227, 239, 203, 230, 222, -206, 234, 231], [-236, -237, -220], [222, 217, 230, 235, 238], [-220, 209, -225], [211, -206, 213, -238, -229], [-224, -215, 206, -216, 239, 240], [-236, 224, 219], [-239, -203, -229], [232, 205, -206, 218, 213, -215, -239], [-208, 238, -225, 226, -209], [-226, 230, -215, 211, -233, 217, -224, -240, -223, 210], [226, 225, 209, 238], [203, -237], [226, -228, 222], [229, 236, -202], [208, -222], [-227, -223, 208, 231, -224, 215], [222, -211], [254, -257], [276, -271, 244, -278], [-260, -258, -266, 276, 250], [257, 277, -261], [-266, 252, -255], [252, 274], [-257, -269, -276], [268, 271, -252], [-253, -254, -265, -260], [-247, 255, -277, 276], [-245, -241, 260], [-269, -280, -265], [270, 246, 268], [-264, -269, -267], [276, 242, 244, 280], [243, -265, -272], [258, 261, -249, -278, -264], [265, -244, -249, 275], [241, 242, 247, -257], [-269, 248, -265, -272, -257], [279, -244, 255, 278], [-251, -245, -264, 260], [248, -271, -266], [-253, -245, 275, -273, -270, 244], [-256, -270, -243, 274], [261, -265], [-245, -257, -260, 252], [272, -280, -276], [-268, -241, -271, 254, -255], [-273, 264], [247, -253, -245, 264], [261, -243, -259, 246], [241, 250, -271], [-266, 273, 277], [-262, -244, -245, 251, 250, 269, -267, -243], [-266, 272, 241, 268, 254, -246], [-262, -257, 265, 274, 260, 250, 269], [-271, 245], [-242, 261, -256, 266], [-274, 248, -275], [271, 279, 251], [276, -253, 259, -252, 254], [-250, 275, 245, -279], [-243, -246, -280], [-274, 251, 262, -265, -270], [-265, -254, -264], [-255, 248, 273, 266], [-253, -247, -264], [274, 254, -273], [-276, 244, 278, -274, 245, 256], [-273, -262, -242], [250, 272, -265, -264, -256], [273, -274, -268], [-247, 249], [264, -254, 252], [-262, 257, 246, 271, -278, 243], [-247, -246, 271, -248], [261, -278, -266], [256, 280, -245, -260, -259, -251], [-279, 259, -241, 266, -263], [-277, -261, -273, 256, 242], [266, 269, -277, -253, 273, -263, -257], [-250, -264, -243, 266, 247, 257, -249], [245, 264, 241, -276, -242, 275], [-269, 272, -271], [-276, -242, 280, 279, -269, -250, -262, 245, -253], [266, 247, 267, -260, -248], [269, -250, -251, 242, 255, -249, 263], [-263, 251, 254, -279, 280], [-242, -280, -264], [247, -268, 248], [-245, 261, 271], [278, 277, -264], [-262, 251, -277], [267, 262, 256, -277, 271, 275], [-255, 242, 241, 265], [-246, -279, -248, 270, 274, -266], [-270, -242, 246], [263, -279, -244, 249], [261, 251, -280], [278, -269, 254], [277, 245, 253, -267, -270, 244, 243, -264, -248], [-241, 267], [280, -242, 260], [-265, 268, -263, -277], [269, -275, -255], [-261, -265, -242, -272], [-277, -252, -263, -266], [252, 275, 247], [256, -245, -270, 258], [280, -255, 242, -265], [-272, -241, 263, 247, -258, 265, 249, -274], [-268, -244, -265], [263, -279, -244, -248], [280, 276, 256], [259, -257, 269], [-261, 270, 265], [-248, 265, -243], [257, 263, 244, 254, -262], [-257, -245, 267], [242, 252, -243], [264, 268, -251], [261, 277, -266, -249], [-265, 279, 266, 276], [-252, 265, -267, -277, 269, 242], [-244, -252, -272], [262, 276, -248, -253], [256, 255, 251, 279, -248], [266, 257, -247], [-250, 253], [-258, -256, -274], [273, 270], [-258, -246, 254], [-266, 242, 277], [-242, 252, 263, 251], [-274, -260], [-268, 251, -259], [242, 246, 257], [-246, 244, 253], [-248, 253, 272, -246, 256], [257, -262, -271], [260, 258, -275, -274, 248], [278, -245, 279], [-273, -279, -260], [255, -252, 273], [249, 267, 246, -252, -277, 273, 244], [278, 265, 272, -268], [-254, -253, -248], [259, 266, -258, 270, -276], [244, 279, -252, -246], [268, 249], [276, -243, 267, 258], [272, -276, -269, 248, 253], [278, 251, 273], [250, 270], [247, 263, -256, -245, 255], [278, 260, 266, 265, -254, -243, -259], [-246, -247, 271], [-262, -271, 258], [256, 243, -250], [-276, -261, -259], [-273, -270, 251, -280, -253, 247, -260], [257, 279, 249, -268], [-269, 250, -261, 247], [-246, -257, -280, 260], [246, -257, -279], [-266, -260], [273, 280], [260, 273, -244], [-258, -251, 263, -276], [-254, 264, -276, 259], [267, -274, 269, -242, 257], [-276, 265, -270, 252, -269, -275, 257], [-260, -277, 275], [252, -245, -261], [267, -259, 250], [-278, -249, 276], [253, 249, -258, -259, -268, 257], [-267, 272, -274], [253, 259, -265, 246, -243], [-254, -265, -271], [-254, -266, -258], [247, 244, 269, -245], [270, -263, -264], [-260, 252, -277, 261], [250, -275, 254], [245, -273], [246, 271, -273], [-278, 268, -259], [-275, 242, 271], [-262, -269, 256], [256, -269, -271], [250, 255, -278], [-270, -250, 261], [-252, 278, -243, 246], [268, -256, 279], [-275, -252], [-257, 274, 245, 241, 266, 271], [276, 245, -271, 250, 255, -278, -261], [-267, -275, 271], [269, 241], [-255, 272, -271], [278, 262, -251, 247], [254, 265, -267], [-244, -261, 270, -258, -277], [278, -269, 255], [257, -265, -262, 260, 266], [248, 255, -272, 261, 254, -250], [279, -253, 247, -275, 274], [271, 280, -268, 267, -252, -250, -246, 253], [-244, -251, 262, 276, -258], [260, -261, 245], [251, 273, 270, -280, 272], [-248, 276, 252, -270, -254, 273], [241, 274, 272, 265, 260], [-271, -258, 275], [271, -251, 272, 256], [245, 265, 253, -268], [260, -241, -252, 261], [-244, -273, -275, -266, -248], [257, -262, 261], [255, -244, 259, 272, -261, -252], [267, 255, -258, 244, 268, -260], [-269, 273, -246, 275, 245, -271], [-255, 270, 248, 245, 247, 274, -261, 263, 271, -277], [275, -274, 269], [255, 250, -271], [-248, 251, -278, 269], [-257, -252, 277, 256, -269], [260, 276, -280, -259, 245], [259, -252, -261], [259, 252, 278], [-266, -248, -253], [275, -252, 249, 247], [-259, 261, 244, -264, 278], [261, 246, -266, -245, 267], [245, 252, 256], [-248, 244, -273, 261], [255, -269, -249, 262], [-261, 263, 247, -260], [268, 243], [294, -297], [316, -311, 284, -318], [-300, -298, -306, 316, 290], [297, 317, -301], [-306, 292, -295], [292, 314], [-297, -309, -316], [308, 311, -292], [-293, -294, -305, -300], [-287, 295, -317, 316], [-285, -281, 300], [-309, -320, -305], [310, 286, 308], [-304, -309, -307], [316, 282, 284, 320], [283, -305, -312], [298, 301, -289, -318, -304], [305, -284, -289, 315], [281, 282, 287, -297], [-309, 288, -305, -312, -297], [319, -284, 295, 318], [-291, -285, -304, 300], [288, -311, -306], [-293, -285, 315, -313, -310, 284], [-296, -310, -283, 314], [301, -305], [-285, -297, -300, 292], [312, -320, -316], [-308, -281, -311, 294, -295], [-313, 304], [287, -293, -285, 304], [301, -283, -299, 286], [281, 290, -311], [-306, 313, 317], [-302, -284, -285, 291, 290, 309, -307, -283], [-306, 312, 281, 308, 294, -286], [-302, -297, 305, 314, 300, 290, 309], [-311, 285], [-282, 301, -296, 306], [-314, 288, -315], [311, 319, 291], [316, -293, 299, -292, 294], [-290, 315, 285, -319], [-283, -286, -320], [-314, 291, 302, -305, -310], [-305, -294, -304], [-295, 288, 313, 306], [-293, -287, -304], [314, 294, -313], [-316, 284, 318, -314, 285, 296], [-313, -302, -282], [290, 312, -305, -304, -296], [313, -314, -308], [-287, 289], [304, -294, 292], [-302, 297, 286, 311, -318, 283], [-287, -286, 311, -288], [301, -318, -306], [296, 320, -285, -300, -299, -291], [-319, 299, -281, 306, -303], [-317, -301, -313, 296, 282], [306, 309, -317, -293, 313, -303, -297], [-290, -304, -283, 306, 287, 297, -289], [285, 304, 281, -316, -282, 315], [-309, 312, -311], [-316, -282, 320, 319, -309, -290, -302, 285, -293], [306, 287, 307, -300, -288], [309, -290, -291, 282, 295, -289, 303], [-303, 291, 294, -319, 320], [-282, -320, -304], [287, -308, 288], [-285, 301, 311], [318, 317, -304], [-302, 291, -317], [307, 302, 296, -317, 311, 315], [-295, 282, 281, 305], [-286, -319, -288, 310, 314, -306], [-310, -282, 286], [303, -319, -284, 289], [301, 291, -320], [318, -309, 294], [317, 285, 293, -307, -310, 284, 283, -304, -288], [-281, 307], [320, -282, 300], [-305, 308, -303, -317], [309, -315, -295], [-301, -305, -282, -312], [-317, -292, -303, -306], [292, 315, 287], [296, -285, -310, 298], [320, -295, 282, -305], [-312, -281, 303, 287, -298, 305, 289, -314], [-308, -284, -305], [303, -319, -284, -288], [320, 316, 296], [299, -297, 309], [-301, 310, 305], [-288, 305, -283], [297, 303, 284, 294, -302], [-297, -285, 307], [282, 292, -283], [304, 308, -291], [301, 317, -306, -289], [-305, 319, 306, 316], [-292, 305, -307, -317, 309, 282], [-284, -292, -312], [302, 316, -288, -293], [296, 295, 291, 319, -288], [306, 297, -287], [-290, 293], [-298, -296, -314], [313, 310], [-298, -286, 294], [-306, 282, 317], [-282, 292, 303, 291], [-314, -300], [-308, 291, -299], [282, 286, 297], [-286, 284, 293], [-288, 293, 312, -286, 296], [297, -302, -311], [300, 298, -315, -314, 288], [318, -285, 319], [-313, -319, -300], [295, -292, 313], [289, 307, 286, -292, -317, 313, 284], [318, 305, 312, -308], [-294, -293, -288], [299, 306, -298, 310, -316], [284, 319, -292, -286], [308, 289], [316, -283, 307, 298], [312, -316, -309, 288, 293], [318, 291, 313], [290, 310], [287, 303, -296, -285, 295], [318, 300, 306, 305, -294, -283, -299], [-286, -287, 311], [-302, -311, 298], [296, 283, -290], [-316, -301, -299], [-313, -310, 291, -320, -293, 287, -300], [297, 319, 289, -308], [-309, 290, -301, 287], [-286, -297, -320, 300], [286, -297, -319], [-306, -300], [313, 320], [300, 313, -284], [-298, -291, 303, -316], [-294, 304, -316, 299], [307, -314, 309, -282, 297], [-316, 305, -310, 292, -309, -315, 297], [-300, -317, 315], [292, -285, -301], [307, -299, 290], [-318, -289, 316], [293, 289, -298, -299, -308, 297], [-307, 312, -314], [293, 299, -305, 286, -283], [-294, -305, -311], [-294, -306, -298], [287, 284, 309, -285], [310, -303, -304], [-300, 292, -317, 301], [290, -315, 294], [285, -313], [286, 311, -313], [-318, 308, -299], [-315, 282, 311], [-302, -309, 296], [296, -309, -311], [290, 295, -318], [-310, -290, 301], [-292, 318, -283, 286], [308, -296, 319], [-315, -292], [-297, 314, 285, 281, 306, 311], [316, 285, -311, 290, 295, -318, -301], [-307, -315, 311], [309, 281], [-295, 312, -311], [318, 302, -291, 287], [294, 305, -307], [-284, -301, 310, -298, -317], [318, -309, 295], [297, -305, -302, 300, 306], [288, 295, -312, 301, 294, -290], [319, -293, 287, -315, 314], [311, 320, -308, 307, -292, -290, -286, 293], [-284, -291, 302, 316, -298], [300, -301, 285], [291, 313, 310, -320, 312], [-288, 316, 292, -310, -294, 313], [281, 314, 312, 305, 300], [-311, -298, 315], [311, -291, 312, 296], [285, 305, 293, -308], [300, -281, -292, 301], [-284, -313, -315, -306, -288], [297, -302, 301], [295, -284, 299, 312, -301, -292], [307, 295, -298, 284, 308, -300], [-309, 313, -286, 315, 285, -311], [-295, 310, 288, 285, 287, 314, -301, 303, 311, -317], [315, -314, 309], [295, 290, -311], [-288, 291, -318, 309], [-297, -292, 317, 296, -309], [300, 316, -320, -299, 285], [299, -292, -301], [299, 292, 318], [-306, -288, -293], [315, -292, 289, 287], [-299, 301, 284, -304, 318], [301, 286, -306, -285, 307], [285, 292, 296], [-288, 284, -313, 301], [295, -309, -289, 302], [-301, 303, 287, -300], [-308, 283], [353, 352, -348, -349, -346], [-360, -353, -347], [-330, -327, 346, 358], [-325, -355, -356, -348, 322], [338, 326, -325], [-350, 338, 340], [-358, 348], [336, 323, 344, 343, -329, 349], [338, -325, -332, -327], [339, 326, -330], [353, -331, -330, -335, 346], [339, -349, 359, 345, 326, 341, -335, 338], [-357, -328, 359, 329], [326, 351], [-331, -328, -348], [332, -323, 322, 353, 340, -347], [322, -327, -332], [323, -360, 334, 350, 322, 354], [-357, -349], [-339, 356, 343, 348, -333], [-357, -324, -358], [-330, -333], [-347, 360, 341, -343, -332], [-338, 344, -348, -357, -324, -336, 333], [-332, -349, -335, 328, 339], [344, -331, 329, 336], [351, -323, -342, -352, 354], [332, -353, 324, 335, 325], [348, 335, 336], [-355, -353, 331], [346, 327, -323, 345], [-338, -327, -348, 353, 340], [332, -339], [337, -340, -331], [323, 334, -332, -336], [353, 339, 323, 358, 322, 360], [-346, 340, -335], [342, 353, 355], [-343, -358, 324, -328, -332, -339, 345, -349, -356], [353, 359, -321, 326, -332, 336, -340], [-328, 350, 354, -359, 345, -348], [-350, 336, 360, -345], [-349, -333, 341, 352, -330, -357], [354, 342, -332], [342, 360, -335, 327, -322, -340], [-359, 331, -321, -357, -334], [325, -349, 344], [-340, 357, 359], [355, -336, -326], [-327, 351, 326], [354, -334], [-360, 332, 359, 333, -323, -334, 336], [-331, 349, 322], [-325, -355], [345, -355, 343], [-330, 355], [331, -348, 350, 346, 349, -324, -360, 355, 343], [359, 322, -340], [-357, -348], [-354, 334, -351, 349], [-330, -343, -338], [356, -328, 342], [348, -350, -358], [336, 330], [326, -332, 346], [360, 330, -328], [332, -349, -359], [351, 359, 342, -360, 321, -349], [-343, -329, 359, -322, -356, -332, 336, -358, -344, -339, 338, -325], [-349, 330, -346, 323], [-345, 329, -353, 338, -358, 321, 328, -322, -336], [345, -355, -331, -332, -323, -335, -346], [-347, -331, 351], [352, -344], [-329, 344, 334, -345, 351], [346, 352, 341, -360, -347, 322], [-324, 349, -325], [341, -353, 335], [-339, 352, -357, 356], [-330, -344, 338, -351, -347], [351, -356, 353, -357, -321, -332], [326, 356, 323], [360, -357, 322], [353, -359], [354, 355, -336, 326], [322, 327, 324, 358, 353, -352, 348, -357], [341, -349, 352, 340, -321, 355, 323, -329, 347, -328, -332], [-337, 323, -359], [351, -359], [-341, 340, 352, 354, -322, 358, 335, -336, 324, -351], [-341, 323, -349, 350], [345, -334, 356, -360, 325, 359, 346], [331, -322, -326], [-345, 343], [339, 321, 322], [348, 357, -352, 349, 338, -328, 327, -331, 347], [-336, -359, -324], [329, -328, 354], [-327, 329, 326], [-357, 356, -338, 359, 360, -332, 358], [-358, 347, -321, -334, -349, 335, -357, 345], [-340, 360, -354, -325, -337, -350, 349, 356], [321, 323, -347], [348, 331], [-344, 346, -331, -333], [330, -326, -349, 348, 340, -333, 350], [348, -337, 346], [-350, -346, 358, -332, 326], [-357, -330, -332], [-358, -331], [349, 356, 333, 350, 355, -332], [-357, 323, 341, 322], [-350, 352, 356, 334, -340], [-339, -324, -333, 348, -338], [-340, -360, -348], [-339, 338, 333, 324, -345], [326, 353, -348, -352], [-345, -340, -353], [-334, -323, -339, 354, 338, -326, 330, 336, 345], [329, -321, -325], [-322, 359, 324], [-358, -325, -337], [352, 353, 322, 334, 345], [-354, -325, 322], [-331, 322, 327, 348, -343], [344, -326], [-359, 340, -358], [332, -353, 324], [352, -331, 330, -346, -360, 355], [-357, -353, 330], [-345, 327], [358, 350, 337, 347, 321], [-348, 327, 330, -344, -338], [-344, 360, 342], [-324, 326, 354, 327], [-324, -326, -348, 323], [355, -342, 325, 352, -333, -337, -336, 346, 327], [325, -336, -331, -324, 352], [-354, -340, 351], [337, -360, 341], [350, 352], [357, -341, 325, -335, 329, 339], [348, -328, -350, -346, 326], [351, -329, -332, -348, -352, 325], [-326, 335, -360, 341, -348], [-336, -341, 355, 359], [-346, 350, 335], [340, 321, -342, 349, 352, -343], [-328, 356, 333, 325], [347, 333, -356, -348, 339, 337, -359, -352], [-359, -332, 340, 333, 322], [-356, 335, -333, 347], [337, -342, -325, 340], [346, 357, 323], [328, -359, 321], [-348, 338, -324, -335, -329], [-353, -351, -357, -345, -326], [342, -356, 331, -343, -348], [-339, 337, 359], [-321, 348, 344], [328, -336, 358, -347, -339, 330, -350], [-354, 346], [351, 359], [-349, -324], [336, -339, -323, 350, -359], [-353, 336, -344], [-325, -352], [332, -346, -355, -357, 347, 322], [-331, 353], [343, -336, 323, 337], [-327, 340, 359, 329, -336, -347, -341, 345], [332, -342, -326], [-326, -343, 347], [-341, 326, -331, 338], [-345, -347, 343, -341, 336, 348], [-332, -360, -346, 356, 333], [-342, -350], [341, 349, 351], [345, 351, -339, -323], [-355, -351, 330], [-333, 330], [337, 360, -329, -342], [-360, 349, -333, 353, 323], [341, 339, 338, 324], [-329, -355, 332, -346], [-337, -354, 328, -339, 355, 348, -341, -344], [-343, -331], [-352, 358, -334], [-339, 327, -330, 332, 326, 345], [-335, 352, -346, -321], [-336, 349, -344], [327, -355, -344], [-333, -344, -349], [-326, -332, -337, -359, -352, -347, -344, 338, 358, 360], [-345, -355, 351, -344, 332, -324, 330, -348, 323, -338, 322, -358, 329], [344, 325, -329, 337], [-349, 323, -330, -337], [-335, -336, -339, -352, -358], [353, -354, -338, -346], [-357, 346, 322], [-344, 343, -324, -334, -359, -326], [360, -351, 355, 348, 321], [343, -358], [-357, -323, -344, 334], [360, 358, -341, -334, -349], [-359, 325, -342, -330], [323, 349, -357, -344, 326, 343], [-325, 338], [-336, 342], [-334, 327, -341, -338], [329, -349, -358], [354, -321], [-325, -342, 341], [-324, 343, -349, -335, -352], [359, 338, 357, 339, -343], [-335, -356, 359, 334, -336], [357, 332, 350, -324, -321, 336], [350, -343, 332], [-352, -337, 346], [-340, -336, 356, 332, 352, 326], [-327, -347, -359], [-347, 327, 326, 346, 332], [360, -339, -333, 334], [341, -328], [330, -336, -358, 334], [360, -351, -331, -324], [335, -337, 336], [-342, 346, 341], [-322, 328, -339, -359, -330, 353, 321], [330, 344, -353], [348, 359, 334], [357, 335, -350, 344], [-349, 355, 356, -335, -338, 340, -360, 346], [339, -349], [335, 353], [334, -328, -347], [330, 343, 351, -341, -355], [-352, 341, 331, -354], [324, -342, -345, 337], [-360, -330, 350, -334, -331], [324, 346, -328], [334, 332, 346], [-331, -334, 327], [-353, -358], [339, -353, -321, -335, -327, 324], [343, 338, -334, 359], [332, -327, 343, -348, -325], [336, -323, 333, -359, -351, 344, -324, -348], [-330, 352], [354, -322, -342], [-353, 335], [342, 329, 331, -334], [-342, 336, -332, 326], [352, 339, 357], [-346, 349, 360, 341], [-354, 338, 349], [325, 348, 355, 330], [359, -356, -340], [-343, 333, 350, -351, 332, 341], [-324, -351, -329, -344, -322], [-351, 340, -323, 335, -324], [-342, -345, 340, -352], [-321, 330, -329, -328, -335, -349], [-333, -325, 358, 338], [342, 339, -349, 352], [354, 330, 340, 328], [-353, -341, -334], [-341, 354, 325, 356], [322, -323, 360, 357, 335, 359, -347], [352, 323, -353], [329, 345, 348], [-347, 327], [-343, 349, -356], [321, 323, 338, 353], [-336, 358, 343], [336, -339, 351, 338], [343, -355], [-354, -353, 348], [332, 349, 346, -333], [324, 321, -352], [328, 336], [355, 334, -329], [358, -334, 340], [-342, -335, 345, -329], [-357, 358], [393, 392, -388, -389, -386], [-400, -393, -387], [-370, -367, 386, 398], [-365, -395, -396, -388, 362], [378, 366, -365], [-390, 378, 380], [-398, 388], [376, 363, 384, 383, -369, 389], [378, -365, -372, -367], [379, 366, -370], [393, -371, -370, -375, 386], [379, -389, 399, 385, 366, 381, -375, 378], [-397, -368, 399, 369], [366, 391], [-371, -368, -388], [372, -363, 362, 393, 380, -387], [362, -367, -372], [363, -400, 374, 390, 362, 394], [-397, -389], [-379, 396, 383, 388, -373], [-397, -364, -398], [-370, -373], [-387, 400, 381, -383, -372], [-378, 384, -388, -397, -364, -376, 373], [-372, -389, -375, 368, 379], [384, -371, 369, 376], [391, -363, -382, -392, 394], [372, -393, 364, 375, 365], [388, 375, 376], [-395, -393, 371], [386, 367, -363, 385], [-378, -367, -388, 393, 380], [372, -379], [377, -380, -371], [363, 374, -372, -376], [393, 379, 363, 398, 362, 400], [-386, 380, -375], [382, 393, 395], [-383, -398, 364, -368, -372, -379, 385, -389, -396], [393, 399, -361, 366, -372, 376, -380], [-368, 390, 394, -399, 385, -388], [-390, 376, 400, -385], [-389, -373, 381, 392, -370, -397], [394, 382, -372], [382, 400, -375, 367, -362, -380], [-399, 371, -361, -397, -374], [365, -389, 384], [-380, 397, 399], [395, -376, -366], [-367, 391, 366], [394, -374], [-400, 372, 399, 373, -363, -374, 376], [-371, 389, 362], [-365, -395], [385, -395, 383], [-370, 395], [371, -388, 390, 386, 389, -364, -400, 395, 383], [399, 362, -380], [-397, -388], [-394, 374, -391, 389], [-370, -383, -378], [396, -368, 382], [388, -390, -398], [376, 370], [366, -372, 386], [400, 370, -368], [372, -389, -399], [391, 399, 382, -400, 361, -389], [-383, -369, 399, -362, -396, -372, 376, -398, -384, -379, 378, -365], [-389, 370, -386, 363], [-385, 369, -393, 378, -398, 361, 368, -362, -376], [385, -395, -371, -372, -363, -375, -386], [-387, -371, 391], [392, -384], [-369, 384, 374, -385, 391], [386, 392, 381, -400, -387, 362], [-364, 389, -365], [381, -393, 375], [-379, 392, -397, 396], [-370, -384, 378, -391, -387], [391, -396, 393, -397, -361, -372], [366, 396, 363], [400, -397, 362], [393, -399], [394, 395, -376, 366], [362, 367, 364, 398, 393, -392, 388, -397], [381, -389, 392, 380, -361, 395, 363, -369, 387, -368, -372], [-377, 363, -399], [391, -399], [-381, 380, 392, 394, -362, 398, 375, -376, 364, -391], [-381, 363, -389, 390], [385, -374, 396, -400, 365, 399, 386], [371, -362, -366], [-385, 383], [379, 361, 362], [388, 397, -392, 389, 378, -368, 367, -371, 387], [-376, -399, -364], [369, -368, 394], [-367, 369, 366], [-397, 396, -378, 399, 400, -372, 398], [-398, 387, -361, -374, -389, 375, -397, 385], [-380, 400, -394, -365, -377, -390, 389, 396], [361, 363, -387], [388, 371], [-384, 386, -371, -373], [370, -366, -389, 388, 380, -373, 390], [388, -377, 386], [-390, -386, 398, -372, 366], [-397, -370, -372], [-398, -371], [389, 396, 373, 390, 395, -372], [-397, 363, 381, 362], [-390, 392, 396, 374, -380], [-379, -364, -373, 388, -378], [-380, -400, -388], [-379, 378, 373, 364, -385], [366, 393, -388, -392], [-385, -380, -393], [-374, -363, -379, 394, 378, -366, 370, 376, 385], [369, -361, -365], [-362, 399, 364], [-398, -365, -377], [392, 393, 362, 374, 385], [-394, -365, 362], [-371, 362, 367, 388, -383], [384, -366], [-399, 380, -398], [372, -393, 364], [392, -371, 370, -386, -400, 395], [-397, -393, 370], [-385, 367], [398, 390, 377, 387, 361], [-388, 367, 370, -384, -378], [-384, 400, 382], [-364, 366, 394, 367], [-364, -366, -388, 363], [395, -382, 365, 392, -373, -377, -376, 386, 367], [365, -376, -371, -364, 392], [-394, -380, 391], [377, -400, 381], [390, 392], [397, -381, 365, -375, 369, 379], [388, -368, -390, -386, 366], [391, -369, -372, -388, -392, 365], [-366, 375, -400, 381, -388], [-376, -381, 395, 399], [-386, 390, 375], [380, 361, -382, 389, 392, -383], [-368, 396, 373, 365], [387, 373, -396, -388, 379, 377, -399, -392], [-399, -372, 380, 373, 362], [-396, 375, -373, 387], [377, -382, -365, 380], [386, 397, 363], [368, -399, 361], [-388, 378, -364, -375, -369], [-393, -391, -397, -385, -366], [382, -396, 371, -383, -388], [-379, 377, 399], [-361, 388, 384], [368, -376, 398, -387, -379, 370, -390], [-394, 386], [391, 399], [-389, -364], [376, -379, -363, 390, -399], [-393, 376, -384], [-365, -392], [372, -386, -395, -397, 387, 362], [-371, 393], [383, -376, 363, 377], [-367, 380, 399, 369, -376, -387, -381, 385], [372, -382, -366], [-366, -383, 387], [-381, 366, -371, 378], [-385, -387, 383, -381, 376, 388], [-372, -400, -386, 396, 373], [-382, -390], [381, 389, 391], [385, 391, -379, -363], [-395, -391, 370], [-373, 370], [377, 400, -369, -382], [-400, 389, -373, 393, 363], [381, 379, 378, 364], [-369, -395, 372, -386], [-377, -394, 368, -379, 395, 388, -381, -384], [-383, -371], [-392, 398, -374], [-379, 367, -370, 372, 366, 385], [-375, 392, -386, -361], [-376, 389, -384], [367, -395, -384], [-373, -384, -389], [-366, -372, -377, -399, -392, -387, -384, 378, 398, 400], [-385, -395, 391, -384, 372, -364, 370, -388, 363, -378, 362, -398, 369], [384, 365, -369, 377], [-389, 363, -370, -377], [-375, -376, -379, -392, -398], [393, -394, -378, -386], [-397, 386, 362], [-384, 383, -364, -374, -399, -366], [400, -391, 395, 388, 361], [383, -398], [-397, -363, -384, 374], [400, 398, -381, -374, -389], [-399, 365, -382, -370], [363, 389, -397, -384, 366, 383], [-365, 378], [-376, 382], [-374, 367, -381, -378], [369, -389, -398], [394, -361], [-365, -382, 381], [-364, 383, -389, -375, -392], [399, 378, 397, 379, -383], [-375, -396, 399, 374, -376], [397, 372, 390, -364, -361, 376], [390, -383, 372], [-392, -377, 386], [-380, -376, 396, 372, 392, 366], [-367, -387, -399], [-387, 367, 366, 386, 372], [400, -379, -373, 374], [381, -368], [370, -376, -398, 374], [400, -391, -371, -364], [375, -377, 376], [-382, 386, 381], [-362, 368, -379, -399, -370, 393, 361], [370, 384, -393], [388, 399, 374], [397, 375, -390, 384], [-389, 395, 396, -375, -378, 380, -400, 386], [379, -389], [375, 393], [374, -368, -387], [370, 383, 391, -381, -395], [-392, 381, 371, -394], [364, -382, -385, 377], [-400, -370, 390, -374, -371], [364, 386, -368], [374, 372, 386], [-371, -374, 367], [-393, -398], [379, -393, -361, -375, -367, 364], [383, 378, -374, 399], [372, -367, 383, -388, -365], [376, -363, 373, -399, -391, 384, -364, -388], [-370, 392], [394, -362, -382], [-393, 375], [382, 369, 371, -374], [-382, 376, -372, 366], [392, 379, 397], [-386, 389, 400, 381], [-394, 378, 389], [365, 388, 395, 370], [399, -396, -380], [-383, 373, 390, -391, 372, 381], [-364, -391, -369, -384, -362], [-391, 380, -363, 375, -364], [-382, -385, 380, -392], [-361, 370, -369, -368, -375, -389], [-373, -365, 398, 378], [382, 379, -389, 392], [394, 370, 380, 368], [-393, -381, -374], [-381, 394, 365, 396], [362, -363, 400, 397, 375, 399, -387], [392, 363, -393], [369, 385, 388], [-387, 367], [-383, 389, -396], [361, 363, 378, 393], [-376, 398, 383], [376, -379, 391, 378], [383, -395], [-394, -393, 388], [372, 389, 386, -373], [364, 361, -392], [368, 376], [395, 374, -369], [398, -374, 380], [-382, -375, 385, -369], [397, 398], [-414, -406, -439], [427, 416, 410], [-413, -415, 405, 419], [-405, 404, 423, -407, 430, -408], [417, 415, -408], [-412, -405, -410], [-413, 427, -430, -435], [-427, -431, 423], [-423, 410, 401, -429], [-415, -414, -408], [409, 422, 426], [-431, 411, 430], [430, -409], [-416, 421, 427, -408, -405], [-414, 416, 437], [434, 411, -415, -416, -405, -426], [-424, 413, -439], [427, 417, -402], [431, 407, 440, -421, -424], [401, -416, -415, -414, -422, 430, 419, 424], [401, 433, 426, -408], [-435, 408, 437, 424, -428, -401], [-413, -409, 402], [408, -411], [-425, -435, -411, 410, -409, -422, 420, -431], [-435, -429, -428, 420, 417, 430, 421, 403], [-411, 437, 408, -430], [-409, 416, -422, 403, -419, -420, -436], [425, -435], [-412, 407, -434, -437], [433, -418, 407], [-424, -408], [429, 407, 437, -436], [-422, -440, -408, 415], [436, 418, -424], [423, -408, -406, 415], [410, 415, -404], [410, -434], [-416, 433], [-434, -430, 436, -433, 417], [-435, 439, -408, 424, 433, 436], [409, -410, 439, 411, 418], [406, -429, 421, 427, -411], [-424, 416, -417], [-421, -440, -406], [-402, -431], [-404, 439, -415], [428, 440, -426], [435, 407, -415, -437], [-417, 429], [416, -413, -435], [440, 421, 424, 407, 422], [-427, 402, -433, 438, -440, -405, -436, 422, 423, 416], [427, -422], [428, -402, -438, 408, 427, 414, -426], [422, -426, -416], [-422, 432, 436], [-404, 434, 410], [-432, 422, 417, 414], [-425, -431, -424, -435, 414, 405], [-430, 403], [-422, -423, 425], [417, -429, 422], [429, 437, -430, -414], [431, 401, 429, 423], [403, -428, -408, 430, -438, 418, -427, 420], [403, 426, -438, -406, 401], [418, 412, -429], [-413, 416, -408], [-407, 439, 414], [-434, 420, -431, 413, 418], [-425, 410, -420, 416, -426, 423], [407, 420, 415, 433, -421], [-422, 428, 411, 432, 438], [409, 426, 410, 427, 412, 416], [-432, 434, 402, -430], [-402, -436, -438], [-409, 417, 434], [433, -421, 437, -422, 412], [-427, -429, 424, -435, -402, 438, -407], [417, -407, -428], [435, -419, -433], [437, 427, -415, -439, -436, -431], [-439, 440, 427], [-427, 421, 429], [438, 419, 413], [-427, -408, -440], [440, 401, -435], [-422, 419, 406], [415, 429, -407, 439], [-409, -404, -432], [-417, 431, -411], [425, -407, 404], [-440, -429, -414, 418], [410, -438, 418, 414], [434, 438, 423, -418, -421], [415, 406, 409, 440, 424], [-401, 402, -404, 420, 412], [428, -407, 406], [-406, -429, -410, -434], [-419, -409, 430], [411, -410, 427], [-423, 411, 435], [-406, -439, 421], [-411, -410], [-437, -409, -425, -412], [429, 434], [429, -427, 402, -430, 426, -410], [-427, -430, 414, 422, -433, -432, 421, -418, 423], [422, 433, -415, 429, -424], [-406, 428], [-415, -426, 428], [-420, 433, -404, -405], [427, 434], [440, 407, -410, -434, -419], [-419, 411, 436, 425, 409, 431], [-430, -412, -436, -418], [-440, -420], [440, -413, -419, -425, -405, 404, 439], [-440, -412, 426], [-428, 402, 413, 406, 412], [436, -425, 435], [-405, -422, -411, -401], [-429, -416, -436, -404], [421, -420, 423], [-439, -430, 413, -411, -402, -425, -408], [-416, 429, 415, -402], [-429, -425, 424, 405, -440, -437], [418, -429, 414, -428, -420], [409, -418], [-436, 418, 428, -424, 431], [-432, -420, -429, -425], [-427, -434, -433], [402, 410, -417], [405, 417, 434, -432, 401, -428, 402, -413, -433, -407, -419, -414, 440], [413, 407, -401, 417], [-402, -420], [-415, -429, -419, 402], [-421, -436, -428, -425, -431, -414], [427, -417, -411, -416, -404, 436], [401, -407, -428], [405, 424, -413, -406, 429], [-414, -407, 438, -401], [-422, -418, 421], [404, -434, 413, -438, -411, -436], [-419, 404], [-432, 428, -439], [-420, -429, -416, 432], [-435, 426, -433], [414, -417, -411, -427, 407], [418, 413, -431], [-424, -427], [-412, 406, 439], [-422, -430, 433, 440, -414, 434], [-429, 422, 435], [-408, 425, 401], [421, -417, -410, 412, 430, 427, 431], [405, 440, 421], [424, 440, 419, -410], [-434, 432, -428, -426, -422, 416], [-432, 417], [419, -411, -436, -417, -410], [438, 417], [440, -437], [403, 433, -418], [-416, 431, -404, 417, -412, -425], [-413, -432, 402, 408, 415], [407, 435, -409], [-428, -439], [-440, 425, -426], [407, 429, 428, 424, 439, 422, -431], [417, 415, 414, -401, 409], [420, 408, -406, -413, 422, 416], [-436, 417, 414, 434, -429, -439, -411], [428, -439, -416], [426, -433], [432, 427, -437], [-426, 431, 417, -404], [425, 435], [426, -436, -435], [426, 409, -434, 425, -413, -427, -404], [412, -434, 419], [435, 421, -401, -414, -413, -420, -404, 433], [-426, -421, -431, -422, -435], [-411, 406, -440], [-403, -433], [427, 409, -423, 429], [410, 438, 432, -418, 425], [440, -438], [425, -405, 416, 401, 418, -413, -412, -432, 433], [411, -439, -431], [-415, 416, -423], [-439, 432, -435, -438, 413], [-423, -435, -439, 407, 440, 401, -418, 433], [-434, 416, 433, -432], [-421, 413, 403, -412, 426, -417, 431], [-438, 412, 425], [-424, 440], [-439, -421, -419, 412, -436, -413, -414, -428, 403, -437, 422], [407, 402], [-440, 421], [440, -439, 433], [-433, 426, -403, -435, 436, -421, -423], [-406, -419, -417, 436, 433, 434, 412], [408, 440, -429], [412, -437, 406, 439], [-417, -432, -426, -430, -409, -429, -413, -435], [-411, 413, 438], [426, -420, 418, -433], [-430, -429, -407, 440], [-437, 425, -436], [-405, 433, -424, -402, -440, -410], [-404, -421, 440, -401], [-407, -422, -408], [425, -406], [-422, -421, -427, 403, -432, 415, 435], [-428, 433, -406, -420, 436, -409, -425, 413], [-402, 433, -409], [424, 421, -439], [-422, -438, -421, 434, -410, 407, -426], [407, 412, -401, -433, 404], [-406, -415, 402, 422], [-422, 434, -435, 404], [-419, -403, -405, -435, 411, 421, 430, 424], [435, -424], [-434, -440, 417, 426, -411, 432], [-432, 436, 430], [426, 431, -427], [-454, -446, -479], [467, 456, 450], [-453, -455, 445, 459], [-445, 444, 463, -447, 470, -448], [457, 455, -448], [-452, -445, -450], [-453, 467, -470, -475], [-467, -471, 463], [-463, 450, 441, -469], [-455, -454, -448], [449, 462, 466], [-471, 451, 470], [470, -449], [-456, 461, 467, -448, -445], [-454, 456, 477], [474, 451, -455, -456, -445, -466], [-464, 453, -479], [467, 457, -442], [471, 447, 480, -461, -464], [441, -456, -455, -454, -462, 470, 459, 464], [441, 473, 466, -448], [-475, 448, 477, 464, -468, -441], [-453, -449, 442], [448, -451], [-465, -475, -451, 450, -449, -462, 460, -471], [-475, -469, -468, 460, 457, 470, 461, 443], [-451, 477, 448, -470], [-449, 456, -462, 443, -459, -460, -476], [465, -475], [-452, 447, -474, -477], [473, -458, 447], [-464, -448], [469, 447, 477, -476], [-462, -480, -448, 455], [476, 458, -464], [463, -448, -446, 455], [450, 455, -444], [450, -474], [-456, 473], [-474, -470, 476, -473, 457], [-475, 479, -448, 464, 473, 476], [449, -450, 479, 451, 458], [446, -469, 461, 467, -451], [-464, 456, -457], [-461, -480, -446], [-442, -471], [-444, 479, -455], [468, 480, -466], [475, 447, -455, -477], [-457, 469], [456, -453, -475], [480, 461, 464, 447, 462], [-467, 442, -473, 478, -480, -445, -476, 462, 463, 456], [467, -462], [468, -442, -478, 448, 467, 454, -466], [462, -466, -456], [-462, 472, 476], [-444, 474, 450], [-472, 462, 457, 454], [-465, -471, -464, -475, 454, 445], [-470, 443], [-462, -463, 465], [457, -469, 462], [469, 477, -470, -454], [471, 441, 469, 463], [443, -468, -448, 470, -478, 458, -467, 460], [443, 466, -478, -446, 441], [458, 452, -469], [-453, 456, -448], [-447, 479, 454], [-474, 460, -471, 453, 458], [-465, 450, -460, 456, -466, 463], [447, 460, 455, 473, -461], [-462, 468, 451, 472, 478], [449, 466, 450, 467, 452, 456], [-472, 474, 442, -470], [-442, -476, -478], [-449, 457, 474], [473, -461, 477, -462, 452], [-467, -469, 464, -475, -442, 478, -447], [457, -447, -468], [475, -459, -473], [477, 467, -455, -479, -476, -471], [-479, 480, 467], [-467, 461, 469], [478, 459, 453], [-467, -448, -480], [480, 441, -475], [-462, 459, 446], [455, 469, -447, 479], [-449, -444, -472], [-457, 471, -451], [465, -447, 444], [-480, -469, -454, 458], [450, -478, 458, 454], [474, 478, 463, -458, -461], [455, 446, 449, 480, 464], [-441, 442, -444, 460, 452], [468, -447, 446], [-446, -469, -450, -474], [-459, -449, 470], [451, -450, 467], [-463, 451, 475], [-446, -479, 461], [-451, -450], [-477, -449, -465, -452], [469, 474], [469, -467, 442, -470, 466, -450], [-467, -470, 454, 462, -473, -472, 461, -458, 463], [462, 473, -455, 469, -464], [-446, 468], [-455, -466, 468], [-460, 473, -444, -445], [467, 474], [480, 447, -450, -474, -459], [-459, 451, 476, 465, 449, 471], [-470, -452, -476, -458], [-480, -460], [480, -453, -459, -465, -445, 444, 479], [-480, -452, 466], [-468, 442, 453, 446, 452], [476, -465, 475], [-445, -462, -451, -441], [-469, -456, -476, -444], [461, -460, 463], [-479, -470, 453, -451, -442, -465, -448], [-456, 469, 455, -442], [-469, -465, 464, 445, -480, -477], [458, -469, 454, -468, -460], [449, -458], [-476, 458, 468, -464, 471], [-472, -460, -469, -465], [-467, -474, -473], [442, 450, -457], [445, 457, 474, -472, 441, -468, 442, -453, -473, -447, -459, -454, 480], [453, 447, -441, 457], [-442, -460], [-455, -469, -459, 442], [-461, -476, -468, -465, -471, -454], [467, -457, -451, -456, -444, 476], [441, -447, -468], [445, 464, -453, -446, 469], [-454, -447, 478, -441], [-462, -458, 461], [444, -474, 453, -478, -451, -476], [-459, 444], [-472, 468, -479], [-460, -469, -456, 472], [-475, 466, -473], [454, -457, -451, -467, 447], [458, 453, -471], [-464, -467], [-452, 446, 479], [-462, -470, 473, 480, -454, 474], [-469, 462, 475], [-448, 465, 441], [461, -457, -450, 452, 470, 467, 471], [445, 480, 461], [464, 480, 459, -450], [-474, 472, -468, -466, -462, 456], [-472, 457], [459, -451, -476, -457, -450], [478, 457], [480, -477], [443, 473, -458], [-456, 471, -444, 457, -452, -465], [-453, -472, 442, 448, 455], [447, 475, -449], [-468, -479], [-480, 465, -466], [447, 469, 468, 464, 479, 462, -471], [457, 455, 454, -441, 449], [460, 448, -446, -453, 462, 456], [-476, 457, 454, 474, -469, -479, -451], [468, -479, -456], [466, -473], [472, 467, -477], [-466, 471, 457, -444], [465, 475], [466, -476, -475], [466, 449, -474, 465, -453, -467, -444], [452, -474, 459], [475, 461, -441, -454, -453, -460, -444, 473], [-466, -461, -471, -462, -475], [-451, 446, -480], [-443, -473], [467, 449, -463, 469], [450, 478, 472, -458, 465], [480, -478], [465, -445, 456, 441, 458, -453, -452, -472, 473], [451, -479, -471], [-455, 456, -463], [-479, 472, -475, -478, 453], [-463, -475, -479, 447, 480, 441, -458, 473], [-474, 456, 473, -472], [-461, 453, 443, -452, 466, -457, 471], [-478, 452, 465], [-464, 480], [-479, -461, -459, 452, -476, -453, -454, -468, 443, -477, 462], [447, 442], [-480, 461], [480, -479, 473], [-473, 466, -443, -475, 476, -461, -463], [-446, -459, -457, 476, 473, 474, 452], [448, 480, -469], [452, -477, 446, 479], [-457, -472, -466, -470, -449, -469, -453, -475], [-451, 453, 478], [466, -460, 458, -473], [-470, -469, -447, 480], [-477, 465, -476], [-445, 473, -464, -442, -480, -450], [-444, -461, 480, -441], [-447, -462, -448], [465, -446], [-462, -461, -467, 443, -472, 455, 475], [-468, 473, -446, -460, 476, -449, -465, 453], [-442, 473, -449], [464, 461, -479], [-462, -478, -461, 474, -450, 447, -466], [447, 452, -441, -473, 444], [-446, -455, 442, 462], [-462, 474, -475, 444], [-459, -443, -445, -475, 451, 461, 470, 464], [475, -464], [-474, -480, 457, 466, -451, 472], [-472, 476, 470], [-466, 471, -467], [483, -513, 486], [487, 493, -490], [499, -511, -508], [-482, 491, 519, -516], [500, -511, -508, 491], [-516, -502], [503, 512], [520, -487, -503, 485], [-515, 498], [-483, 489], [-485, -514, -503, -487], [-506, -507, 518, 490, 513], [-518, 502, -490, -497, -511], [-516, 510, -511], [-519, 483, 510], [-481, 505, 510], [-485, 495, 496, -515, -516, -499, -504], [-506, -493, 519, -500, 502], [500, -484, 501, -486], [-509, -495, 487, -492, 485], [488, -514, -513], [-505, 504, 488], [-498, 507], [-515, -499, -513, 507, -498, -518], [-507, -501, -505, 502], [-506, -517, 489], [481, 504, -507, 493], [514, -490, 494], [-511, -488, 508, 485], [-497, 509, 485], [510, -503, -481, 490], [496, 510, 519, 492, 490, -497, 504, -488, 494, -484], [-509, -484, 505], [510, 496], [481, 500, 513, 487], [519, 492, 498, 488, 503, -485, 483, 490, 486], [-517, 481, -516, -495, 488, 494, -492, -518, 500, 497], [-518, 487, 507], [517, -510, -499], [505, -487], [481, -513, 518, -496], [-499, 496, 514, -495, -492, 487], [483, -517, -489], [517, 503, 518, -493], [512, 513, 514, -517, -488], [490, 485, 482, -504, 492], [-501, -517], [-515, -481], [-492, 490, 497, 482, -510, -504, 491, 506, -484], [505, 495, -484], [495, 487, -494], [495, -509, -504], [481, 520, -503, -488], [497, 487, -490], [-491, -506, -502, 518], [516, -486, -510, -494, 515], [-481, -514, -484, -520, 489, -495], [-506, -487, 503, 512, 493, 481, -505, 511, 518, 509, 494, -510], [-483, 492, -494, 485], [483, -487, 496, 502, -491, 512, 511], [517, -502, -514, -489, -510, -516, -506, 509, 491, -484], [481, 495, -514], [-484, -491], [488, 512, 498, -507], [-501, -502, -512, 520, -505, -481, -499], [520, -503, -501, -518, 502, 499, -513], [495, -512, 503], [520, -510, -488, 481, -509], [-483, 498, -482, -511], [-502, -490, 513], [-499, 482, -511, -517, 510, 485, 507], [-509, 508], [483, -502, -495], [-481, -514, -496], [-513, 503, -484], [-485, 519, -491, -482], [515, -502, -491, -520], [489, 493, -500, 509, 505, 485, 482], [-504, 511, -483, -515], [-496, -493, 513], [-485, -491], [-520, 487, 494, -500, -483, 503, -512, 484], [519, 518, 508], [-499, 514, -495, 485, -507], [520, 517, 519], [483, 497, -516, 491, 508, -493], [491, 509, 497, -508, -515], [499, 500, 495, 501, 519], [-498, -514, 505], [498, 499, -496, 513, -488, 503, 508, -495, -512, -497, -483], [-490, 495, -513], [-489, -505, -508], [-498, 497, 502], [-520, -484, 496, -486, -493], [515, 495, 494, -519, 488, -520, -499], [514, 491, -513, -509, 492], [498, -514, -487], [-515, -508, 514], [-493, 485, -486], [489, -505, -481], [-491, -515, 484, -492, 499], [-485, 508, -514, -482, -499], [-501, -481, 498], [-517, -485, 495, 503, -492, 516, -514, -510, 484, 490], [509, -498, -517, 489], [519, 482, -505, 494], [-508, 509, -503, 493, -506], [-495, 483], [495, 502, 485, -520, 487], [-516, 508, 488], [507, 498, -493], [-492, -486, 519, -512], [510, 487, -501], [512, -516, -510, 511], [500, 518, 504], [-514, 515, 493, 501, 485], [-489, 486, -493, 500, 507, 488], [-500, -497, 506, 513], [-517, -511, 483], [-518, -497, -504], [-488, -487, 492], [-483, -484, -493], [-487, -508, 500, -490, -516], [-486, 513, -502, 505, 508], [-486, 483, 490, 491, -499, 497, -508, -507], [-498, 494, 514], [-515, 520, -511, 507, 500], [-503, 496, 490], [-485, -491, -498], [511, -508, -499, -486, -520], [-482, -511, -515, 487], [-520, 510, 497], [499, 492, 517], [-487, -484], [-489, 506, -515, -482, 493], [509, -490, -492, 486, 482, 504], [-518, 498, 497], [514, 518, -490], [-516, 505, -493], [482, -507, 496, 490, 504, -513, -518, 499, 517], [-513, 497, -498], [-490, 495, -506, -493, 516, -504, 508, 481], [-511, -481, -501, -487, -513, 496, 506, 494], [-492, -482, 501, 494], [520, 518, -509, 503], [-482, -496, -511, 489], [-487, 492, 504, -519, -505, -515, 497, 508, 500], [519, 510, -515, -489], [496, -491, 520], [487, -502, 498], [488, 484, -517, -511], [-488, -487, -489, -508], [481, 502, -485, 483, 482, -507, 508, 488], [497, -517], [514, 509, -502, 482, 513, -483], [-481, -502, 496, 488, -509, 519, -494, 515, -499, 506], [491, -509, -494], [505, 514, -507, 518, -489, -499], [-517, -492, 483, -499, 520], [-498, -513, -503, -492], [-503, -512, -517, -518, -505, -490, 509, -485, -496], [-519, 520], [-503, 502, -512, -517, 486, -484, -491], [501, -503], [-516, -492, -484], [-504, 492, -499], [-488, 490, -496], [-481, -484, 512, 516, 510, -493, -492], [506, -495, 508, -497, -485, -516, 509, 518, -513, -496, 505], [-497, -492, 498], [511, -486, 497, 488, 514, 493, -512], [509, 508, 503, 486], [511, 491, -508, -494], [510, 485, 488, 515], [519, -508], [483, -497, 513, 487], [-520, -513, 517], [-492, 481, -497], [513, 503, 520, 500], [-518, -502], [-487, 504, 519, 484], [490, -499, 517, 506], [508, 491, 494, 481], [489, -481, -488, -486, 520, -513], [520, -489, -490, -512], [-490, 519], [-496, -483, 491, -519, 508], [-515, -505, 509, -506, -483], [499, -488, -495, 484, -496], [-506, 520, 517, 494], [495, 501, -490, 481], [488, -501, 499, 505, 507], [492, 506], [517, 519, -503, -482, 499], [481, 493, 494, 512, -497, -513], [-484, 498, 500, -516, -506, -482, 514], [500, 512, -509], [482, 513, 509], [-511, 512, -488, -509], [489, -494, 503, -492], [482, 506, 520], [500, -493, 512, -517, 483, -484], [511, 510, 509], [-487, 488, -489, -492, 516, 505, -494], [506, -504, -489], [506, 486, -502], [487, -488, -502, 515], [-493, -486, 503, 517, -518], [-517, -497, -513], [517, 487], [-482, 502, 506, -489], [500, 483, 499], [517, -512, -515, -520], [-513, -517, 495, 497, -491, -494, 504, -485, -489], [-501, -485, -498, -506, 515, 510, 499], [491, 511, 510, 512], [-500, 501, -511, 484, -491], [-482, -508, 516], [494, 488, 498], [508, 520, -484], [489, 513, 484], [-518, -482, -499, -505, -506], [-492, 509, -487, 505], [-489, -494, -491], [-501, -513], [501, -505, 504, 497, -492, 513, -481], [-520, -488, 512, -506], [-487, -513, -520], [-494, -501, 486, -484, 520, -504, 502, 491], [504, 511, -513], [-507, 494, -483], [-489, 483], [-494, -500, 499, -495, 491], [-485, -503, 510], [481, -487, 492, -507, 485], [506, -513, -519, 507, 502], [-492, 497, -495], [-509, -515, -506, -490, 512, -495, 489, 519], [488, -490, -507, -481], [-520, 517, 518], [498, -481, -519], [520, 481], [502, -493, 505, -508, 513, -484, -519, -515], [488, 498], [493, 486], [487, 491, -492, -483, 507, -510, 489], [507, 512, 520], [-484, -507, 520], [-486, 511], [490, -508, -512, 496, 488, -517, -503, -482, -498, -502, -492], [-483, -502, -486, -506], [511, 518, 497], [-499, 492], [-500, -494], [523, -553, 526], [527, 533, -530], [539, -551, -548], [-522, 531, 559, -556], [540, -551, -548, 531], [-556, -542], [543, 552], [560, -527, -543, 525], [-555, 538], [-523, 529], [-525, -554, -543, -527], [-546, -547, 558, 530, 553], [-558, 542, -530, -537, -551], [-556, 550, -551], [-559, 523, 550], [-521, 545, 550], [-525, 535, 536, -555, -556, -539, -544], [-546, -533, 559, -540, 542], [540, -524, 541, -526], [-549, -535, 527, -532, 525], [528, -554, -553], [-545, 544, 528], [-538, 547], [-555, -539, -553, 547, -538, -558], [-547, -541, -545, 542], [-546, -557, 529], [521, 544, -547, 533], [554, -530, 534], [-551, -528, 548, 525], [-537, 549, 525], [550, -543, -521, 530], [536, 550, 559, 532, 530, -537, 544, -528, 534, -524], [-549, -524, 545], [550, 536], [521, 540, 553, 527], [559, 532, 538, 528, 543, -525, 523, 530, 526], [-557, 521, -556, -535, 528, 534, -532, -558, 540, 537], [-558, 527, 547], [557, -550, -539], [545, -527], [521, -553, 558, -536], [-539, 536, 554, -535, -532, 527], [523, -557, -529], [557, 543, 558, -533], [552, 553, 554, -557, -528], [530, 525, 522, -544, 532], [-541, -557], [-555, -521], [-532, 530, 537, 522, -550, -544, 531, 546, -524], [545, 535, -524], [535, 527, -534], [535, -549, -544], [521, 560, -543, -528], [537, 527, -530], [-531, -546, -542, 558], [556, -526, -550, -534, 555], [-521, -554, -524, -560, 529, -535], [-546, -527, 543, 552, 533, 521, -545, 551, 558, 549, 534, -550], [-523, 532, -534, 525], [523, -527, 536, 542, -531, 552, 551], [557, -542, -554, -529, -550, -556, -546, 549, 531, -524], [521, 535, -554], [-524, -531], [528, 552, 538, -547], [-541, -542, -552, 560, -545, -521, -539], [560, -543, -541, -558, 542, 539, -553], [535, -552, 543], [560, -550, -528, 521, -549], [-523, 538, -522, -551], [-542, -530, 553], [-539, 522, -551, -557, 550, 525, 547], [-549, 548], [523, -542, -535], [-521, -554, -536], [-553, 543, -524], [-525, 559, -531, -522], [555, -542, -531, -560], [529, 533, -540, 549, 545, 525, 522], [-544, 551, -523, -555], [-536, -533, 553], [-525, -531], [-560, 527, 534, -540, -523, 543, -552, 524], [559, 558, 548], [-539, 554, -535, 525, -547], [560, 557, 559], [523, 537, -556, 531, 548, -533], [531, 549, 537, -548, -555], [539, 540, 535, 541, 559], [-538, -554, 545], [538, 539, -536, 553, -528, 543, 548, -535, -552, -537, -523], [-530, 535, -553], [-529, -545, -548], [-538, 537, 542], [-560, -524, 536, -526, -533], [555, 535, 534, -559, 528, -560, -539], [554, 531, -553, -549, 532], [538, -554, -527], [-555, -548, 554], [-533, 525, -526], [529, -545, -521], [-531, -555, 524, -532, 539], [-525, 548, -554, -522, -539], [-541, -521, 538], [-557, -525, 535, 543, -532, 556, -554, -550, 524, 530], [549, -538, -557, 529], [559, 522, -545, 534], [-548, 549, -543, 533, -546], [-535, 523], [535, 542, 525, -560, 527], [-556, 548, 528], [547, 538, -533], [-532, -526, 559, -552], [550, 527, -541], [552, -556, -550, 551], [540, 558, 544], [-554, 555, 533, 541, 525], [-529, 526, -533, 540, 547, 528], [-540, -537, 546, 553], [-557, -551, 523], [-558, -537, -544], [-528, -527, 532], [-523, -524, -533], [-527, -548, 540, -530, -556], [-526, 553, -542, 545, 548], [-526, 523, 530, 531, -539, 537, -548, -547], [-538, 534, 554], [-555, 560, -551, 547, 540], [-543, 536, 530], [-525, -531, -538], [551, -548, -539, -526, -560], [-522, -551, -555, 527], [-560, 550, 537], [539, 532, 557], [-527, -524], [-529, 546, -555, -522, 533], [549, -530, -532, 526, 522, 544], [-558, 538, 537], [554, 558, -530], [-556, 545, -533], [522, -547, 536, 530, 544, -553, -558, 539, 557], [-553, 537, -538], [-530, 535, -546, -533, 556, -544, 548, 521], [-551, -521, -541, -527, -553, 536, 546, 534], [-532, -522, 541, 534], [560, 558, -549, 543], [-522, -536, -551, 529], [-527, 532, 544, -559, -545, -555, 537, 548, 540], [559, 550, -555, -529], [536, -531, 560], [527, -542, 538], [528, 524, -557, -551], [-528, -527, -529, -548], [521, 542, -525, 523, 522, -547, 548, 528], [537, -557], [554, 549, -542, 522, 553, -523], [-521, -542, 536, 528, -549, 559, -534, 555, -539, 546], [531, -549, -534], [545, 554, -547, 558, -529, -539], [-557, -532, 523, -539, 560], [-538, -553, -543, -532], [-543, -552, -557, -558, -545, -530, 549, -525, -536], [-559, 560], [-543, 542, -552, -557, 526, -524, -531], [541, -543], [-556, -532, -524], [-544, 532, -539], [-528, 530, -536], [-521, -524, 552, 556, 550, -533, -532], [546, -535, 548, -537, -525, -556, 549, 558, -553, -536, 545], [-537, -532, 538], [551, -526, 537, 528, 554, 533, -552], [549, 548, 543, 526], [551, 531, -548, -534], [550, 525, 528, 555], [559, -548], [523, -537, 553, 527], [-560, -553, 557], [-532, 521, -537], [553, 543, 560, 540], [-558, -542], [-527, 544, 559, 524], [530, -539, 557, 546], [548, 531, 534, 521], [529, -521, -528, -526, 560, -553], [560, -529, -530, -552], [-530, 559], [-536, -523, 531, -559, 548], [-555, -545, 549, -546, -523], [539, -528, -535, 524, -536], [-546, 560, 557, 534], [535, 541, -530, 521], [528, -541, 539, 545, 547], [532, 546], [557, 559, -543, -522, 539], [521, 533, 534, 552, -537, -553], [-524, 538, 540, -556, -546, -522, 554], [540, 552, -549], [522, 553, 549], [-551, 552, -528, -549], [529, -534, 543, -532], [522, 546, 560], [540, -533, 552, -557, 523, -524], [551, 550, 549], [-527, 528, -529, -532, 556, 545, -534], [546, -544, -529], [546, 526, -542], [527, -528, -542, 555], [-533, -526, 543, 557, -558], [-557, -537, -553], [557, 527], [-522, 542, 546, -529], [540, 523, 539], [557, -552, -555, -560], [-553, -557, 535, 537, -531, -534, 544, -525, -529], [-541, -525, -538, -546, 555, 550, 539], [531, 551, 550, 552], [-540, 541, -551, 524, -531], [-522, -548, 556], [534, 528, 538], [548, 560, -524], [529, 553, 524], [-558, -522, -539, -545, -546], [-532, 549, -527, 545], [-529, -534, -531], [-541, -553], [541, -545, 544, 537, -532, 553, -521], [-560, -528, 552, -546], [-527, -553, -560], [-534, -541, 526, -524, 560, -544, 542, 531], [544, 551, -553], [-547, 534, -523], [-529, 523], [-534, -540, 539, -535, 531], [-525, -543, 550], [521, -527, 532, -547, 525], [546, -553, -559, 547, 542], [-532, 537, -535], [-549, -555, -546, -530, 552, -535, 529, 559], [528, -530, -547, -521], [-560, 557, 558], [538, -521, -559], [560, 521], [542, -533, 545, -548, 553, -524, -559, -555], [528, 538], [533, 526], [527, 531, -532, -523, 547, -550, 529], [547, 552, 560], [-524, -547, 560], [-526, 551], [530, -548, -552, 536, 528, -557, -543, -522, -538, -542, -532], [-523, -542, -526, -546], [551, 558, 537], [-539, 532], [540, -534], [591, -572, -566, -576, -592], [583, -591, -563], [563, -596, 580, -592, -583, -586], [-563, -586, -576, -561, 584, 590], [-589, 597], [-591, -580, 577, -568, -562, 597, 565, -570, 584, 561], [-585, 561, -563], [587, -593, 569], [-564, 589], [589, -572, 583, 593, -568, 565, 571, -582, 566], [-574, -582, -587], [565, -588, -586], [-581, -567, -595], [-579, -566, 588], [-592, -562, -587], [-593, 566], [572, -595, 571, -580, -570], [586, 581, 565, 571, 578, -580, -566], [564, 599, -565], [596, 590, 589], [-562, -588, -600], [565, -594, 593], [-590, -575, -569], [-588, 565, 583, 597, 582, 563, -586], [-578, -561, -580, 579], [596, -584, 595], [590, -575], [-598, -563, 591, -593], [597, 564, 585, -573, 575, 577, -562], [577, -590, -596], [-589, 592, 577, -576], [580, 583, -599], [574, -577, -568], [-585, 596, 578], [-585, -595, -568, -565, 580, 566, -596, -577, -600], [600, -591, -571, 594], [600, 583, -570, -566], [575, 590, 565, -577], [590, -571, 580], [-592, -600, 569, 588], [581, 596, 563, 589, 585, 572, -562], [591, -595], [564, 583, 580], [-578, -571, 563], [-597, 587, 571, 566], [569, 591, 577], [-587, 589, 561, -564], [571, 563, 594], [562, -600, -589, 584], [574, 561, 571, -575], [597, -567, -583], [574, 564, 582], [596, 561, -573, -564, 571, 568, 595, -589], [-568, 576, 596], [590, -566, -573], [586, -576, -587, -591, 585], [-575, -571, 580, 577, -593, 569, -578, -572, -576, 595, 586], [-594, -572, -580, -570, -590, -595, 566, 578], [574, 582, 591, -586], [-579, 577, -582, 581], [-590, -568, -569, -577], [562, -600, -567, 595, 586], [-567, 572, 580], [-571, 564, 565, -589], [-600, 596, -562, -565, 567, -599, 570, 595, 579, -594], [572, 588, 584, -591], [-565, -596, -600, -572, 582, -576, 580], [564, 577, 562], [598, -575, -577, -572, 580, -591, -585], [566, -597, 579, 572], [-576, -564, 595, 567], [585, 571, 572], [-578, -567, 579, -564], [580, 592, -563, 562], [574, 562, -589], [566, -587, 572, 561, 565, 578, -569], [588, 575, 576, 582, 564], [566, -582, -562, -593], [575, -572, 568, -571], [-591, -600, 585, 576, 569, -575], [597, -574, 565, -583, -595, 593], [-591, 573, -572], [-571, -562, -563, -565, -580, 600, 572, -590], [581, 595, -564, -585, -571, -569], [578, -574, -597], [-580, -572, 567, -600, -588], [575, -592, 582, 600], [-600, -564, 567, 586, 595, 588], [-600, -574, -596, 594], [-592, -600, -598], [565, 573, -588, 575], [-597, -578, -581], [569, -594, 589, -575], [591, 590, 600, 566], [564, -580, -566, 573, 582], [-584, -597, -594], [-563, 598, -597, -592, -586], [593, -587, -567, 575], [-598, -563, 568, -575], [578, -599, 598, 574], [-585, -595, -593], [-592, -593, -591, 588, -586, 598, -596, -564], [-587, -563, -598, 592, 570], [573, -584, -587, 576], [-594, -576, 565, -600], [580, 594, -595, 575, -582], [-561, 597, -595], [576, 568, 591], [590, 594], [566, -568, 578], [579, -567, -561], [-568, -571], [-568, 586, 595, -592, 591], [-586, -581, 595, -570], [580, -600, 570, -571], [-600, -564, 563, -583], [571, 590, -563], [563, 581, 568], [-569, -570, -579, 593, -568, 575, -580, 586], [-584, -563, 593], [-576, 597, -578, 590], [-594, -572, -590, 585], [580, -567, 593, 562, -573, 597, -578], [-579, -573, -564, 587, 561], [598, 590, -565], [575, -591, -587, 594, 562], [-600, 581, -573, -585], [589, 564, 591, 598], [-578, 584, 588], [599, -571, -595, 584, 586], [-591, -584, 581, -561, -572, 598, 593, -573], [590, -570, 598, 600, -588, -579, -582, -585], [581, 580, 594, 573, 590, 563, -574, 585], [-576, 566], [-580, 563, -569, -587, 599, -568, -574, 598], [587, -562, 593], [578, -566, 571, 570, -565, -600, 584, -598, 587, 595], [-584, 575, -594], [586, -573, -596], [561, -596, 565], [570, -599, -572, -581], [-570, 562, -564, -582], [-577, 572, -585, -584], [-593, -566, -585], [590, -585, -584, -596, 599], [-589, -573, -564], [590, -585, -598], [-588, 585, 570, -574, 587], [566, 567, 599], [-595, -574, 584, -565, 591], [-577, -582, 569], [-563, 575, -572], [598, -583, 566], [563, 591, -564, 579], [570, 582, -569, 580, 564], [594, 592, -574, -566, 573, 562], [574, 580], [562, 585, 595], [-578, 562, -582], [595, 561], [574, 577, -597, -565], [-595, -599, 569], [592, -582, -599], [597, 564, 586], [572, -593, -567], [-599, 583, -572, 580, -578], [-583, -590], [570, 569, 599, 588], [-574, 589, 577], [-586, -589, 572, -595, 561], [575, 582, 564, 588, 592, 589, -568, -565], [594, 566, -589], [576, -567, -572, 592], [578, -573, 583], [566, -596, -586, -580], [-587, -585, 598, 566], [-569, -568, -592, -599], [-599, 583, -577], [-589, 583, 599], [-593, -583, -563, 591], [-592, 582, -581], [-583, 573, -577], [-566, 592, -599], [596, -590, 587, 572], [-561, 600, 568, 595, -579, 596], [-562, 589], [592, 586], [-574, 581, 590], [582, -585, 576], [583, -568, 592], [-576, 586, 579, -578, 569, 587, 583, 585], [-569, -568, -586, 585, -573], [581, 590, 571, 591], [-592, -598], [600, -564, 585, 592], [-582, 570, 580], [-597, 590, 598, 586, 589, -578], [578, 570, -596], [-579, 585, 580], [574, -567], [596, 587], [569, -573], [574, 586, 564], [579, -564, 565], [-599, -583], [574, 565, -589], [589, -586, 600, -563, -581, -575, 574], [-591, -565], [-596, -571, 563, 597, 580], [588, -575, -580], [571, 561, -564], [571, 597, -584, -593, 564], [-600, 595], [-561, -590, -599, -592], [595, 569, -594, 564, -587], [587, 561, 562], [-567, 584, 571, -565, -579, -592, -594], [561, 580, -585, -594, 564, 565, 578, -582, -572], [-568, -569, 586, 574], [575, -590, 594, 585, -573, -571, -561, 563], [-585, -582], [563, 579, -594, 599, -596, -593], [595, 594], [631, -612, -606, -616, -632], [623, -631, -603], [603, -636, 620, -632, -623, -626], [-603, -626, -616, -601, 624, 630], [-629, 637], [-631, -620, 617, -608, -602, 637, 605, -610, 624, 601], [-625, 601, -603], [627, -633, 609], [-604, 629], [629, -612, 623, 633, -608, 605, 611, -622, 606], [-614, -622, -627], [605, -628, -626], [-621, -607, -635], [-619, -606, 628], [-632, -602, -627], [-633, 606], [612, -635, 611, -620, -610], [626, 621, 605, 611, 618, -620, -606], [604, 639, -605], [636, 630, 629], [-602, -628, -640], [605, -634, 633], [-630, -615, -609], [-628, 605, 623, 637, 622, 603, -626], [-618, -601, -620, 619], [636, -624, 635], [630, -615], [-638, -603, 631, -633], [637, 604, 625, -613, 615, 617, -602], [617, -630, -636], [-629, 632, 617, -616], [620, 623, -639], [614, -617, -608], [-625, 636, 618], [-625, -635, -608, -605, 620, 606, -636, -617, -640], [640, -631, -611, 634], [640, 623, -610, -606], [615, 630, 605, -617], [630, -611, 620], [-632, -640, 609, 628], [621, 636, 603, 629, 625, 612, -602], [631, -635], [604, 623, 620], [-618, -611, 603], [-637, 627, 611, 606], [609, 631, 617], [-627, 629, 601, -604], [611, 603, 634], [602, -640, -629, 624], [614, 601, 611, -615], [637, -607, -623], [614, 604, 622], [636, 601, -613, -604, 611, 608, 635, -629], [-608, 616, 636], [630, -606, -613], [626, -616, -627, -631, 625], [-615, -611, 620, 617, -633, 609, -618, -612, -616, 635, 626], [-634, -612, -620, -610, -630, -635, 606, 618], [614, 622, 631, -626], [-619, 617, -622, 621], [-630, -608, -609, -617], [602, -640, -607, 635, 626], [-607, 612, 620], [-611, 604, 605, -629], [-640, 636, -602, -605, 607, -639, 610, 635, 619, -634], [612, 628, 624, -631], [-605, -636, -640, -612, 622, -616, 620], [604, 617, 602], [638, -615, -617, -612, 620, -631, -625], [606, -637, 619, 612], [-616, -604, 635, 607], [625, 611, 612], [-618, -607, 619, -604], [620, 632, -603, 602], [614, 602, -629], [606, -627, 612, 601, 605, 618, -609], [628, 615, 616, 622, 604], [606, -622, -602, -633], [615, -612, 608, -611], [-631, -640, 625, 616, 609, -615], [637, -614, 605, -623, -635, 633], [-631, 613, -612], [-611, -602, -603, -605, -620, 640, 612, -630], [621, 635, -604, -625, -611, -609], [618, -614, -637], [-620, -612, 607, -640, -628], [615, -632, 622, 640], [-640, -604, 607, 626, 635, 628], [-640, -614, -636, 634], [-632, -640, -638], [605, 613, -628, 615], [-637, -618, -621], [609, -634, 629, -615], [631, 630, 640, 606], [604, -620, -606, 613, 622], [-624, -637, -634], [-603, 638, -637, -632, -626], [633, -627, -607, 615], [-638, -603, 608, -615], [618, -639, 638, 614], [-625, -635, -633], [-632, -633, -631, 628, -626, 638, -636, -604], [-627, -603, -638, 632, 610], [613, -624, -627, 616], [-634, -616, 605, -640], [620, 634, -635, 615, -622], [-601, 637, -635], [616, 608, 631], [630, 634], [606, -608, 618], [619, -607, -601], [-608, -611], [-608, 626, 635, -632, 631], [-626, -621, 635, -610], [620, -640, 610, -611], [-640, -604, 603, -623], [611, 630, -603], [603, 621, 608], [-609, -610, -619, 633, -608, 615, -620, 626], [-624, -603, 633], [-616, 637, -618, 630], [-634, -612, -630, 625], [620, -607, 633, 602, -613, 637, -618], [-619, -613, -604, 627, 601], [638, 630, -605], [615, -631, -627, 634, 602], [-640, 621, -613, -625], [629, 604, 631, 638], [-618, 624, 628], [639, -611, -635, 624, 626], [-631, -624, 621, -601, -612, 638, 633, -613], [630, -610, 638, 640, -628, -619, -622, -625], [621, 620, 634, 613, 630, 603, -614, 625], [-616, 606], [-620, 603, -609, -627, 639, -608, -614, 638], [627, -602, 633], [618, -606, 611, 610, -605, -640, 624, -638, 627, 635], [-624, 615, -634], [626, -613, -636], [601, -636, 605], [610, -639, -612, -621], [-610, 602, -604, -622], [-617, 612, -625, -624], [-633, -606, -625], [630, -625, -624, -636, 639], [-629, -613, -604], [630, -625, -638], [-628, 625, 610, -614, 627], [606, 607, 639], [-635, -614, 624, -605, 631], [-617, -622, 609], [-603, 615, -612], [638, -623, 606], [603, 631, -604, 619], [610, 622, -609, 620, 604], [634, 632, -614, -606, 613, 602], [614, 620], [602, 625, 635], [-618, 602, -622], [635, 601], [614, 617, -637, -605], [-635, -639, 609], [632, -622, -639], [637, 604, 626], [612, -633, -607], [-639, 623, -612, 620, -618], [-623, -630], [610, 609, 639, 628], [-614, 629, 617], [-626, -629, 612, -635, 601], [615, 622, 604, 628, 632, 629, -608, -605], [634, 606, -629], [616, -607, -612, 632], [618, -613, 623], [606, -636, -626, -620], [-627, -625, 638, 606], [-609, -608, -632, -639], [-639, 623, -617], [-629, 623, 639], [-633, -623, -603, 631], [-632, 622, -621], [-623, 613, -617], [-606, 632, -639], [636, -630, 627, 612], [-601, 640, 608, 635, -619, 636], [-602, 629], [632, 626], [-614, 621, 630], [622, -625, 616], [623, -608, 632], [-616, 626, 619, -618, 609, 627, 623, 625], [-609, -608, -626, 625, -613], [621, 630, 611, 631], [-632, -638], [640, -604, 625, 632], [-622, 610, 620], [-637, 630, 638, 626, 629, -618], [618, 610, -636], [-619, 625, 620], [614, -607], [636, 627], [609, -613], [614, 626, 604], [619, -604, 605], [-639, -623], [614, 605, -629], [629, -626, 640, -603, -621, -615, 614], [-631, -605], [-636, -611, 603, 637, 620], [628, -615, -620], [611, 601, -604], [611, 637, -624, -633, 604], [-640, 635], [-601, -630, -639, -632], [635, 609, -634, 604, -627], [627, 601, 602], [-607, 624, 611, -605, -619, -632, -634], [601, 620, -625, -634, 604, 605, 618, -622, -612], [-608, -609, 626, 614], [615, -630, 634, 625, -613, -611, -601, 603], [-625, -622], [603, 619, -634, 639, -636, -633], [-635, 634], [-648, 662, 657], [677, -667, -675, 641, -673, 657], [-641, -647, -657, 654], [672, 656, -653, -668], [678, -657, 661, -646, 667, -652, -653, -658, -649], [645, -641, 675, -664], [657, -655, -666, 671, 680], [-680, -658, 676], [-658, 669, -666, 651], [677, -670, -669, 650, -645, -657], [654, -668, -665], [655, 643, 674], [-663, -670, -645, 668], [675, 649, -655], [-660, 646, 661], [-658, 669, 680, -649, -662], [669, 665], [-673, -643, 680, -668, 653, 669], [-647, 667], [-658, -678, -653, 646, 676, 659, -665], [-652, 657], [-648, -662, -658, -666, 672, 651, 646], [658, -676, 675], [654, 670], [654, 680, 679, -661, -651], [650, -645, 671, 646, 679], [668, -644, -663, 677, -641, 673, -680, -670, 661, -669], [658, 674, 656], [-661, -668, -650, 656], [652, -658, -656, -643, 680, -666, 677, -660, -645], [-678, -670, 672], [-649, 660], [646, 644, 668], [-653, 679, -664, 669, -674, 648, -641, 676], [679, -647, -671], [660, 652, -663], [-678, 651, 656, 649, 665], [-675, -663, 680, -643, 647, 645, 676], [-645, -677, 669], [-662, -655, 676], [-643, 670, -667], [666, -674, 658], [-641, -667, -680, -648, 649], [665, -677], [-652, 676, 653, -670, -655], [-669, 647, 643, -645], [-668, 678, 679, 653], [-643, -666, -673], [646, -644, 674, -651, -670], [653, 661, 667], [-665, 673, 661], [661, 664, -652], [646, 643, 680, 677], [667, 644, -673, -655, -643, -646], [-669, 642, -655, -660, -648], [-668, 644, -680], [-642, 673, 650, 664, -652, 679, 668, 665, 669, -657], [649, 654, -658, 667], [-678, 665, -651], [641, -677, -644, 650, 660, 670, -662], [-651, -680, 658, 641], [667, 654, 671], [-655, 651, 659, -678, 680], [674, 659, -672], [-679, -670, -667], [675, 653, -679, 668, 647, -664, 644], [654, -662, -650], [641, -668, -657], [-660, -668, -664, 667, -642, -647, 670], [668, 667, 653, -657, -680, 651], [-655, -651, -650, 674, -676], [650, -655, -678], [-663, -673, 661, 653, 646, -660, -665, -652, -678], [-675, 644, 658], [-657, -660, -641], [-660, -647, 642], [666, -655], [661, 659, 664, -672], [666, 656, -642, -658], [657, 674, -658], [-680, -656, 649], [644, -647, 658, -664, -669, -660, -654, 678], [-662, 677, 674, 680, -651], [653, 661, -671], [671, 655], [677, 678, 679, -646, 654, 670, 650, 642, -673, -658, -663], [646, -648, -672], [-673, -659, 665], [-670, 643, -642], [662, 677, 670], [679, 655, -647], [642, -647, -646, 671, -674], [-646, 662, 660, 657, 656, -644, 672, 649], [665, 655, 662], [-644, -664, -678], [-655, -677], [659, -680, -651, -650, 670], [645, -670], [-655, 669, -650], [661, 673], [-648, -664], [666, -658, -675, 663, 648, 676, -643, 660, -656], [676, 659, -662, 642], [667, -668, 646], [-652, 644], [-665, 678, 670, -652], [668, -646, -655, -661], [-655, 676, 660], [-656, -655, -664], [-658, 666, -642, -659], [-646, -642], [663, -662, -660, 653, -671], [648, 670, 659, 665, -644], [671, 663, -678], [-663, 641, -667, 651], [-680, 643, -679, -673, -663], [647, -653, 645], [643, 660, 658, 647, -680], [-670, -646, 647], [-644, -670, 662], [666, -668], [-667, 643], [-666, -647, -664], [-672, 661], [667, -654, 644], [-665, 666, -644, 641, -646, 679, 672], [-649, 680, -677, -678, 642], [676, 668, 658, 643], [-663, -675, -648], [-642, 671, -647, 674, -643], [-654, 674, 658], [650, 655, 641, -657], [662, -653, -657, -647, -652, 667, -654, -673, 644], [-646, -656, 648, -667, -669, -641], [661, 664, -674, -669, -662], [-653, -662, 650], [-646, 644, 665, -656, -674, 651, 645], [-673, -666], [671, 677, 667], [-672, 647, -642, -678], [-648, -657, 673, 643, -678, 677, -647, 641, -680, 665], [-662, -651, -644], [-671, -680, -644], [-661, -649, -660], [-642, 673, 677], [-645, -651], [673, -653], [665, 678, -666], [676, 645, 679], [679, -657, 641, -663, 678], [-663, -665, -675, 669, -650, 656, 659, 651, -641, 658, 664, 661], [645, -652, 642], [680, 650, 666, 670, -674], [661, 650, -641, 665], [-648, 677, 665, 653, 678], [650, 675, -658, 663, 667, 674, 666, 642, -653, -659, 651, 656, 677, 647, 670, 672, 678, -646, -645, 662], [665, -671, -648], [-646, -670, -667, 657, 663, 675, -662], [-657, -655, -660, 667, -651, -664], [657, -651, 664], [-653, 680], [649, 651, -641, -644, -673, -675], [-672, 679], [648, -650, 674, 669, -649], [658, -678, 650, 645], [-658, -668, 678], [-671, 676, -642, 662, -658], [-661, -675, -666], [661, -672], [-679, -666, -644], [679, -657, 645], [674, 654, -672], [-662, -672, 669, -680], [657, 662, -648, -672, -651, -660], [670, 666], [674, 670, 651], [-664, -653, 659, 671, -649], [-657, -664, -662, -652, 668, 665], [672, 678, 664], [-655, -670, -667], [-656, 664, -662, -653], [641, -676, 660], [-668, 663, -656, -671], [-680, 645, 652, -667, -655], [-672, 674, -651, 665], [-658, -671, -678], [663, -654, -655], [643, 656, 665, 666, 663, 657, 646], [-668, 642], [648, 666, 659, 651, 647], [672, 676, -644, -679, 661, -666, -652], [-645, 671, -676], [-643, -647, -642, 660], [-674, 655, 671, -676, 665, 658, -645, -672, 653, -663, -659, -650], [659, 647, -670, -648, -669], [-672, 646, -665, -645], [657, -660, -649, 650, 678, -644], [-656, 671, 658], [655, 678, -671, -650, 670], [-642, -672, -650, 667, 645, 641, -679, -659, 652], [-651, 667], [652, 664, -648], [-680, 653, -649, -678, 648, -656, -641], [-676, -677, -680], [-653, 661, -679], [-650, 678, -675, -674], [645, -680, -666], [-669, -661], [667, -675, 647], [-653, 652], [668, 660], [678, -668, 648, 662], [-666, 662, 643, -642], [-655, 642, 672], [-688, 702, 697], [717, -707, -715, 681, -713, 697], [-681, -687, -697, 694], [712, 696, -693, -708], [718, -697, 701, -686, 707, -692, -693, -698, -689], [685, -681, 715, -704], [697, -695, -706, 711, 720], [-720, -698, 716], [-698, 709, -706, 691], [717, -710, -709, 690, -685, -697], [694, -708, -705], [695, 683, 714], [-703, -710, -685, 708], [715, 689, -695], [-700, 686, 701], [-698, 709, 720, -689, -702], [709, 705], [-713, -683, 720, -708, 693, 709], [-687, 707], [-698, -718, -693, 686, 716, 699, -705], [-692, 697], [-688, -702, -698, -706, 712, 691, 686], [698, -716, 715], [694, 710], [694, 720, 719, -701, -691], [690, -685, 711, 686, 719], [708, -684, -703, 717, -681, 713, -720, -710, 701, -709], [698, 714, 696], [-701, -708, -690, 696], [692, -698, -696, -683, 720, -706, 717, -700, -685], [-718, -710, 712], [-689, 700], [686, 684, 708], [-693, 719, -704, 709, -714, 688, -681, 716], [719, -687, -711], [700, 692, -703], [-718, 691, 696, 689, 705], [-715, -703, 720, -683, 687, 685, 716], [-685, -717, 709], [-702, -695, 716], [-683, 710, -707], [706, -714, 698], [-681, -707, -720, -688, 689], [705, -717], [-692, 716, 693, -710, -695], [-709, 687, 683, -685], [-708, 718, 719, 693], [-683, -706, -713], [686, -684, 714, -691, -710], [693, 701, 707], [-705, 713, 701], [701, 704, -692], [686, 683, 720, 717], [707, 684, -713, -695, -683, -686], [-709, 682, -695, -700, -688], [-708, 684, -720], [-682, 713, 690, 704, -692, 719, 708, 705, 709, -697], [689, 694, -698, 707], [-718, 705, -691], [681, -717, -684, 690, 700, 710, -702], [-691, -720, 698, 681], [707, 694, 711], [-695, 691, 699, -718, 720], [714, 699, -712], [-719, -710, -707], [715, 693, -719, 708, 687, -704, 684], [694, -702, -690], [681, -708, -697], [-700, -708, -704, 707, -682, -687, 710], [708, 707, 693, -697, -720, 691], [-695, -691, -690, 714, -716], [690, -695, -718], [-703, -713, 701, 693, 686, -700, -705, -692, -718], [-715, 684, 698], [-697, -700, -681], [-700, -687, 682], [706, -695], [701, 699, 704, -712], [706, 696, -682, -698], [697, 714, -698], [-720, -696, 689], [684, -687, 698, -704, -709, -700, -694, 718], [-702, 717, 714, 720, -691], [693, 701, -711], [711, 695], [717, 718, 719, -686, 694, 710, 690, 682, -713, -698, -703], [686, -688, -712], [-713, -699, 705], [-710, 683, -682], [702, 717, 710], [719, 695, -687], [682, -687, -686, 711, -714], [-686, 702, 700, 697, 696, -684, 712, 689], [705, 695, 702], [-684, -704, -718], [-695, -717], [699, -720, -691, -690, 710], [685, -710], [-695, 709, -690], [701, 713], [-688, -704], [706, -698, -715, 703, 688, 716, -683, 700, -696], [716, 699, -702, 682], [707, -708, 686], [-692, 684], [-705, 718, 710, -692], [708, -686, -695, -701], [-695, 716, 700], [-696, -695, -704], [-698, 706, -682, -699], [-686, -682], [703, -702, -700, 693, -711], [688, 710, 699, 705, -684], [711, 703, -718], [-703, 681, -707, 691], [-720, 683, -719, -713, -703], [687, -693, 685], [683, 700, 698, 687, -720], [-710, -686, 687], [-684, -710, 702], [706, -708], [-707, 683], [-706, -687, -704], [-712, 701], [707, -694, 684], [-705, 706, -684, 681, -686, 719, 712], [-689, 720, -717, -718, 682], [716, 708, 698, 683], [-703, -715, -688], [-682, 711, -687, 714, -683], [-694, 714, 698], [690, 695, 681, -697], [702, -693, -697, -687, -692, 707, -694, -713, 684], [-686, -696, 688, -707, -709, -681], [701, 704, -714, -709, -702], [-693, -702, 690], [-686, 684, 705, -696, -714, 691, 685], [-713, -706], [711, 717, 707], [-712, 687, -682, -718], [-688, -697, 713, 683, -718, 717, -687, 681, -720, 705], [-702, -691, -684], [-711, -720, -684], [-701, -689, -700], [-682, 713, 717], [-685, -691], [713, -693], [705, 718, -706], [716, 685, 719], [719, -697, 681, -703, 718], [-703, -705, -715, 709, -690, 696, 699, 691, -681, 698, 704, 701], [685, -692, 682], [720, 690, 706, 710, -714], [701, 690, -681, 705], [-688, 717, 705, 693, 718], [690, 715, -698, 703, 707, 714, 706, 682, -693, -699, 691, 696, 717, 687, 710, 712, 718, -686, -685, 702], [705, -711, -688], [-686, -710, -707, 697, 703, 715, -702], [-697, -695, -700, 707, -691, -704], [697, -691, 704], [-693, 720], [689, 691, -681, -684, -713, -715], [-712, 719], [688, -690, 714, 709, -689], [698, -718, 690, 685], [-698, -708, 718], [-711, 716, -682, 702, -698], [-701, -715, -706], [701, -712], [-719, -706, -684], [719, -697, 685], [714, 694, -712], [-702, -712, 709, -720], [697, 702, -688, -712, -691, -700], [710, 706], [714, 710, 691], [-704, -693, 699, 711, -689], [-697, -704, -702, -692, 708, 705], [712, 718, 704], [-695, -710, -707], [-696, 704, -702, -693], [681, -716, 700], [-708, 703, -696, -711], [-720, 685, 692, -707, -695], [-712, 714, -691, 705], [-698, -711, -718], [703, -694, -695], [683, 696, 705, 706, 703, 697, 686], [-708, 682], [688, 706, 699, 691, 687], [712, 716, -684, -719, 701, -706, -692], [-685, 711, -716], [-683, -687, -682, 700], [-714, 695, 711, -716, 705, 698, -685, -712, 693, -703, -699, -690], [699, 687, -710, -688, -709], [-712, 686, -705, -685], [697, -700, -689, 690, 718, -684], [-696, 711, 698], [695, 718, -711, -690, 710], [-682, -712, -690, 707, 685, 681, -719, -699, 692], [-691, 707], [692, 704, -688], [-720, 693, -689, -718, 688, -696, -681], [-716, -717, -720], [-693, 701, -719], [-690, 718, -715, -714], [685, -720, -706], [-709, -701], [707, -715, 687], [-693, 692], [708, 700], [718, -708, 688, 702], [-706, 702, 683, -682], [695, 682, 712], [-758, -744, 731], [744, -746, 735], [-738, -734, -726, -754], [-754, 728, 759], [724, -737, -743], [-755, 736, -748, -721, -730, 731, 739, 756, -724, -743, -733], [721, 746, -743, -756, -736, 748], [744, -736], [-750, -739, -760, 735], [748, 724, 744], [723, 752, -722, -743, -745], [726, -722, 756, -741, 745, 759], [751, 734, -729, 722, 746, -759, 739, 731, 725], [-756, -726, -730, 736, 733], [-728, 759, -749, 723, -741, 724, -754], [737, 729, -723], [747, 732, 726], [722, -742, 744, 735], [740, 752, 760, -747, 736, -745, 746, -729, -727, 735, -754, 721, 741], [734, 732, -739, -727], [732, -739, -722, -745], [-741, -736], [-751, 747, -750, 726, 760], [757, 738, 747], [747, 744, -730], [-727, -725, 732], [-735, -748, -752, -759, 733], [759, 760, -725, -727, 732], [739, -726, 759], [738, -731, -743], [735, 754], [744, 750, 751, 737], [-752, 733, 742, 729, 744], [-725, 724, 727], [-744, 735, -760, -734, -723, -745], [756, 729, 749, -755, 739], [-757, -726, -753, 733, 743], [-751, -757, -726, -737, 739, -731], [749, 742, -730, 743, -759, -751, -735, 728], [728, -738, 760], [730, 734, 721, -727, -725], [-725, 741, 723, -735, 749, 731], [738, -759], [-760, -759, 748, 754, 752], [750, 729, 760, 748, -755], [735, 741], [-756, -737], [750, 747, -737], [-725, 721, 747, -743, 735], [-722, -745, -756, 751, -727], [-737, 741, -725, -724, -757, 752, -731, 750], [760, -722, -732], [736, 752, 751, 728], [-740, -737, -725, -738, -726, 749], [723, 759, -740], [725, -723, -745], [-746, 753, 759], [-751, 754, -727, 732], [-737, -751], [-731, -748, 750, -759], [736, -728, -722, 757], [-759, 732, 757, 738, -747, -760], [-742, 748, -727, 740], [727, -724, 757, -746], [722, 726, 728], [-734, 725, -721], [-758, 756, -759, -727, -723, -757, 755, 744, 737, -754, -729, 722, -753, 751], [739, -741, 723, -749], [-753, 746, -758, -739, 722, -733], [-758, 741], [734, -739, -759, -726], [738, -744, -759, -729, 740, -725], [-753, -745, 756], [748, -746, 757, -729], [-737, 759, -741, 738, -724, 733, -743], [742, -739, 756], [723, 746], [-752, 738, 729, -751], [755, -747, 741, -748], [757, 758, -745, 754, -743, 727, -728], [726, -721, -727], [735, 758, -748], [731, 722, 735, 738], [-751, -725, -730, 749, 748, 738], [-733, 752, 721], [-730, -747, -733, 742], [-749, -755], [750, -760, 753], [-733, -721], [-744, -734, 731], [747, -730, 753], [755, 758, 728, -721, 740, 724, 733], [725, -760], [-759, -741, -735, 743], [736, -731, 749, -734, -741, -759], [737, 734, -733, -735], [724, -731, -745], [-759, 722], [735, 757, 740, 728], [759, -738, -750, -760], [742, -750, 740], [-747, -759], [739, 737, 744], [-738, -730], [725, -754, 738], [755, -742, -759, -728], [-723, -733, -742, -749, 753], [725, 732, -723, -752, -751, 733], [735, 746, 754, 732, -749], [741, 725, 739, -757], [-758, -743, -724, -745], [758, 753, -739, 724, -732, -727, 740, -759], [-726, 758, -736, -729, -755, 727], [724, 738, 750], [723, 739, 754, -743], [758, -742], [-758, 730, 760, 745, -721, -748], [737, 733, 739, 743, 727], [-759, 721, -760, -735], [-736, 740, -749, 760, -741, -728], [739, -760, -744, 732, -731, -728], [-735, 751, 738, 728], [-735, -759, 727, -740, 728, 734], [-740, -756], [729, -757, 737, -754], [-740, -755, -749, 727], [752, 729, -753, 759], [736, 744, -752], [-742, 750], [-737, 758], [749, 733, 738], [749, 736, -753], [725, -741, -744], [725, 742, 759, -756], [-723, -726, -739, 729], [-757, -731, -723], [-746, -739, -754, 724, 728], [735, 754, 729, 745], [-728, -721, 760, 737], [-752, 728, -745], [739, 728, -752, -743, 753, 722, 757, -745, -750, 759], [753, 721, -738], [-739, -749, -722], [-758, -728], [743, 727, 721], [-746, -728, -737, -736], [-736, -747, 752], [738, 752, 754, 731, 747], [730, 725, -756], [-750, 743, 749, 759, -744, 754], [-723, -750, -748], [-728, 721, -727, -744], [-736, -748, -753], [739, -743, -726], [746, 754, 753, -751, -730, 726, 738, 752, -744, -722, -734], [-754, -724, -748], [731, -744, -759], [-753, -743], [-747, 760, -757], [722, 737, 725, 745, -752, 726], [725, 724, -722, -759], [734, 745, -749], [738, 723, 730, -737], [-755, 735, -749, -725], [-754, -751, 745, -736, 757, -727, 748], [-724, -740, -747], [737, 754, -759, 724], [-725, 755, -736, -723, -750], [-751, -739, -741, 733], [753, 749, -756, 725, -744, -735], [-750, -744, -749, 755], [736, 756, 722, -760, 755], [746, -723, -753, 752, -744, 730, -728], [-757, 721, 750], [-743, 741, -760, -751, 727, 745], [-760, -751], [737, 739, -746, -741], [-732, 760, 748], [746, -728], [-758, -726, -721], [739, 747, -748, 738, 728], [730, -725], [753, 746, -750, 738, 760, 759], [748, -730], [728, -727, 739, 723, -741], [-732, 742, -740], [750, 725, -723, 729, -746, -756, -724], [-759, 744, -723, -745, -748, 752, 732, 728], [-743, -729, 727], [-742, 721, 736, -726], [-723, 737, -725, 758, -730, 736], [-746, 741, -747, -758, -742], [758, 731, 722, 730, -733, -727, -737, 755], [749, -740, 733], [741, -727, -730, -755], [-739, -743, 737], [-733, 744, -725], [757, 723, -741, -743, -732], [-750, 745, 731], [722, 741, 731, 747, 754], [740, -748, 742, 746, -745], [-750, -759, -735, -758, 730, -737, 740, -723], [-750, 735, -721], [753, 727, -746, 724], [-745, 736, 731], [-751, 755, -747, 736, -759, 724, -753, 756, -742], [-745, 755, -751, 744, 732], [757, -722, -750], [-723, 760, -732], [-749, -731, -757, 743, -734], [-759, -740, 722], [743, 729, -739, -721, -730], [-743, 738, -721], [751, -727, 756, -753, 722], [-726, 721, 741, -735, -750], [-758, -724], [749, -752, -744], [727, 756, -751, 730, 746], [-747, -737, -734, -729], [-745, 732], [-732, 722, -759, 724], [-728, -740, 752], [754, 752], [749, -726, -735, 759, -733], [737, -732, 725], [-757, -755, 739, 731], [-747, -749, 723, 740, 732], [-737, 740, 736], [721, -757], [-798, -784, 771], [784, -786, 775], [-778, -774, -766, -794], [-794, 768, 799], [764, -777, -783], [-795, 776, -788, -761, -770, 771, 779, 796, -764, -783, -773], [761, 786, -783, -796, -776, 788], [784, -776], [-790, -779, -800, 775], [788, 764, 784], [763, 792, -762, -783, -785], [766, -762, 796, -781, 785, 799], [791, 774, -769, 762, 786, -799, 779, 771, 765], [-796, -766, -770, 776, 773], [-768, 799, -789, 763, -781, 764, -794], [777, 769, -763], [787, 772, 766], [762, -782, 784, 775], [780, 792, 800, -787, 776, -785, 786, -769, -767, 775, -794, 761, 781], [774, 772, -779, -767], [772, -779, -762, -785], [-781, -776], [-791, 787, -790, 766, 800], [797, 778, 787], [787, 784, -770], [-767, -765, 772], [-775, -788, -792, -799, 773], [799, 800, -765, -767, 772], [779, -766, 799], [778, -771, -783], [775, 794], [784, 790, 791, 777], [-792, 773, 782, 769, 784], [-765, 764, 767], [-784, 775, -800, -774, -763, -785], [796, 769, 789, -795, 779], [-797, -766, -793, 773, 783], [-791, -797, -766, -777, 779, -771], [789, 782, -770, 783, -799, -791, -775, 768], [768, -778, 800], [770, 774, 761, -767, -765], [-765, 781, 763, -775, 789, 771], [778, -799], [-800, -799, 788, 794, 792], [790, 769, 800, 788, -795], [775, 781], [-796, -777], [790, 787, -777], [-765, 761, 787, -783, 775], [-762, -785, -796, 791, -767], [-777, 781, -765, -764, -797, 792, -771, 790], [800, -762, -772], [776, 792, 791, 768], [-780, -777, -765, -778, -766, 789], [763, 799, -780], [765, -763, -785], [-786, 793, 799], [-791, 794, -767, 772], [-777, -791], [-771, -788, 790, -799], [776, -768, -762, 797], [-799, 772, 797, 778, -787, -800], [-782, 788, -767, 780], [767, -764, 797, -786], [762, 766, 768], [-774, 765, -761], [-798, 796, -799, -767, -763, -797, 795, 784, 777, -794, -769, 762, -793, 791], [779, -781, 763, -789], [-793, 786, -798, -779, 762, -773], [-798, 781], [774, -779, -799, -766], [778, -784, -799, -769, 780, -765], [-793, -785, 796], [788, -786, 797, -769], [-777, 799, -781, 778, -764, 773, -783], [782, -779, 796], [763, 786], [-792, 778, 769, -791], [795, -787, 781, -788], [797, 798, -785, 794, -783, 767, -768], [766, -761, -767], [775, 798, -788], [771, 762, 775, 778], [-791, -765, -770, 789, 788, 778], [-773, 792, 761], [-770, -787, -773, 782], [-789, -795], [790, -800, 793], [-773, -761], [-784, -774, 771], [787, -770, 793], [795, 798, 768, -761, 780, 764, 773], [765, -800], [-799, -781, -775, 783], [776, -771, 789, -774, -781, -799], [777, 774, -773, -775], [764, -771, -785], [-799, 762], [775, 797, 780, 768], [799, -778, -790, -800], [782, -790, 780], [-787, -799], [779, 777, 784], [-778, -770], [765, -794, 778], [795, -782, -799, -768], [-763, -773, -782, -789, 793], [765, 772, -763, -792, -791, 773], [775, 786, 794, 772, -789], [781, 765, 779, -797], [-798, -783, -764, -785], [798, 793, -779, 764, -772, -767, 780, -799], [-766, 798, -776, -769, -795, 767], [764, 778, 790], [763, 779, 794, -783], [798, -782], [-798, 770, 800, 785, -761, -788], [777, 773, 779, 783, 767], [-799, 761, -800, -775], [-776, 780, -789, 800, -781, -768], [779, -800, -784, 772, -771, -768], [-775, 791, 778, 768], [-775, -799, 767, -780, 768, 774], [-780, -796], [769, -797, 777, -794], [-780, -795, -789, 767], [792, 769, -793, 799], [776, 784, -792], [-782, 790], [-777, 798], [789, 773, 778], [789, 776, -793], [765, -781, -784], [765, 782, 799, -796], [-763, -766, -779, 769], [-797, -771, -763], [-786, -779, -794, 764, 768], [775, 794, 769, 785], [-768, -761, 800, 777], [-792, 768, -785], [779, 768, -792, -783, 793, 762, 797, -785, -790, 799], [793, 761, -778], [-779, -789, -762], [-798, -768], [783, 767, 761], [-786, -768, -777, -776], [-776, -787, 792], [778, 792, 794, 771, 787], [770, 765, -796], [-790, 783, 789, 799, -784, 794], [-763, -790, -788], [-768, 761, -767, -784], [-776, -788, -793], [779, -783, -766], [786, 794, 793, -791, -770, 766, 778, 792, -784, -762, -774], [-794, -764, -788], [771, -784, -799], [-793, -783], [-787, 800, -797], [762, 777, 765, 785, -792, 766], [765, 764, -762, -799], [774, 785, -789], [778, 763, 770, -777], [-795, 775, -789, -765], [-794, -791, 785, -776, 797, -767, 788], [-764, -780, -787], [777, 794, -799, 764], [-765, 795, -776, -763, -790], [-791, -779, -781, 773], [793, 789, -796, 765, -784, -775], [-790, -784, -789, 795], [776, 796, 762, -800, 795], [786, -763, -793, 792, -784, 770, -768], [-797, 761, 790], [-783, 781, -800, -791, 767, 785], [-800, -791], [777, 779, -786, -781], [-772, 800, 788], [786, -768], [-798, -766, -761], [779, 787, -788, 778, 768], [770, -765], [793, 786, -790, 778, 800, 799], [788, -770], [768, -767, 779, 763, -781], [-772, 782, -780], [790, 765, -763, 769, -786, -796, -764], [-799, 784, -763, -785, -788, 792, 772, 768], [-783, -769, 767], [-782, 761, 776, -766], [-763, 777, -765, 798, -770, 776], [-786, 781, -787, -798, -782], [798, 771, 762, 770, -773, -767, -777, 795], [789, -780, 773], [781, -767, -770, -795], [-779, -783, 777], [-773, 784, -765], [797, 763, -781, -783, -772], [-790, 785, 771], [762, 781, 771, 787, 794], [780, -788, 782, 786, -785], [-790, -799, -775, -798, 770, -777, 780, -763], [-790, 775, -761], [793, 767, -786, 764], [-785, 776, 771], [-791, 795, -787, 776, -799, 764, -793, 796, -782], [-785, 795, -791, 784, 772], [797, -762, -790], [-763, 800, -772], [-789, -771, -797, 783, -774], [-799, -780, 762], [783, 769, -779, -761, -770], [-783, 778, -761], [791, -767, 796, -793, 762], [-766, 761, 781, -775, -790], [-798, -764], [789, -792, -784], [767, 796, -791, 770, 786], [-787, -777, -774, -769], [-785, 772], [-772, 762, -799, 764], [-768, -780, 792], [794, 792], [789, -766, -775, 799, -773], [777, -772, 765], [-797, -795, 779, 771], [-787, -789, 763, 780, 772], [-777, 780, 776], [-761, -797], [827, 814, 826], [828, 801, 805], [810, 828, 830], [-838, 802], [817, 829, -810, -813, 824, 804, 834], [812, 838], [808, -804, 836], [832, -837, 802], [834, -824, 808], [810, -813, -839], [836, -829, -804], [-834, -812, 804], [804, -808, 810, -801], [-819, 825, -832, -833], [816, 822, -831], [814, 830, -825], [-833, -807, 838, -820], [819, 839, 806, 829], [-835, -808, -829, -816], [-802, 809, 830], [-811, 818, 810, -821], [802, -824, -834, -831], [-830, 837], [805, -809, 803], [839, 836, -809], [-806, 819, -829, -821, -823, -817], [832, 826, 823], [828, 817, 830, -825], [-820, 814], [810, 832, 840, 814], [840, -812, 831], [-814, 839, 826, 830], [-825, -837, 818, -802, 838], [813, 804, -837, -838], [837, -831, 827], [840, 825, -816, -833, 836], [-823, -820], [821, 833, 815, -808, 802], [-817, -838, 813], [-828, 824, 816], [832, 826, 814], [-807, 810, 828], [-828, -817], [831, 821, -830], [819, -833, -835, -802], [805, -833, -838], [820, 818, 830, 810, -821], [-840, 814, 809, 824, -838], [828, 840, 833, -806, 822], [-824, 806, -835, 826, 830], [-828, 805, 838], [-838, 825, -824, 835, -806], [-822, -834, 824, -835], [830, 840, -834, -805, 838], [-827, -836, 804], [-835, 831, 829, 824], [805, 818], [-815, 828, 814, -811, 802], [839, 810, 840], [816, -825, 828, 831], [827, 835], [-818, 805], [-808, 822, 802, 839, 837, -826, -833, 829, 820], [809, 803, 822, -840, 802], [-801, 817, 804, -840], [-802, 818, -815, -838], [831, 811, 803, -806, -829, -805, -807], [-822, 828], [832, 802, -814, -801, 810, 805, 833, -836, -806], [831, -820, -812, 827], [-805, 832, 804], [-815, -829, 809, -818], [-824, 820, 829, -833, -818], [-810, 830, 808, -829, 806, -813, -814, -803, 836, 818], [816, -811, -832], [809, -806, -814, 817], [-812, 810], [-809, 826], [820, -830, 811], [-801, -834, 838], [-839, -835, -816, -827, -823], [-837, 802, 821, -826, -810], [805, -829, -802, -833, 824, 839, -818, 810], [-830, 813, -835, -823, -838], [808, -827, -826], [812, -838, 816, 840, 822, -801], [804, -819, -834, -823], [823, -807, 832, 839], [-822, -833, 828], [-830, -818, 821], [-815, 828, -818], [-840, 803, 816, -839, -828, -814], [810, 813, 819], [810, 822, 812], [-838, 815, -801], [-827, -829, 817], [-805, -802, -811], [-804, 832, -808], [-810, -808], [807, 821, 803, 840, -838], [831, 824, 806], [-839, 817, 801], [814, 838], [839, 834, -831], [828, -826, 836, -829], [-833, -834, 830], [-813, -826, -820, -823, -832, 802], [833, -815, -829, -814], [826, -807], [-819, -801, -818], [823, 826, 821, 802], [-807, 822, 811, -828, -834, -832, -838], [-826, -805, -839], [831, 802, 804], [830, 827, -825, 819, 807], [837, 833, -803, -807], [-819, 833, -801], [825, -822, 840], [818, 809, -840, 835], [824, -832, 834, 829], [829, 801, 832, -809], [-824, -826, 810, -811, -805], [829, 812, -813], [833, 839, 817], [815, 824, -835, -831, 813, -808], [-823, 807, 831], [818, 828, -817, -825], [-838, -811, -829], [-819, -817, 829, -801, 816, 839, 837, 831], [-806, -831, 818], [803, 835, -806, 825, 840, -817, -804, 839], [-804, -820, -833, 840, -828, -839, 814, 824], [-812, -839, 814, -830, -818, 824, 827, 831], [-817, -823], [-835, -824, -820], [-821, 820, 830, 810, 818, 805, -804], [834, 811, 820, -821, 832, -818, -817, -825, 835], [-828, 823, -836, -825], [818, -810, -803], [-814, -825, 816], [-805, 823, 818], [-825, -804, -820, -811, 839], [822, 801, 834], [-829, 827, 830, -804], [830, -839, 822, -826, -807], [812, -825], [-834, -807, -811, 806, 829, 812], [-806, 837, -830], [-820, -840], [834, -829, -804, 838, -801], [-825, -812, -817, -835], [-820, -806], [-833, 813], [-830, 834, 814], [814, 822], [815, -806, -808], [808, -816, 829], [837, 822, -816], [807, -831, -821, 826, -830, -820, 838, 806], [-806, 814], [801, -818, 827], [840, -838, 817], [809, -810, 814], [-805, 836, -828], [-801, 835, 832], [-807, 823, 824], [-836, 808, 822], [-836, -821, -814], [-812, -821, -811], [801, 809, -806, -802, -810], [832, 819, -820, -801, 810, 803], [-817, 823, 819], [-808, 812, 816], [-827, 812, 819], [834, 831, -811, 824, -826, -818], [819, 823, 838], [831, 829, 803, -801, 838], [-811, -802, -821], [835, 825, 809, 805, -820, -838, 822], [-804, 826, 829, 820, -805, 816], [812, -826, 802, -829, -815], [-823, -830, 812, 810], [840, 836, 835], [831, -827, -839, -836], [-840, 819, -811, -837, 802, 821, -826], [826, 831, 806, -840, 805, -839, 827, 837], [816, -820, 839, 829, 808, -840, 814], [818, 840, 819, 816, -829, -807, 817, 804, -828], [-805, -828, 839], [834, 817, 812], [823, -835, 827, -839, -829], [823, -816, 839, -813, 828], [-827, -840, -805, 806, 804, 830, 809, 836], [-820, -811, 825, 818], [814, 817, 807, -810, -835], [-837, 808, -806, 821, -820], [-822, -833, -821], [807, -827, -840, 823], [815, -819], [-801, 811], [-803, 826, 817], [814, -828, 829, -825, 812, -822, -824], [-811, -830, 825, -802, -821, -818, 832, 809], [-801, 818, 821], [-829, 801, 827], [-837, 835, -820, -823], [837, -834, 833, 825], [-819, -829, 840], [867, 854, 866], [868, 841, 845], [850, 868, 870], [-878, 842], [857, 869, -850, -853, 864, 844, 874], [852, 878], [848, -844, 876], [872, -877, 842], [874, -864, 848], [850, -853, -879], [876, -869, -844], [-874, -852, 844], [844, -848, 850, -841], [-859, 865, -872, -873], [856, 862, -871], [854, 870, -865], [-873, -847, 878, -860], [859, 879, 846, 869], [-875, -848, -869, -856], [-842, 849, 870], [-851, 858, 850, -861], [842, -864, -874, -871], [-870, 877], [845, -849, 843], [879, 876, -849], [-846, 859, -869, -861, -863, -857], [872, 866, 863], [868, 857, 870, -865], [-860, 854], [850, 872, 880, 854], [880, -852, 871], [-854, 879, 866, 870], [-865, -877, 858, -842, 878], [853, 844, -877, -878], [877, -871, 867], [880, 865, -856, -873, 876], [-863, -860], [861, 873, 855, -848, 842], [-857, -878, 853], [-868, 864, 856], [872, 866, 854], [-847, 850, 868], [-868, -857], [871, 861, -870], [859, -873, -875, -842], [845, -873, -878], [860, 858, 870, 850, -861], [-880, 854, 849, 864, -878], [868, 880, 873, -846, 862], [-864, 846, -875, 866, 870], [-868, 845, 878], [-878, 865, -864, 875, -846], [-862, -874, 864, -875], [870, 880, -874, -845, 878], [-867, -876, 844], [-875, 871, 869, 864], [845, 858], [-855, 868, 854, -851, 842], [879, 850, 880], [856, -865, 868, 871], [867, 875], [-858, 845], [-848, 862, 842, 879, 877, -866, -873, 869, 860], [849, 843, 862, -880, 842], [-841, 857, 844, -880], [-842, 858, -855, -878], [871, 851, 843, -846, -869, -845, -847], [-862, 868], [872, 842, -854, -841, 850, 845, 873, -876, -846], [871, -860, -852, 867], [-845, 872, 844], [-855, -869, 849, -858], [-864, 860, 869, -873, -858], [-850, 870, 848, -869, 846, -853, -854, -843, 876, 858], [856, -851, -872], [849, -846, -854, 857], [-852, 850], [-849, 866], [860, -870, 851], [-841, -874, 878], [-879, -875, -856, -867, -863], [-877, 842, 861, -866, -850], [845, -869, -842, -873, 864, 879, -858, 850], [-870, 853, -875, -863, -878], [848, -867, -866], [852, -878, 856, 880, 862, -841], [844, -859, -874, -863], [863, -847, 872, 879], [-862, -873, 868], [-870, -858, 861], [-855, 868, -858], [-880, 843, 856, -879, -868, -854], [850, 853, 859], [850, 862, 852], [-878, 855, -841], [-867, -869, 857], [-845, -842, -851], [-844, 872, -848], [-850, -848], [847, 861, 843, 880, -878], [871, 864, 846], [-879, 857, 841], [854, 878], [879, 874, -871], [868, -866, 876, -869], [-873, -874, 870], [-853, -866, -860, -863, -872, 842], [873, -855, -869, -854], [866, -847], [-859, -841, -858], [863, 866, 861, 842], [-847, 862, 851, -868, -874, -872, -878], [-866, -845, -879], [871, 842, 844], [870, 867, -865, 859, 847], [877, 873, -843, -847], [-859, 873, -841], [865, -862, 880], [858, 849, -880, 875], [864, -872, 874, 869], [869, 841, 872, -849], [-864, -866, 850, -851, -845], [869, 852, -853], [873, 879, 857], [855, 864, -875, -871, 853, -848], [-863, 847, 871], [858, 868, -857, -865], [-878, -851, -869], [-859, -857, 869, -841, 856, 879, 877, 871], [-846, -871, 858], [843, 875, -846, 865, 880, -857, -844, 879], [-844, -860, -873, 880, -868, -879, 854, 864], [-852, -879, 854, -870, -858, 864, 867, 871], [-857, -863], [-875, -864, -860], [-861, 860, 870, 850, 858, 845, -844], [874, 851, 860, -861, 872, -858, -857, -865, 875], [-868, 863, -876, -865], [858, -850, -843], [-854, -865, 856], [-845, 863, 858], [-865, -844, -860, -851, 879], [862, 841, 874], [-869, 867, 870, -844], [870, -879, 862, -866, -847], [852, -865], [-874, -847, -851, 846, 869, 852], [-846, 877, -870], [-860, -880], [874, -869, -844, 878, -841], [-865, -852, -857, -875], [-860, -846], [-873, 853], [-870, 874, 854], [854, 862], [855, -846, -848], [848, -856, 869], [877, 862, -856], [847, -871, -861, 866, -870, -860, 878, 846], [-846, 854], [841, -858, 867], [880, -878, 857], [849, -850, 854], [-845, 876, -868], [-841, 875, 872], [-847, 863, 864], [-876, 848, 862], [-876, -861, -854], [-852, -861, -851], [841, 849, -846, -842, -850], [872, 859, -860, -841, 850, 843], [-857, 863, 859], [-848, 852, 856], [-867, 852, 859], [874, 871, -851, 864, -866, -858], [859, 863, 878], [871, 869, 843, -841, 878], [-851, -842, -861], [875, 865, 849, 845, -860, -878, 862], [-844, 866, 869, 860, -845, 856], [852, -866, 842, -869, -855], [-863, -870, 852, 850], [880, 876, 875], [871, -867, -879, -876], [-880, 859, -851, -877, 842, 861, -866], [866, 871, 846, -880, 845, -879, 867, 877], [856, -860, 879, 869, 848, -880, 854], [858, 880, 859, 856, -869, -847, 857, 844, -868], [-845, -868, 879], [874, 857, 852], [863, -875, 867, -879, -869], [863, -856, 879, -853, 868], [-867, -880, -845, 846, 844, 870, 849, 876], [-860, -851, 865, 858], [854, 857, 847, -850, -875], [-877, 848, -846, 861, -860], [-862, -873, -861], [847, -867, -880, 863], [855, -859], [-841, 851], [-843, 866, 857], [854, -868, 869, -865, 852, -862, -864], [-851, -870, 865, -842, -861, -858, 872, 849], [-841, 858, 861], [-869, 841, 867], [-877, 875, -860, -863], [877, -874, 873, 865], [859, -869, 880], [-910, 920, -899, 909], [-915, -894, 912], [-883, -917, -899], [-894, -915, 916, -900, 903], [905, -912, -910], [920, 891, 886], [912, 894, -913], [-893, -885, 883, 888], [-895, 900, 894, 884, 883, -918], [898, 919], [892, 902, -885, 899], [-912, -898, -883], [-884, 900, 904, -898, -882, 885, 888, 890, -881], [-915, 898, 886, -914, -883, -888], [-890, -911, 894, -883, 896, 914], [910, 895, 900, 897], [-883, -891, 907, 894, -908, -881, -919], [-900, 918, 901, 911], [915, -884, 881], [-884, 895, -915], [-892, -915, -887, 891, -898, -908], [-916, 889, -882, -898, 915, -901, 895], [-903, -888, -894, 892, -904], [-908, -895, 910], [-919, -884, -910, 911, -907, 909], [-893, -884], [886, -887, 916], [-898, -913, -909, 917, 918], [-898, -913, 904], [-898, 911, -917, -884, -886], [-890, -910, -899, -891, -894, -892, -897, 903], [917, -900, -881, -883, 913, -893, -884, 889], [885, -883, -881, -882, 902], [903, 897, -883], [-896, -910, -882, 891], [-915, 920, 900, 881], [-913, -902, -911, 894, 893, 917], [882, -913], [-899, 893, 906, 901], [909, -911, 888, -919, -912], [899, 910, -890], [-886, -918, -919, -883], [-883, 919, 905], [-890, -911, -915, 897, -905, 913, -889, -914, -902], [-901, 910, 892], [-896, 883, 915, -902], [-892, 902, 899, 883, 888, -884, 898, 904], [906, 917, 897, 884, -916, 918], [917, 907, -889, 905, -910, 882, -890], [911, -909, -918, 910, 906, -904], [-888, 911, -881], [-914, -897, -905], [907, -903, -908, 916], [919, 920, -896, 882], [915, -905, -896], [915, -914, 892, 911, -903, -904, -905, -900, -918, 888, -883, -908, 902], [885, 913, -889, -908], [898, 904, -906], [-893, -886, 910, -913, 907, 914, -909, -896, 897, -882, 902, -912, -894, -918], [917, 893], [-885, -894], [882, -898], [-883, -882, -907, 901, -898, 902, 900], [-915, -889, -913], [885, -916, -904], [-914, 908, 899], [901, 893, 920, -912, 908], [885, -912, -897], [-885, 894, -897, 906, 893], [892, -888, -908], [-897, -889], [-910, -919], [911, -907, -905, 904, -896, 909, -895], [901, -913, 909], [891, -911, -916, 897], [-911, -907, -898, 903, 900, 913], [-919, 910, 905, -902, 886], [-892, -907, -889], [888, -883, 881], [899, -888], [896, -918, -893], [-917, -890, 902], [915, -909], [-911, 895, -909], [918, -882, -908], [911, 882, -894, 913], [-904, -915, -901], [906, -891], [918, 881, 893, 902, 900], [-895, 915, -894, -899, -890], [-887, 913, 885, 890, 892], [-883, -909, 897], [915, -906, 884, -898, -882, 899, 910, -885, -907], [-889, 901, -913, -888, 881], [-889, 885, -910], [-911, 888, 898, 891, -916], [882, 906, -895, -913], [-904, -893, 911, 903], [-886, -896, 907, 909], [898, -893, 919], [919, -920, -910, -913], [-884, -916, -900, -882, 913], [915, 897, 896, -900, -891, 886], [891, -902, -901, 881, 895, 910, -915], [899, 883], [-920, -892, 885, 883], [907, -897, -912], [-881, -913], [-904, 890, 885, -915, 884], [918, -896, 913, -909], [897, -881], [916, 909, -915], [-889, 916, -897, 918], [-885, 912, -889, 900], [-891, 915, -920], [-917, 906], [-898, 894, 919, 906], [899, -910, 897], [-898, 918, -914], [884, 914, 886, -907], [909, 893], [-909, -885, 907, 882, -889, -908], [-910, -907, -912, -913, 917], [906, 898, 904, -914, 910, 917, -916], [912, 881, 883], [-910, 883], [-912, 902, 901, 898], [885, -907, -883, 920, -901, -917, -894], [-895, 900], [887, -897, 886, -908, 912, 892, -896], [-892, 895, -888], [903, 899], [-917, 883, -896], [-894, -919, -892], [-900, -890, -903, 893, -896], [-912, 885, -895, 892, 889, 881, 916], [892, -919, 911], [897, 905, 895, 890], [906, -897, 900, 914, -908], [-892, 910, 914], [-885, 909, -886, -917, 907, -910], [917, 900], [909, 893, -907], [903, 914], [885, 892, 910], [899, -881], [-904, -881, 909], [884, -903, -905, -895], [887, 889, -909, -893], [916, -901, -917], [906, 896, 903, 892], [-917, -910, -896, 903, -881], [887, 917, 905, -884], [912, 884, -893, -920, 903], [-920, -918, 888, -916, -902], [-919, 912, 904, -887], [-898, -907, -914, -883, -895, 918], [-918, 883], [-893, -885, -903, 898, -891, -902, -887, 912, -899, -900], [884, 920, 915], [906, -885, 897, 889, 884, 904], [881, 905, 903, 912, -917], [893, -888, 916, 896, -914, 908], [897, 882, -896, -900], [-889, 891, -911, -886], [-896, 888, 881, 885, 911], [-908, 906, -896], [912, -910, 882, 903], [-902, -907, 901, -895, 882], [883, 892, -900], [890, -894, -886], [914, 889, 897, -913], [-909, 914, 908], [903, -883, -892, -890], [900, 915, 910], [-895, 891], [910, -884, -918, 911, 885, 909, 883], [-914, -906, -887], [-896, 904, -915], [-881, 904, 910, 915, 907, -883], [902, -883, 889], [-894, 902, -917, -912, -916], [881, -897, -885, -903, 896, -886, 919], [-893, -909, 906, -881], [910, 906, 914, -883, 892], [-912, -919], [-899, 886, 895], [-889, 883, 893, 915, 907, -898, 886], [-881, 903, -891], [-915, 917, -905], [899, 892, -909], [914, 890, 909, -888, -904, -882, -889], [915, 907], [891, -900], [888, -887, -899, 886, -907, -903, -902, -914, -912, -919, -884, 904, -911], [-902, -890, -905, -917, 901], [-888, -891, 902, 909], [885, 901, 887, 892, 897, -902, -883], [888, 896, -883], [-899, -917, 894, 909, -901], [919, -915], [888, -889, -886], [-891, 918], [-911, -889, -903, -898, -886], [918, 908, -881, -886], [913, -900, -897], [-893, 912, 882], [-950, 960, -939, 949], [-955, -934, 952], [-923, -957, -939], [-934, -955, 956, -940, 943], [945, -952, -950], [960, 931, 926], [952, 934, -953], [-933, -925, 923, 928], [-935, 940, 934, 924, 923, -958], [938, 959], [932, 942, -925, 939], [-952, -938, -923], [-924, 940, 944, -938, -922, 925, 928, 930, -921], [-955, 938, 926, -954, -923, -928], [-930, -951, 934, -923, 936, 954], [950, 935, 940, 937], [-923, -931, 947, 934, -948, -921, -959], [-940, 958, 941, 951], [955, -924, 921], [-924, 935, -955], [-932, -955, -927, 931, -938, -948], [-956, 929, -922, -938, 955, -941, 935], [-943, -928, -934, 932, -944], [-948, -935, 950], [-959, -924, -950, 951, -947, 949], [-933, -924], [926, -927, 956], [-938, -953, -949, 957, 958], [-938, -953, 944], [-938, 951, -957, -924, -926], [-930, -950, -939, -931, -934, -932, -937, 943], [957, -940, -921, -923, 953, -933, -924, 929], [925, -923, -921, -922, 942], [943, 937, -923], [-936, -950, -922, 931], [-955, 960, 940, 921], [-953, -942, -951, 934, 933, 957], [922, -953], [-939, 933, 946, 941], [949, -951, 928, -959, -952], [939, 950, -930], [-926, -958, -959, -923], [-923, 959, 945], [-930, -951, -955, 937, -945, 953, -929, -954, -942], [-941, 950, 932], [-936, 923, 955, -942], [-932, 942, 939, 923, 928, -924, 938, 944], [946, 957, 937, 924, -956, 958], [957, 947, -929, 945, -950, 922, -930], [951, -949, -958, 950, 946, -944], [-928, 951, -921], [-954, -937, -945], [947, -943, -948, 956], [959, 960, -936, 922], [955, -945, -936], [955, -954, 932, 951, -943, -944, -945, -940, -958, 928, -923, -948, 942], [925, 953, -929, -948], [938, 944, -946], [-933, -926, 950, -953, 947, 954, -949, -936, 937, -922, 942, -952, -934, -958], [957, 933], [-925, -934], [922, -938], [-923, -922, -947, 941, -938, 942, 940], [-955, -929, -953], [925, -956, -944], [-954, 948, 939], [941, 933, 960, -952, 948], [925, -952, -937], [-925, 934, -937, 946, 933], [932, -928, -948], [-937, -929], [-950, -959], [951, -947, -945, 944, -936, 949, -935], [941, -953, 949], [931, -951, -956, 937], [-951, -947, -938, 943, 940, 953], [-959, 950, 945, -942, 926], [-932, -947, -929], [928, -923, 921], [939, -928], [936, -958, -933], [-957, -930, 942], [955, -949], [-951, 935, -949], [958, -922, -948], [951, 922, -934, 953], [-944, -955, -941], [946, -931], [958, 921, 933, 942, 940], [-935, 955, -934, -939, -930], [-927, 953, 925, 930, 932], [-923, -949, 937], [955, -946, 924, -938, -922, 939, 950, -925, -947], [-929, 941, -953, -928, 921], [-929, 925, -950], [-951, 928, 938, 931, -956], [922, 946, -935, -953], [-944, -933, 951, 943], [-926, -936, 947, 949], [938, -933, 959], [959, -960, -950, -953], [-924, -956, -940, -922, 953], [955, 937, 936, -940, -931, 926], [931, -942, -941, 921, 935, 950, -955], [939, 923], [-960, -932, 925, 923], [947, -937, -952], [-921, -953], [-944, 930, 925, -955, 924], [958, -936, 953, -949], [937, -921], [956, 949, -955], [-929, 956, -937, 958], [-925, 952, -929, 940], [-931, 955, -960], [-957, 946], [-938, 934, 959, 946], [939, -950, 937], [-938, 958, -954], [924, 954, 926, -947], [949, 933], [-949, -925, 947, 922, -929, -948], [-950, -947, -952, -953, 957], [946, 938, 944, -954, 950, 957, -956], [952, 921, 923], [-950, 923], [-952, 942, 941, 938], [925, -947, -923, 960, -941, -957, -934], [-935, 940], [927, -937, 926, -948, 952, 932, -936], [-932, 935, -928], [943, 939], [-957, 923, -936], [-934, -959, -932], [-940, -930, -943, 933, -936], [-952, 925, -935, 932, 929, 921, 956], [932, -959, 951], [937, 945, 935, 930], [946, -937, 940, 954, -948], [-932, 950, 954], [-925, 949, -926, -957, 947, -950], [957, 940], [949, 933, -947], [943, 954], [925, 932, 950], [939, -921], [-944, -921, 949], [924, -943, -945, -935], [927, 929, -949, -933], [956, -941, -957], [946, 936, 943, 932], [-957, -950, -936, 943, -921], [927, 957, 945, -924], [952, 924, -933, -960, 943], [-960, -958, 928, -956, -942], [-959, 952, 944, -927], [-938, -947, -954, -923, -935, 958], [-958, 923], [-933, -925, -943, 938, -931, -942, -927, 952, -939, -940], [924, 960, 955], [946, -925, 937, 929, 924, 944], [921, 945, 943, 952, -957], [933, -928, 956, 936, -954, 948], [937, 922, -936, -940], [-929, 931, -951, -926], [-936, 928, 921, 925, 951], [-948, 946, -936], [952, -950, 922, 943], [-942, -947, 941, -935, 922], [923, 932, -940], [930, -934, -926], [954, 929, 937, -953], [-949, 954, 948], [943, -923, -932, -930], [940, 955, 950], [-935, 931], [950, -924, -958, 951, 925, 949, 923], [-954, -946, -927], [-936, 944, -955], [-921, 944, 950, 955, 947, -923], [942, -923, 929], [-934, 942, -957, -952, -956], [921, -937, -925, -943, 936, -926, 959], [-933, -949, 946, -921], [950, 946, 954, -923, 932], [-952, -959], [-939, 926, 935], [-929, 923, 933, 955, 947, -938, 926], [-921, 943, -931], [-955, 957, -945], [939, 932, -949], [954, 930, 949, -928, -944, -922, -929], [955, 947], [931, -940], [928, -927, -939, 926, -947, -943, -942, -954, -952, -959, -924, 944, -951], [-942, -930, -945, -957, 941], [-928, -931, 942, 949], [925, 941, 927, 932, 937, -942, -923], [928, 936, -923], [-939, -957, 934, 949, -941], [959, -955], [928, -929, -926], [-931, 958], [-951, -929, -943, -938, -926], [958, 948, -921, -926], [953, -940, -937], [933, 952, 922], [-989, 979, 980, -964], [1000, 965], [-991, -964, -966, -993, -995, 969, 998, 999, -962], [-973, -989], [1000, 964, 976, 992], [964, -989, -995, 993, -981, 978], [-967, 996, -992, 975], [-989, -996, 991], [997, -988], [970, -986, 1000], [-998, -969, 965, -1000], [982, 989, -994, -980], [985, 992, 995, -977], [-984, 997, 992], [993, -1000], [997, 973, -964], [1000, 988], [-981, -974, 984, 991, -986, 961, -983, -969, -988, -967, -970, 975], [992, 998], [975, 986, -985, -999], [964, -965, 998, 989, 982], [-986, 971, -962], [993, 984], [984, 962, -998, -989, 961], [995, -988, -987, -983, -971], [-994, 968, 972, 976], [981, 986, 988, -991, -984], [-987, -1000, 976, 966], [-964, 966, -988, -974, 971, -978], [986, -998, -962, -992, 980], [962, -963, 973, 990, 995], [-991, 978, -974], [980, 995, -989], [966, -977, 968, -984, -970, 974, -969, 992, -962], [969, -968], [997, 993], [996, -992, 989, -963, -968, 962, 993], [-986, -965, -963, -975], [968, 978, 993, -973, -999], [-997, 961, 975, -999], [-980, -964, -987], [981, -966, 969], [989, 999, 998, -997, 966], [990, 971], [986, 979, -977], [-975, 981, 963, 971, 1000, -966, 992, 983, 994, -980, 970, 986, -976], [-996, -965, 988, 966, 985], [-975, -990, 986], [999, -972, -969, -977], [962, -977, 989], [-975, 962, 980, -995], [977, 983, 996], [-992, -979, -983, 981, -976, -999, 964], [-970, -972], [992, 968, -995, -983], [970, 967, 976, -971], [981, 980, -970], [-978, 999, -968, 989, 991, 983], [979, -992, 988], [-978, -975, -991, -968, 989, 967, 999], [961, 998, -997], [975, -965, 974], [-984, -967, 998, -1000, -988, -997, 995, -969, 975, -977], [967, 988, 976, 973], [989, -977, -986], [-973, -989, -980, 991], [-962, 969, 1000, 994, 967, 979, -980], [-970, -998], [966, -985, -997, -971], [-961, 985, -994, 1000, 962, 973, -995], [-967, -990, -964, -981, -986, -994, 970, -968], [996, -970, 965, 981, 964, -990, -963, 975], [-963, -985, 993, 988, 974, 971, -965, -968, -983], [-977, 974, 970], [-963, 964, -970, 974, 987, -997, -976], [-979, -981, -985, 996, 976, 977], [-975, 989, -994, -990, -972, 970, -976, 995, 991], [-967, 973, -995], [983, 962, -961, 970], [-962, 966], [-968, -977, -993, -976, 979], [-976, 962, 968, 990, -989], [-997, 963, 979, 980, 971], [975, 966, -974], [-967, 963, 964, -986], [979, -983, 971, -976], [983, 998, 986], [993, 963], [-999, 970, 968], [-965, -966, -980], [972, -984], [990, -987, -973], [-969, -993, 998], [968, -985, -966], [-969, -983, 990, -999], [-978, 974, -969, -991, -994, 997, 982], [963, 975, 971, 961], [964, -977, -999], [961, 966, 978], [961, 997, -978, 992, 987], [-962, -984, -990], [-998, -966, -984], [-1000, -973], [-990, 979, 991, 986, -968, 996, -965, 984, 974], [-987, -993, -989, 978, -972], [-998, -979, -989], [-996, -971, -994], [991, -983, 966, -965, -974], [969, -963, 978], [998, 990], [973, -971, 1000], [991, 961, -971], [-992, -978], [978, 979, -996], [-973, -964, -968], [988, -999, 993, -987, -963], [981, -977, -973, 987, 985, 997], [992, 979, 990], [999, -962, 995, 996, 980, 973, 970, 979, -966, -989, -982], [-976, -975, 973], [-986, -967, -969, -975], [990, 966, 964, -981, 968], [-967, 981, -991], [988, 976, 964, -963, 974, -962, 983, -985], [-1000, 964, 991, 975], [-962, -969, 980, -978, -974], [980, 998, -997, 995, 971, -994], [990, 969, -995], [979, -989, -983, -986, 981, 992, 973], [-969, 986], [-969, -991, -980, 968, -986, -977, 963, -966, -983, -967, -988], [-982, 972, -987, -970], [995, 982, -975, -986, 983, 981, -973, 969, -968], [972, 989, 998, 994], [975, -967, 995, -969], [-985, -981], [-988, -964, 979, -984, 995, 990], [968, -999, -990, 986], [-984, 971, 978, -974, -985, 982, 973, -997, -968, 993], [973, -974, -967], [978, 980, 995, 994, -999, -983], [-984, 971, 987, -985, -988, -972], [996, -1000, -970, 968, 990, 981, 985], [983, 967, -972], [981, -987, -997], [-969, 974, 998], [-985, -986, 970, 993, -973], [996, 970, -978, -984, 999], [995, 971], [982, 981, -963], [975, 991, -963, 982], [-971, -996, 976], [-979, 961, 965], [-997, 999, -969, 962], [989, -976, 997, 992], [-993, -963, -968, 989, -996], [982, 966], [962, 979, 965], [-982, 992], [992, 981, 964, 968], [-969, -985], [967, -986, -974, -975], [983, 979, 969, -999, 998, 972], [969, 992, 981], [-977, 991, 1000, -996], [966, 981, -973, -994], [-997, 991, 989, -976], [-978, 979, 973, 970, -993, -989, -997], [997, -961, 992], [-988, -973, 996, -983, -993, 994, 982], [-979, -967], [-971, -984, -981, 977], [995, 977, -976, 998], [-972, -979, -962], [-973, -983, 982, 976, 996], [972, 961, 986, 970, 1000], [984, -972, -968, 983], [-971, 961, -993], [997, -967, 974, -971, -982, 986, 972], [-977, -978, 991, 1000, 992, 965], [-980, 998, -995, 982, -974], [965, 963, 962, 991, -995, -988, -1000, -971, 969], [981, 983, 999, 967, 988], [976, 981], [-988, 970], [980, -972, -996], [977, 974, -978], [-968, -971], [-963, -966, 978, 987], [-999, 974, 990], [997, 985, 962, 977], [-965, 997], [-994, -979, 978], [986, -977], [-961, -963, -986, 997, -996, -982, -975, -969], [-984, 997], [-970, -979, 966], [-965, 985, 984, -990, 962], [976, -973], [980, -994, 1000], [-969, -996, -983, 970], [-999, 981, -980, 979, -995], [962, 989], [969, -965, 990], [994, 986, -970, 982, -965], [994, 969, 997, -993], [987, 969, 991], [-994, 968, -964], [976, 988, 970], [967, -964, 989, 969, 981], [985, -962, 984], [969, -977, -966], [-980, -961, -974], [-976, 978, -984, -987, -977, -964, -967], [975, 993, 996], [967, -1000, 989, 966, 977, -981], [-972, -998, -971, -981, 962, 997], [-976, 985, 967], [-981, -987], [-990, 980, -969], [962, -984, 969, 986], [995, -972, -968, 993], [-967, 992, 983, 988, -980, -1000, 982, 990], [989, -990, 991], [967, -980, -978], [-991, -977, 964], [972, -977, 987], [961, 976, 967, -1000, 992, 984, -977], [-985, -974, 992, -962], [-977, 965, 976, 983], [994, 961, -973], [986, -988, 978, -999, -981, -971], [985, -991, 992, 971], [967, 990, -979], [971, -993, -996, 961], [986, 980], [982, 979, -1000], [-995, 978, 985], [961, -982, 992, -979], [974, 987, -996], [973, -966, -990, 999, -972], [971, -968, 963, -976, 978], [-968, 984], [974, -964, 998, 973], [-977, -979, 981, 965, 1000, -986], [989, 987], [999, 991, -970, 979], [-976, 962, 999], [-987, 966, -977, -992, -984, -976, -995], [994, 992, -984], [-995, -997, 968, 980], [-962, 967, 969], [976, 986, 968, 996, 999, -995], [-996, -990, -992, 989], [999, 972, 984, 963], [-962, 969], [-1029, 1019, 1020, -1004], [1040, 1005], [-1031, -1004, -1006, -1033, -1035, 1009, 1038, 1039, -1002], [-1013, -1029], [1040, 1004, 1016, 1032], [1004, -1029, -1035, 1033, -1021, 1018], [-1007, 1036, -1032, 1015], [-1029, -1036, 1031], [1037, -1028], [1010, -1026, 1040], [-1038, -1009, 1005, -1040], [1022, 1029, -1034, -1020], [1025, 1032, 1035, -1017], [-1024, 1037, 1032], [1033, -1040], [1037, 1013, -1004], [1040, 1028], [-1021, -1014, 1024, 1031, -1026, 1001, -1023, -1009, -1028, -1007, -1010, 1015], [1032, 1038], [1015, 1026, -1025, -1039], [1004, -1005, 1038, 1029, 1022], [-1026, 1011, -1002], [1033, 1024], [1024, 1002, -1038, -1029, 1001], [1035, -1028, -1027, -1023, -1011], [-1034, 1008, 1012, 1016], [1021, 1026, 1028, -1031, -1024], [-1027, -1040, 1016, 1006], [-1004, 1006, -1028, -1014, 1011, -1018], [1026, -1038, -1002, -1032, 1020], [1002, -1003, 1013, 1030, 1035], [-1031, 1018, -1014], [1020, 1035, -1029], [1006, -1017, 1008, -1024, -1010, 1014, -1009, 1032, -1002], [1009, -1008], [1037, 1033], [1036, -1032, 1029, -1003, -1008, 1002, 1033], [-1026, -1005, -1003, -1015], [1008, 1018, 1033, -1013, -1039], [-1037, 1001, 1015, -1039], [-1020, -1004, -1027], [1021, -1006, 1009], [1029, 1039, 1038, -1037, 1006], [1030, 1011], [1026, 1019, -1017], [-1015, 1021, 1003, 1011, 1040, -1006, 1032, 1023, 1034, -1020, 1010, 1026, -1016], [-1036, -1005, 1028, 1006, 1025], [-1015, -1030, 1026], [1039, -1012, -1009, -1017], [1002, -1017, 1029], [-1015, 1002, 1020, -1035], [1017, 1023, 1036], [-1032, -1019, -1023, 1021, -1016, -1039, 1004], [-1010, -1012], [1032, 1008, -1035, -1023], [1010, 1007, 1016, -1011], [1021, 1020, -1010], [-1018, 1039, -1008, 1029, 1031, 1023], [1019, -1032, 1028], [-1018, -1015, -1031, -1008, 1029, 1007, 1039], [1001, 1038, -1037], [1015, -1005, 1014], [-1024, -1007, 1038, -1040, -1028, -1037, 1035, -1009, 1015, -1017], [1007, 1028, 1016, 1013], [1029, -1017, -1026], [-1013, -1029, -1020, 1031], [-1002, 1009, 1040, 1034, 1007, 1019, -1020], [-1010, -1038], [1006, -1025, -1037, -1011], [-1001, 1025, -1034, 1040, 1002, 1013, -1035], [-1007, -1030, -1004, -1021, -1026, -1034, 1010, -1008], [1036, -1010, 1005, 1021, 1004, -1030, -1003, 1015], [-1003, -1025, 1033, 1028, 1014, 1011, -1005, -1008, -1023], [-1017, 1014, 1010], [-1003, 1004, -1010, 1014, 1027, -1037, -1016], [-1019, -1021, -1025, 1036, 1016, 1017], [-1015, 1029, -1034, -1030, -1012, 1010, -1016, 1035, 1031], [-1007, 1013, -1035], [1023, 1002, -1001, 1010], [-1002, 1006], [-1008, -1017, -1033, -1016, 1019], [-1016, 1002, 1008, 1030, -1029], [-1037, 1003, 1019, 1020, 1011], [1015, 1006, -1014], [-1007, 1003, 1004, -1026], [1019, -1023, 1011, -1016], [1023, 1038, 1026], [1033, 1003], [-1039, 1010, 1008], [-1005, -1006, -1020], [1012, -1024], [1030, -1027, -1013], [-1009, -1033, 1038], [1008, -1025, -1006], [-1009, -1023, 1030, -1039], [-1018, 1014, -1009, -1031, -1034, 1037, 1022], [1003, 1015, 1011, 1001], [1004, -1017, -1039], [1001, 1006, 1018], [1001, 1037, -1018, 1032, 1027], [-1002, -1024, -1030], [-1038, -1006, -1024], [-1040, -1013], [-1030, 1019, 1031, 1026, -1008, 1036, -1005, 1024, 1014], [-1027, -1033, -1029, 1018, -1012], [-1038, -1019, -1029], [-1036, -1011, -1034], [1031, -1023, 1006, -1005, -1014], [1009, -1003, 1018], [1038, 1030], [1013, -1011, 1040], [1031, 1001, -1011], [-1032, -1018], [1018, 1019, -1036], [-1013, -1004, -1008], [1028, -1039, 1033, -1027, -1003], [1021, -1017, -1013, 1027, 1025, 1037], [1032, 1019, 1030], [1039, -1002, 1035, 1036, 1020, 1013, 1010, 1019, -1006, -1029, -1022], [-1016, -1015, 1013], [-1026, -1007, -1009, -1015], [1030, 1006, 1004, -1021, 1008], [-1007, 1021, -1031], [1028, 1016, 1004, -1003, 1014, -1002, 1023, -1025], [-1040, 1004, 1031, 1015], [-1002, -1009, 1020, -1018, -1014], [1020, 1038, -1037, 1035, 1011, -1034], [1030, 1009, -1035], [1019, -1029, -1023, -1026, 1021, 1032, 1013], [-1009, 1026], [-1009, -1031, -1020, 1008, -1026, -1017, 1003, -1006, -1023, -1007, -1028], [-1022, 1012, -1027, -1010], [1035, 1022, -1015, -1026, 1023, 1021, -1013, 1009, -1008], [1012, 1029, 1038, 1034], [1015, -1007, 1035, -1009], [-1025, -1021], [-1028, -1004, 1019, -1024, 1035, 1030], [1008, -1039, -1030, 1026], [-1024, 1011, 1018, -1014, -1025, 1022, 1013, -1037, -1008, 1033], [1013, -1014, -1007], [1018, 1020, 1035, 1034, -1039, -1023], [-1024, 1011, 1027, -1025, -1028, -1012], [1036, -1040, -1010, 1008, 1030, 1021, 1025], [1023, 1007, -1012], [1021, -1027, -1037], [-1009, 1014, 1038], [-1025, -1026, 1010, 1033, -1013], [1036, 1010, -1018, -1024, 1039], [1035, 1011], [1022, 1021, -1003], [1015, 1031, -1003, 1022], [-1011, -1036, 1016], [-1019, 1001, 1005], [-1037, 1039, -1009, 1002], [1029, -1016, 1037, 1032], [-1033, -1003, -1008, 1029, -1036], [1022, 1006], [1002, 1019, 1005], [-1022, 1032], [1032, 1021, 1004, 1008], [-1009, -1025], [1007, -1026, -1014, -1015], [1023, 1019, 1009, -1039, 1038, 1012], [1009, 1032, 1021], [-1017, 1031, 1040, -1036], [1006, 1021, -1013, -1034], [-1037, 1031, 1029, -1016], [-1018, 1019, 1013, 1010, -1033, -1029, -1037], [1037, -1001, 1032], [-1028, -1013, 1036, -1023, -1033, 1034, 1022], [-1019, -1007], [-1011, -1024, -1021, 1017], [1035, 1017, -1016, 1038], [-1012, -1019, -1002], [-1013, -1023, 1022, 1016, 1036], [1012, 1001, 1026, 1010, 1040], [1024, -1012, -1008, 1023], [-1011, 1001, -1033], [1037, -1007, 1014, -1011, -1022, 1026, 1012], [-1017, -1018, 1031, 1040, 1032, 1005], [-1020, 1038, -1035, 1022, -1014], [1005, 1003, 1002, 1031, -1035, -1028, -1040, -1011, 1009], [1021, 1023, 1039, 1007, 1028], [1016, 1021], [-1028, 1010], [1020, -1012, -1036], [1017, 1014, -1018], [-1008, -1011], [-1003, -1006, 1018, 1027], [-1039, 1014, 1030], [1037, 1025, 1002, 1017], [-1005, 1037], [-1034, -1019, 1018], [1026, -1017], [-1001, -1003, -1026, 1037, -1036, -1022, -1015, -1009], [-1024, 1037], [-1010, -1019, 1006], [-1005, 1025, 1024, -1030, 1002], [1016, -1013], [1020, -1034, 1040], [-1009, -1036, -1023, 1010], [-1039, 1021, -1020, 1019, -1035], [1002, 1029], [1009, -1005, 1030], [1034, 1026, -1010, 1022, -1005], [1034, 1009, 1037, -1033], [1027, 1009, 1031], [-1034, 1008, -1004], [1016, 1028, 1010], [1007, -1004, 1029, 1009, 1021], [1025, -1002, 1024], [1009, -1017, -1006], [-1020, -1001, -1014], [-1016, 1018, -1024, -1027, -1017, -1004, -1007], [1015, 1033, 1036], [1007, -1040, 1029, 1006, 1017, -1021], [-1012, -1038, -1011, -1021, 1002, 1037], [-1016, 1025, 1007], [-1021, -1027], [-1030, 1020, -1009], [1002, -1024, 1009, 1026], [1035, -1012, -1008, 1033], [-1007, 1032, 1023, 1028, -1020, -1040, 1022, 1030], [1029, -1030, 1031], [1007, -1020, -1018], [-1031, -1017, 1004], [1012, -1017, 1027], [1001, 1016, 1007, -1040, 1032, 1024, -1017], [-1025, -1014, 1032, -1002], [-1017, 1005, 1016, 1023], [1034, 1001, -1013], [1026, -1028, 1018, -1039, -1021, -1011], [1025, -1031, 1032, 1011], [1007, 1030, -1019], [1011, -1033, -1036, 1001], [1026, 1020], [1022, 1019, -1040], [-1035, 1018, 1025], [1001, -1022, 1032, -1019], [1014, 1027, -1036], [1013, -1006, -1030, 1039, -1012], [1011, -1008, 1003, -1016, 1018], [-1008, 1024], [1014, -1004, 1038, 1013], [-1017, -1019, 1021, 1005, 1040, -1026], [1029, 1027], [1039, 1031, -1010, 1019], [-1016, 1002, 1039], [-1027, 1006, -1017, -1032, -1024, -1016, -1035], [1034, 1032, -1024], [-1035, -1037, 1008, 1020], [-1002, 1007, 1009], [1016, 1026, 1008, 1036, 1039, -1035], [-1036, -1030, -1032, 1029], [1039, 1012, 1024, 1003], [1002, 1009], [1078, -1054], [1049, 1042, 1043], [1079, -1048, -1069, -1058], [-1075, -1045, 1043], [1048, -1055, 1079, 1058], [1072, -1050, 1046], [-1041, 1078, 1051], [-1078, 1063, -1079, -1042, 1068, -1074], [1065, 1068, -1057], [1062, -1076, -1080], [1064, -1048, 1073, -1063, 1065, -1046], [1080, 1055, 1072, -1073], [-1044, -1058], [1050, 1042, -1056, 1078, -1054], [1070, 1053, 1069, 1056, 1057, 1044], [1046, 1052, 1050], [1067, 1056, 1065, 1076, 1069], [1042, 1045, 1061], [1077, -1064, 1069, -1044], [-1073, -1051, -1069, -1064, -1061, 1045], [1066, 1080, 1067, 1071], [1064, -1047, -1075], [1073, -1067, 1061], [1073, -1051, -1064, 1049, 1058], [1048, -1063, -1054], [-1041, 1054, -1061, 1064, -1078, 1044, -1049, -1079], [-1047, 1075, 1070], [1045, 1074, 1067], [-1047, -1058, -1064, -1051, 1041, 1063, -1078, -1073, 1044, -1068, 1055], [1063, 1058, -1077, -1052, 1078, 1068, -1054, 1051, -1047, -1072], [-1043, 1045, 1057, 1064, -1065, -1079], [-1050, 1071, 1064], [1042, -1046, -1045, -1079, -1072], [-1055, -1078, 1050], [-1047, -1043], [1053, 1061, 1059], [1071, -1043, 1070], [1063, 1049, -1058, 1042, 1061, -1071, -1056], [-1042, 1077, 1072], [1054, -1050, -1061], [1052, 1051, 1053, -1056], [-1047, 1069, 1072], [1055, 1071, -1059], [-1070, 1045, 1054, -1051], [-1062, 1071, 1076], [-1054, 1047, 1046, -1052, 1074, -1042, -1048, -1075, 1069, 1064, 1049, -1050, -1062, 1068], [1052, -1061, 1072], [-1074, 1045], [-1056, 1080], [-1067, -1078], [-1047, 1079, -1051, -1061, -1042, -1043, 1045, -1052], [1074, -1060, -1056, -1046, 1062, 1066, 1058, -1054, -1072, -1075, -1073, -1044], [-1050, -1065, -1075], [-1065, 1069, -1073], [-1059, -1044, -1072, -1057, -1070], [-1052, 1078, -1070], [-1077, -1046, -1066, -1080], [-1042, -1059], [-1056, 1079, 1069, 1045], [-1052, -1054, -1069, -1045, 1051, -1060, 1074], [1046, -1076, 1054, -1069, -1080, -1073, 1050], [1080, -1054], [-1073, -1048, 1068], [-1065, -1061, 1075], [-1068, -1051, -1050, -1055, -1062], [-1054, 1049, 1048, -1074], [1060, 1055, -1079, -1041], [-1075, 1077, 1078, -1072, -1044, -1059, -1054, -1041], [-1077, -1080, -1066], [-1049, 1071, 1075, 1068], [-1056, -1070, 1057, 1076, -1058, -1071, 1044, 1043], [-1058, 1072, -1071], [1079, 1059, 1069, 1073, -1044, -1051], [1055, -1054, -1045], [1080, 1041, 1052], [-1075, -1055, -1052], [1043, -1052], [-1059, 1041], [1052, -1080, 1065], [-1058, 1064], [-1057, -1078, -1045, -1049], [1050, 1048], [1064, -1051, -1075, 1041, -1069, -1056], [-1053, -1045, -1055, -1075, -1070], [1056, -1044, 1066, 1042, -1050], [-1054, -1056, 1044, -1067, -1073, 1075, 1047, 1051, 1041, -1055], [1047, -1041, 1076, -1069, -1072], [-1071, 1053, -1041], [1055, -1080, -1059], [-1041, 1047, -1058, -1079, 1078], [1073, -1044, -1070, 1065, 1061, -1051, 1057, 1056, -1080], [1063, 1077, -1060], [-1077, 1080, 1062, 1063, 1043, 1059, -1073, -1060], [-1045, 1080, 1055], [1075, 1047, 1056, 1050], [-1060, 1076, -1061], [1066, 1065, 1055], [-1071, -1072, -1047], [1052, 1041, 1080, 1072, 1075], [-1057, 1050, -1062, -1046, -1072, -1061], [-1050, 1063, 1079], [-1066, 1059, -1077, 1069], [-1073, -1047, 1069], [-1052, -1065], [-1049, -1073, 1060, -1044, 1070, 1079, -1041, -1066], [1062, -1069, -1047, -1046, 1056, 1077, 1070, -1071], [-1053, 1049, 1055], [-1047, -1064, -1041, 1045, -1072], [1068, 1047, -1063], [-1052, -1041, 1069, -1042, -1055], [1061, 1074, -1052], [1074, -1066, 1073], [-1074, -1052, -1042, 1047, -1061], [1079, 1046, 1053, 1074, 1044, -1068, -1061], [1058, -1055, 1077], [1052, 1064, -1050], [1068, -1077, 1074, -1073], [1047, -1073, 1066, -1069, 1049], [1073, -1053, -1068], [-1045, 1051], [1056, 1052, -1078, 1051], [1076, -1078], [-1047, 1080, 1061], [-1041, 1075, 1065, 1066, -1051, -1076], [-1072, -1078, 1042, -1069], [-1070, 1044, -1068], [-1063, -1065, -1076, -1042, 1057], [1064, -1046], [1044, -1049, 1074], [1050, -1076, -1070, 1065, 1071], [1055, -1046, -1059], [-1077, -1073, 1063, 1058, 1065, 1041], [1053, 1070, 1060], [1062, -1047, 1054, 1045, 1043], [-1059, 1051, -1063], [-1078, -1052, 1046], [1070, -1057, -1048, 1078, 1052], [-1066, -1062, -1075], [1045, 1041, 1049], [1077, 1073], [-1067, 1059, -1079, 1042, -1076], [-1063, 1077, -1042], [-1067, -1048, -1064], [-1044, 1062, -1041], [1057, -1061, 1053, 1078, 1065], [1064, 1075, 1047, 1043, -1078, 1050, -1058, 1055, 1072, 1046], [-1042, 1041], [1075, 1069, -1073, -1046], [-1060, -1056, -1059, 1053, 1052, -1051, 1072, -1063, 1046, 1076], [-1064, -1049, -1052], [1050, 1060, -1052], [-1045, -1053, -1072, -1046, -1074], [1079, 1075, -1066], [1072, 1066, -1058], [-1069, -1068, 1078], [-1075, -1074, 1078, -1073, -1065, -1042], [1073, 1055, 1072], [1072, 1075, -1062, -1077, -1064, 1069], [1073, -1055], [1067, 1056, 1058, -1072, -1059], [1060, 1078, -1047, -1069, 1050, 1058], [1051, -1074, -1042, -1068], [-1074, 1060, 1043], [1059, 1071, 1065, 1043], [1051, -1073, 1058, 1048, -1047, -1062, -1053], [1056, -1066, -1067], [1056, 1055, 1078, -1051], [1066, -1064, 1067, 1072, 1044, -1063, -1078, 1042], [-1054, -1057, -1068, 1046], [1069, 1051], [-1064, 1060], [-1046, 1050, 1059], [-1043, -1046, -1055], [-1042, -1070, 1069], [-1049, 1061, 1057, 1070, 1048, -1069, -1074], [-1050, -1049, -1056, 1052], [1048, -1049, 1058, -1065], [1076, -1052, 1067, -1064, -1069], [1051, -1066, 1064], [-1046, 1061, 1065, 1076, 1063], [-1047, -1048, 1078, -1080, 1062], [1061, 1047, -1043], [-1071, 1065, -1073, -1041, 1047], [-1059, -1047, 1071, 1077, 1075, -1064], [1051, -1045, -1048], [-1070, -1066, -1041], [-1062, 1078, -1054, -1079], [-1066, 1049], [-1044, 1045, -1067, 1064, 1053, -1057], [1047, 1073, 1041, -1050], [-1047, 1054, 1074], [-1054, 1069, 1066, -1073], [-1046, -1049], [-1059, 1055], [1074, -1046, 1056, 1052, -1053, -1079, -1051], [1043, 1054, 1052], [-1077, 1063, 1053, 1074], [-1048, -1053, -1055, 1063, -1058], [-1072, -1066, 1063], [-1048, 1058, 1044], [1076, 1078, -1048, -1058, 1074, 1047], [1061, -1048, 1047, -1063, -1052, 1044, 1062], [1078, 1071, 1080], [-1050, 1074, -1065, -1077], [1062, 1061, 1069, 1053], [-1066, 1062, -1059, -1079, -1050], [1069, -1072, 1056], [-1063, -1046, 1079], [1051, -1047, -1065, -1055, -1053], [-1042, -1047, -1057, 1071, -1051], [-1069, -1051], [1067, -1051, -1054], [1077, 1045, -1041], [1058, -1048, -1076, 1055, 1046], [1047, 1076, -1064, -1044, -1069, 1077], [-1064, -1059, 1048], [1057, -1080, 1063, 1072, -1056], [1041, 1056, -1052, 1062], [-1041, -1046, 1063, 1066, -1065, -1067, 1068, 1048, 1061, 1043, -1045], [-1079, -1048, -1078, -1052, -1059, -1061, 1067], [1072, -1054], [-1065, 1066, 1041], [-1045, -1057], [1051, -1067, 1052, 1079], [-1044, 1051, 1056, 1061], [-1072, -1054, -1080, -1066, 1071], [-1042, -1062, 1073], [-1078, -1051], [1043, -1045, -1072, 1075, 1080, 1044, 1064], [-1059, -1061], [-1042, 1050, -1054], [-1073, -1054, -1065], [1064, 1068, 1065, -1063, -1074, 1045, -1042], [1064, 1042], [-1052, -1075, 1050], [1079, -1055, -1069], [-1063, 1071, 1057, -1065, 1076], [1062, -1059, -1054], [1059, -1076, 1053, -1049, -1075, 1046, 1080, 1047, -1067, -1065, 1055], [1080, -1041], [-1048, -1077, 1052, -1065], [1059, 1067, 1045, -1042], [-1059, -1066, 1065, -1068], [-1048, 1051, 1061], [1058, 1052, -1057, -1076], [1059, 1048, 1042], [1063, -1074, -1041, -1075], [1076, 1080, -1067, -1051, 1069], [-1076, 1043, -1071], [1077, -1079, -1061, -1051], [1060, 1042, -1046], [1079, -1041], [-1053, -1080, -1075, 1061], [-1069, 1061, 1042], [1052, 1041, 1061], [1072, 1044, -1058, 1047, 1064, -1042, 1059], [1047, 1056, -1041, -1043], [-1062, -1043, -1048, 1059, -1045, -1080, 1052, 1058, 1067, 1050, 1070, -1041], [1073, 1066, -1072], [-1061, 1065, 1052], [-1064, -1069, -1070, -1068, 1065, -1050], [-1079, 1070, -1080, 1065, 1041, -1046, 1047, -1048], [1073, 1072, 1065], [-1047, 1065, 1049], [1118, -1094], [1089, 1082, 1083], [1119, -1088, -1109, -1098], [-1115, -1085, 1083], [1088, -1095, 1119, 1098], [1112, -1090, 1086], [-1081, 1118, 1091], [-1118, 1103, -1119, -1082, 1108, -1114], [1105, 1108, -1097], [1102, -1116, -1120], [1104, -1088, 1113, -1103, 1105, -1086], [1120, 1095, 1112, -1113], [-1084, -1098], [1090, 1082, -1096, 1118, -1094], [1110, 1093, 1109, 1096, 1097, 1084], [1086, 1092, 1090], [1107, 1096, 1105, 1116, 1109], [1082, 1085, 1101], [1117, -1104, 1109, -1084], [-1113, -1091, -1109, -1104, -1101, 1085], [1106, 1120, 1107, 1111], [1104, -1087, -1115], [1113, -1107, 1101], [1113, -1091, -1104, 1089, 1098], [1088, -1103, -1094], [-1081, 1094, -1101, 1104, -1118, 1084, -1089, -1119], [-1087, 1115, 1110], [1085, 1114, 1107], [-1087, -1098, -1104, -1091, 1081, 1103, -1118, -1113, 1084, -1108, 1095], [1103, 1098, -1117, -1092, 1118, 1108, -1094, 1091, -1087, -1112], [-1083, 1085, 1097, 1104, -1105, -1119], [-1090, 1111, 1104], [1082, -1086, -1085, -1119, -1112], [-1095, -1118, 1090], [-1087, -1083], [1093, 1101, 1099], [1111, -1083, 1110], [1103, 1089, -1098, 1082, 1101, -1111, -1096], [-1082, 1117, 1112], [1094, -1090, -1101], [1092, 1091, 1093, -1096], [-1087, 1109, 1112], [1095, 1111, -1099], [-1110, 1085, 1094, -1091], [-1102, 1111, 1116], [-1094, 1087, 1086, -1092, 1114, -1082, -1088, -1115, 1109, 1104, 1089, -1090, -1102, 1108], [1092, -1101, 1112], [-1114, 1085], [-1096, 1120], [-1107, -1118], [-1087, 1119, -1091, -1101, -1082, -1083, 1085, -1092], [1114, -1100, -1096, -1086, 1102, 1106, 1098, -1094, -1112, -1115, -1113, -1084], [-1090, -1105, -1115], [-1105, 1109, -1113], [-1099, -1084, -1112, -1097, -1110], [-1092, 1118, -1110], [-1117, -1086, -1106, -1120], [-1082, -1099], [-1096, 1119, 1109, 1085], [-1092, -1094, -1109, -1085, 1091, -1100, 1114], [1086, -1116, 1094, -1109, -1120, -1113, 1090], [1120, -1094], [-1113, -1088, 1108], [-1105, -1101, 1115], [-1108, -1091, -1090, -1095, -1102], [-1094, 1089, 1088, -1114], [1100, 1095, -1119, -1081], [-1115, 1117, 1118, -1112, -1084, -1099, -1094, -1081], [-1117, -1120, -1106], [-1089, 1111, 1115, 1108], [-1096, -1110, 1097, 1116, -1098, -1111, 1084, 1083], [-1098, 1112, -1111], [1119, 1099, 1109, 1113, -1084, -1091], [1095, -1094, -1085], [1120, 1081, 1092], [-1115, -1095, -1092], [1083, -1092], [-1099, 1081], [1092, -1120, 1105], [-1098, 1104], [-1097, -1118, -1085, -1089], [1090, 1088], [1104, -1091, -1115, 1081, -1109, -1096], [-1093, -1085, -1095, -1115, -1110], [1096, -1084, 1106, 1082, -1090], [-1094, -1096, 1084, -1107, -1113, 1115, 1087, 1091, 1081, -1095], [1087, -1081, 1116, -1109, -1112], [-1111, 1093, -1081], [1095, -1120, -1099], [-1081, 1087, -1098, -1119, 1118], [1113, -1084, -1110, 1105, 1101, -1091, 1097, 1096, -1120], [1103, 1117, -1100], [-1117, 1120, 1102, 1103, 1083, 1099, -1113, -1100], [-1085, 1120, 1095], [1115, 1087, 1096, 1090], [-1100, 1116, -1101], [1106, 1105, 1095], [-1111, -1112, -1087], [1092, 1081, 1120, 1112, 1115], [-1097, 1090, -1102, -1086, -1112, -1101], [-1090, 1103, 1119], [-1106, 1099, -1117, 1109], [-1113, -1087, 1109], [-1092, -1105], [-1089, -1113, 1100, -1084, 1110, 1119, -1081, -1106], [1102, -1109, -1087, -1086, 1096, 1117, 1110, -1111], [-1093, 1089, 1095], [-1087, -1104, -1081, 1085, -1112], [1108, 1087, -1103], [-1092, -1081, 1109, -1082, -1095], [1101, 1114, -1092], [1114, -1106, 1113], [-1114, -1092, -1082, 1087, -1101], [1119, 1086, 1093, 1114, 1084, -1108, -1101], [1098, -1095, 1117], [1092, 1104, -1090], [1108, -1117, 1114, -1113], [1087, -1113, 1106, -1109, 1089], [1113, -1093, -1108], [-1085, 1091], [1096, 1092, -1118, 1091], [1116, -1118], [-1087, 1120, 1101], [-1081, 1115, 1105, 1106, -1091, -1116], [-1112, -1118, 1082, -1109], [-1110, 1084, -1108], [-1103, -1105, -1116, -1082, 1097], [1104, -1086], [1084, -1089, 1114], [1090, -1116, -1110, 1105, 1111], [1095, -1086, -1099], [-1117, -1113, 1103, 1098, 1105, 1081], [1093, 1110, 1100], [1102, -1087, 1094, 1085, 1083], [-1099, 1091, -1103], [-1118, -1092, 1086], [1110, -1097, -1088, 1118, 1092], [-1106, -1102, -1115], [1085, 1081, 1089], [1117, 1113], [-1107, 1099, -1119, 1082, -1116], [-1103, 1117, -1082], [-1107, -1088, -1104], [-1084, 1102, -1081], [1097, -1101, 1093, 1118, 1105], [1104, 1115, 1087, 1083, -1118, 1090, -1098, 1095, 1112, 1086], [-1082, 1081], [1115, 1109, -1113, -1086], [-1100, -1096, -1099, 1093, 1092, -1091, 1112, -1103, 1086, 1116], [-1104, -1089, -1092], [1090, 1100, -1092], [-1085, -1093, -1112, -1086, -1114], [1119, 1115, -1106], [1112, 1106, -1098], [-1109, -1108, 1118], [-1115, -1114, 1118, -1113, -1105, -1082], [1113, 1095, 1112], [1112, 1115, -1102, -1117, -1104, 1109], [1113, -1095], [1107, 1096, 1098, -1112, -1099], [1100, 1118, -1087, -1109, 1090, 1098], [1091, -1114, -1082, -1108], [-1114, 1100, 1083], [1099, 1111, 1105, 1083], [1091, -1113, 1098, 1088, -1087, -1102, -1093], [1096, -1106, -1107], [1096, 1095, 1118, -1091], [1106, -1104, 1107, 1112, 1084, -1103, -1118, 1082], [-1094, -1097, -1108, 1086], [1109, 1091], [-1104, 1100], [-1086, 1090, 1099], [-1083, -1086, -1095], [-1082, -1110, 1109], [-1089, 1101, 1097, 1110, 1088, -1109, -1114], [-1090, -1089, -1096, 1092], [1088, -1089, 1098, -1105], [1116, -1092, 1107, -1104, -1109], [1091, -1106, 1104], [-1086, 1101, 1105, 1116, 1103], [-1087, -1088, 1118, -1120, 1102], [1101, 1087, -1083], [-1111, 1105, -1113, -1081, 1087], [-1099, -1087, 1111, 1117, 1115, -1104], [1091, -1085, -1088], [-1110, -1106, -1081], [-1102, 1118, -1094, -1119], [-1106, 1089], [-1084, 1085, -1107, 1104, 1093, -1097], [1087, 1113, 1081, -1090], [-1087, 1094, 1114], [-1094, 1109, 1106, -1113], [-1086, -1089], [-1099, 1095], [1114, -1086, 1096, 1092, -1093, -1119, -1091], [1083, 1094, 1092], [-1117, 1103, 1093, 1114], [-1088, -1093, -1095, 1103, -1098], [-1112, -1106, 1103], [-1088, 1098, 1084], [1116, 1118, -1088, -1098, 1114, 1087], [1101, -1088, 1087, -1103, -1092, 1084, 1102], [1118, 1111, 1120], [-1090, 1114, -1105, -1117], [1102, 1101, 1109, 1093], [-1106, 1102, -1099, -1119, -1090], [1109, -1112, 1096], [-1103, -1086, 1119], [1091, -1087, -1105, -1095, -1093], [-1082, -1087, -1097, 1111, -1091], [-1109, -1091], [1107, -1091, -1094], [1117, 1085, -1081], [1098, -1088, -1116, 1095, 1086], [1087, 1116, -1104, -1084, -1109, 1117], [-1104, -1099, 1088], [1097, -1120, 1103, 1112, -1096], [1081, 1096, -1092, 1102], [-1081, -1086, 1103, 1106, -1105, -1107, 1108, 1088, 1101, 1083, -1085], [-1119, -1088, -1118, -1092, -1099, -1101, 1107], [1112, -1094], [-1105, 1106, 1081], [-1085, -1097], [1091, -1107, 1092, 1119], [-1084, 1091, 1096, 1101], [-1112, -1094, -1120, -1106, 1111], [-1082, -1102, 1113], [-1118, -1091], [1083, -1085, -1112, 1115, 1120, 1084, 1104], [-1099, -1101], [-1082, 1090, -1094], [-1113, -1094, -1105], [1104, 1108, 1105, -1103, -1114, 1085, -1082], [1104, 1082], [-1092, -1115, 1090], [1119, -1095, -1109], [-1103, 1111, 1097, -1105, 1116], [1102, -1099, -1094], [1099, -1116, 1093, -1089, -1115, 1086, 1120, 1087, -1107, -1105, 1095], [1120, -1081], [-1088, -1117, 1092, -1105], [1099, 1107, 1085, -1082], [-1099, -1106, 1105, -1108], [-1088, 1091, 1101], [1098, 1092, -1097, -1116], [1099, 1088, 1082], [1103, -1114, -1081, -1115], [1116, 1120, -1107, -1091, 1109], [-1116, 1083, -1111], [1117, -1119, -1101, -1091], [1100, 1082, -1086], [1119, -1081], [-1093, -1120, -1115, 1101], [-1109, 1101, 1082], [1092, 1081, 1101], [1112, 1084, -1098, 1087, 1104, -1082, 1099], [1087, 1096, -1081, -1083], [-1102, -1083, -1088, 1099, -1085, -1120, 1092, 1098, 1107, 1090, 1110, -1081], [1113, 1106, -1112], [-1101, 1105, 1092], [-1104, -1109, -1110, -1108, 1105, -1090], [-1119, 1110, -1120, 1105, 1081, -1086, 1087, -1088], [1113, 1112, 1105], [1087, 1105, 1089], [-1157, -1125, -1151, 1150, -1126, -1155], [1136, 1149, 1124], [-1149, -1125, 1144], [1135, 1150], [1147, 1160, -1143, 1157], [1128, -1140, -1134, 1156, 1123, 1145], [-1144, -1129, 1159, -1140, -1145, -1122, 1151], [1128, -1127], [1148, -1147, -1151, -1141, 1159], [-1124, 1125, -1149], [1151, 1123, 1126, -1124, -1129, 1144, 1150, -1132, -1138], [1148, 1122, 1144, 1152], [-1122, 1138, 1153, -1141, -1139], [-1123, -1139, 1155], [-1148, -1154, -1160], [-1150, -1160, -1124], [-1129, -1154, 1153, 1152, 1125], [1136, 1153], [-1129, 1125, -1126, -1157], [-1129, -1147, -1158, -1134], [-1126, 1154, 1139], [1160, 1150, -1123, -1134, -1135, -1158], [-1131, -1140, -1138, -1145, -1129, 1126], [-1153, -1144], [1133, -1145, -1128], [-1141, 1131, -1137], [-1136, 1151, -1122], [1143, 1121, 1136, -1149, -1138, 1145, 1122, -1148], [1143, 1142, 1121, 1152, 1141], [-1148, 1144, 1123, -1128], [1157, -1141, -1126], [1138, 1130, 1139, 1159, 1142], [-1140, -1146, -1147, 1142, 1136], [1145, 1122], [1143, -1138, -1160, -1157, -1126, -1131], [1133, -1126], [1148, 1123, -1122, -1137, -1152], [1152, 1132, -1137, 1121], [-1133, 1149, -1132, 1148], [-1158, -1132, 1124, 1153], [1131, 1129, -1151], [-1138, -1154, 1156, -1137, -1130], [-1148, -1142, -1128], [1125, 1155, -1134, -1139], [-1135, 1126, -1125, 1141, 1160, 1150], [-1139, -1151, -1145, 1137], [-1135, 1139, -1129, -1122, -1158], [1146, -1142, -1157, 1132], [-1138, 1135, -1125, -1144], [1123, -1135, 1159, -1130], [-1121, -1122, -1123], [1131, -1151, 1123], [1135, -1134, -1145], [-1124, 1146, -1145, 1129, -1132, 1154, -1135, 1123], [-1158, 1130, -1129], [1138, -1160, 1129, 1144, -1124, -1137], [1152, 1150, -1123, 1146], [1158, 1133, 1155], [1155, -1149, 1123], [-1139, 1132, 1122, 1125, -1133, -1140, 1130, -1154], [-1159, 1152, 1122], [-1151, 1144, 1127, -1130, -1124], [-1155, 1150, 1138, 1132], [-1144, -1131, 1126, 1157, -1158, -1124], [1130, 1129, 1155, 1133, -1150, -1151], [-1160, 1124, -1138], [1129, 1121, 1134], [1154, 1152, 1131, -1150, 1122, 1157], [-1128, 1136, 1156, 1121], [-1153, 1123], [-1146, 1147, -1126], [-1145, 1129, -1123, -1151, -1139, -1141], [-1144, 1139, 1150, -1123, 1147], [-1126, -1134, 1123, -1136, 1142, 1144, 1154], [-1135, 1140, -1159, 1150], [-1141, 1136, -1126, -1130, -1129, -1155], [-1143, 1133, 1144], [1147, -1125, -1160], [1125, 1135, -1158, 1154], [1139, -1135, -1131], [1133, 1155, -1142, -1121, -1127, 1154], [1158, -1146, 1154, 1125, -1131, -1150], [1138, -1143, -1134], [-1132, 1127], [1154, -1121, -1153, -1142, 1132, 1138], [-1153, 1151, -1132], [1136, 1142, -1143, -1160, 1141, -1134, 1153], [-1142, -1155, -1140], [1155, -1126, 1150], [1156, 1145, 1150, 1153, 1157, -1144], [-1130, -1144, -1151], [1125, -1145, 1150, -1142, 1158], [-1137, -1136, -1151], [-1146, 1132, 1159], [-1149, 1127, -1160, -1152, 1129, -1159], [1140, 1123, 1142, 1159, -1138], [-1145, -1133, 1143, 1122, -1126], [-1129, 1146, -1156, -1130, -1138], [-1145, -1127, -1148, 1137, -1150], [1137, -1148, -1129], [1158, 1136, -1134], [-1126, -1121, -1124, -1138], [-1136, -1157, -1130], [1139, 1135, -1124], [-1150, -1147, -1130, -1157], [-1145, -1149, 1147, -1153], [-1121, 1126, 1156, -1155], [-1157, -1148, 1145, 1136, 1131, 1152, 1158, 1156, 1124, 1153, 1125, -1144, 1133, -1126], [-1160, -1132, -1131], [-1144, 1126, 1130, 1156, -1158, -1147], [1126, 1156], [-1138, -1129, -1127, -1153, 1155, -1131, -1151, 1126, 1143], [-1158, 1129, -1127, 1151, 1153, 1123], [-1155, 1143, -1152], [-1148, -1132, -1145, -1138, -1157, -1122, -1140], [-1156, 1157, -1124, 1139, -1130, -1122], [-1130, -1141, 1125], [-1138, -1144, -1157, -1155, -1151, -1128], [-1155, -1152, 1154, 1127, -1141], [1158, -1134, -1145], [-1132, -1147, -1148, 1130, 1149, 1158], [1130, 1121, -1160], [1143, 1125, 1154, -1148], [-1141, 1124, -1129, 1158], [-1149, -1136, 1153, -1154], [1148, 1129, -1145], [1123, -1136, -1153, 1159, 1158, -1140], [-1131, -1130, -1129, 1139, -1151, 1156], [1122, -1128, 1147], [-1138, -1123, -1132], [1125, 1144], [1126, -1153], [1133, 1156, 1152, -1129], [1125, -1145, -1134, -1122], [1157, -1123, -1155, -1124, 1137], [1140, 1134, -1147], [-1148, 1157, -1156], [-1157, -1136, 1129, -1138], [-1148, 1137, -1151, -1136, -1126, -1142], [1126, 1152, 1129], [-1134, 1129, 1123, 1131], [-1136, 1141, 1138, 1140, 1129, -1157], [-1157, 1138, -1135], [1143, 1160, -1140, -1159, 1128, 1147, -1132], [1144, -1153, -1157, -1138, 1143, 1130, -1126], [1153, 1142, 1158, -1124, -1159], [1138, 1157, 1146, 1150, 1126], [1136, 1138, -1121], [-1156, 1140, -1152], [1128, 1127, 1156], [1150, -1153, 1143, -1138, 1142, 1122, 1131, -1154, 1125], [1123, 1156, -1135], [1146, 1138, 1142, -1151, -1131, -1160, -1147, -1130], [1140, -1139, -1158, -1130], [1149, -1151, 1142, 1158, 1150], [1145, 1123, -1142, 1160, 1130, -1131, 1132], [-1144, 1155, -1150, -1149], [1149, 1128], [1139, 1140, 1146, 1149], [1127, 1150, -1132, 1153], [1124, 1123], [-1150, -1121, -1154, 1123, 1140], [1153, -1160, 1134, 1137], [1139, 1152, 1150, -1148, 1143, 1140, -1138, -1149], [-1121, 1132, -1122, 1159], [1133, -1135, 1150, -1127, 1149, 1125], [1157, -1124, -1153, -1130, -1135, -1142, 1154], [1141, 1151, -1160, 1137, 1135], [-1124, -1137, -1127, -1125], [1157, -1140, -1138], [-1153, -1124, -1132, -1156], [-1126, 1159, 1131], [-1122, -1131, -1132], [-1145, 1160, -1148], [1131, 1122, -1125, -1140], [1149, 1122, -1129, -1160, 1130, -1121, 1132, 1155], [1139, -1158, 1154], [1131, 1146], [1127, -1124, -1147], [-1134, -1122, -1145, -1157, 1125], [-1147, -1145, 1153, -1124, -1143, -1128, -1160], [1132, 1124, -1136, 1122], [1152, -1160, 1142, -1122, 1130, -1149], [1154, -1151, 1133], [-1141, 1154, 1160], [-1150, 1134, 1121, 1138], [1121, -1156, 1159, 1147, -1143, 1125], [-1154, -1130, 1128, 1142, -1144], [-1130, 1129, 1153, 1160, 1159, -1147, 1137, -1149, 1122], [-1132, -1160, -1159], [1126, 1156, -1142, 1147, -1152, -1124, 1132, -1144], [-1139, 1128, 1153, 1140, -1141, 1156, -1150, 1129, 1127, -1131, -1125], [1126, 1148, 1145], [1121, 1146, 1132, -1130, -1148, 1158, 1145], [-1143, 1137, 1150], [1137, 1121, -1148, 1129, -1126, -1160, -1133, 1150, -1153], [1157, 1142, 1133, 1144], [-1137, -1141, -1159], [1128, 1154, -1122], [-1156, -1141, 1124, 1144, 1155], [1143, -1156], [1132, 1126], [1128, -1153, 1121], [-1147, 1131, 1156, 1140], [1129, 1133], [-1126, -1136, -1131, 1148, 1142, -1158, -1145, -1143, 1130, 1128, 1139, 1153, 1151], [1156, -1157], [-1127, -1144, -1124, -1142], [-1126, 1138, 1150], [-1145, -1138, 1156, 1133, 1160, -1125], [-1156, -1140, 1159, 1152, 1121, -1126], [-1125, -1134, 1147, 1124], [1123, -1136, -1133], [-1156, -1130], [1143, -1134, 1138, 1128, -1127], [1143, -1152, -1130], [1160, 1136, -1124, 1135, -1122, -1123, -1127], [-1156, -1137], [-1159, 1126], [-1129, 1152, 1160, 1156], [-1128, 1156, 1147, -1134, 1137], [-1123, -1135, -1139, -1157, 1159], [-1137, 1146, -1156], [-1153, 1149], [-1145, -1137, -1126], [1138, 1158, 1128, 1146], [1158, 1140, -1134, -1127, -1130, -1132, -1124, -1141, 1136], [1157, 1155, -1151], [-1129, 1125], [-1147, -1157, 1138], [-1146, -1130, 1160, 1137], [1147, -1143, 1160], [-1137, 1140, 1141], [1148, -1140, 1159, 1154, 1155, -1126, -1160, -1145, 1124, -1132, 1123, -1127, 1142, 1158, 1153], [1145, 1135], [1149, 1131, 1156, -1138], [-1128, -1146, 1143, -1129, -1154, 1130], [1133, -1158, -1129, -1154], [-1140, 1158, -1151, -1153, 1129, -1138, 1125, -1156, 1135, -1159], [1131, -1144, 1159, -1138, 1128, 1130, -1148], [-1146, -1144, 1131, -1132, -1123, 1151], [-1128, -1129, 1148, -1127, -1124], [1137, 1142, -1138], [1146, 1150, 1132, -1128, -1147, -1136, -1155], [-1144, 1148, -1133], [-1149, -1124], [1133, 1122, -1154], [1145, 1153, -1133, -1141], [1127, 1134, 1144], [1133, -1141, -1121, 1138], [-1156, 1137], [1123, -1136, 1144], [-1197, -1165, -1191, 1190, -1166, -1195], [1176, 1189, 1164], [-1189, -1165, 1184], [1175, 1190], [1187, 1200, -1183, 1197], [1168, -1180, -1174, 1196, 1163, 1185], [-1184, -1169, 1199, -1180, -1185, -1162, 1191], [1168, -1167], [1188, -1187, -1191, -1181, 1199], [-1164, 1165, -1189], [1191, 1163, 1166, -1164, -1169, 1184, 1190, -1172, -1178], [1188, 1162, 1184, 1192], [-1162, 1178, 1193, -1181, -1179], [-1163, -1179, 1195], [-1188, -1194, -1200], [-1190, -1200, -1164], [-1169, -1194, 1193, 1192, 1165], [1176, 1193], [-1169, 1165, -1166, -1197], [-1169, -1187, -1198, -1174], [-1166, 1194, 1179], [1200, 1190, -1163, -1174, -1175, -1198], [-1171, -1180, -1178, -1185, -1169, 1166], [-1193, -1184], [1173, -1185, -1168], [-1181, 1171, -1177], [-1176, 1191, -1162], [1183, 1161, 1176, -1189, -1178, 1185, 1162, -1188], [1183, 1182, 1161, 1192, 1181], [-1188, 1184, 1163, -1168], [1197, -1181, -1166], [1178, 1170, 1179, 1199, 1182], [-1180, -1186, -1187, 1182, 1176], [1185, 1162], [1183, -1178, -1200, -1197, -1166, -1171], [1173, -1166], [1188, 1163, -1162, -1177, -1192], [1192, 1172, -1177, 1161], [-1173, 1189, -1172, 1188], [-1198, -1172, 1164, 1193], [1171, 1169, -1191], [-1178, -1194, 1196, -1177, -1170], [-1188, -1182, -1168], [1165, 1195, -1174, -1179], [-1175, 1166, -1165, 1181, 1200, 1190], [-1179, -1191, -1185, 1177], [-1175, 1179, -1169, -1162, -1198], [1186, -1182, -1197, 1172], [-1178, 1175, -1165, -1184], [1163, -1175, 1199, -1170], [-1161, -1162, -1163], [1171, -1191, 1163], [1175, -1174, -1185], [-1164, 1186, -1185, 1169, -1172, 1194, -1175, 1163], [-1198, 1170, -1169], [1178, -1200, 1169, 1184, -1164, -1177], [1192, 1190, -1163, 1186], [1198, 1173, 1195], [1195, -1189, 1163], [-1179, 1172, 1162, 1165, -1173, -1180, 1170, -1194], [-1199, 1192, 1162], [-1191, 1184, 1167, -1170, -1164], [-1195, 1190, 1178, 1172], [-1184, -1171, 1166, 1197, -1198, -1164], [1170, 1169, 1195, 1173, -1190, -1191], [-1200, 1164, -1178], [1169, 1161, 1174], [1194, 1192, 1171, -1190, 1162, 1197], [-1168, 1176, 1196, 1161], [-1193, 1163], [-1186, 1187, -1166], [-1185, 1169, -1163, -1191, -1179, -1181], [-1184, 1179, 1190, -1163, 1187], [-1166, -1174, 1163, -1176, 1182, 1184, 1194], [-1175, 1180, -1199, 1190], [-1181, 1176, -1166, -1170, -1169, -1195], [-1183, 1173, 1184], [1187, -1165, -1200], [1165, 1175, -1198, 1194], [1179, -1175, -1171], [1173, 1195, -1182, -1161, -1167, 1194], [1198, -1186, 1194, 1165, -1171, -1190], [1178, -1183, -1174], [-1172, 1167], [1194, -1161, -1193, -1182, 1172, 1178], [-1193, 1191, -1172], [1176, 1182, -1183, -1200, 1181, -1174, 1193], [-1182, -1195, -1180], [1195, -1166, 1190], [1196, 1185, 1190, 1193, 1197, -1184], [-1170, -1184, -1191], [1165, -1185, 1190, -1182, 1198], [-1177, -1176, -1191], [-1186, 1172, 1199], [-1189, 1167, -1200, -1192, 1169, -1199], [1180, 1163, 1182, 1199, -1178], [-1185, -1173, 1183, 1162, -1166], [-1169, 1186, -1196, -1170, -1178], [-1185, -1167, -1188, 1177, -1190], [1177, -1188, -1169], [1198, 1176, -1174], [-1166, -1161, -1164, -1178], [-1176, -1197, -1170], [1179, 1175, -1164], [-1190, -1187, -1170, -1197], [-1185, -1189, 1187, -1193], [-1161, 1166, 1196, -1195], [-1197, -1188, 1185, 1176, 1171, 1192, 1198, 1196, 1164, 1193, 1165, -1184, 1173, -1166], [-1200, -1172, -1171], [-1184, 1166, 1170, 1196, -1198, -1187], [1166, 1196], [-1178, -1169, -1167, -1193, 1195, -1171, -1191, 1166, 1183], [-1198, 1169, -1167, 1191, 1193, 1163], [-1195, 1183, -1192], [-1188, -1172, -1185, -1178, -1197, -1162, -1180], [-1196, 1197, -1164, 1179, -1170, -1162], [-1170, -1181, 1165], [-1178, -1184, -1197, -1195, -1191, -1168], [-1195, -1192, 1194, 1167, -1181], [1198, -1174, -1185], [-1172, -1187, -1188, 1170, 1189, 1198], [1170, 1161, -1200], [1183, 1165, 1194, -1188], [-1181, 1164, -1169, 1198], [-1189, -1176, 1193, -1194], [1188, 1169, -1185], [1163, -1176, -1193, 1199, 1198, -1180], [-1171, -1170, -1169, 1179, -1191, 1196], [1162, -1168, 1187], [-1178, -1163, -1172], [1165, 1184], [1166, -1193], [1173, 1196, 1192, -1169], [1165, -1185, -1174, -1162], [1197, -1163, -1195, -1164, 1177], [1180, 1174, -1187], [-1188, 1197, -1196], [-1197, -1176, 1169, -1178], [-1188, 1177, -1191, -1176, -1166, -1182], [1166, 1192, 1169], [-1174, 1169, 1163, 1171], [-1176, 1181, 1178, 1180, 1169, -1197], [-1197, 1178, -1175], [1183, 1200, -1180, -1199, 1168, 1187, -1172], [1184, -1193, -1197, -1178, 1183, 1170, -1166], [1193, 1182, 1198, -1164, -1199], [1178, 1197, 1186, 1190, 1166], [1176, 1178, -1161], [-1196, 1180, -1192], [1168, 1167, 1196], [1190, -1193, 1183, -1178, 1182, 1162, 1171, -1194, 1165], [1163, 1196, -1175], [1186, 1178, 1182, -1191, -1171, -1200, -1187, -1170], [1180, -1179, -1198, -1170], [1189, -1191, 1182, 1198, 1190], [1185, 1163, -1182, 1200, 1170, -1171, 1172], [-1184, 1195, -1190, -1189], [1189, 1168], [1179, 1180, 1186, 1189], [1167, 1190, -1172, 1193], [1164, 1163], [-1190, -1161, -1194, 1163, 1180], [1193, -1200, 1174, 1177], [1179, 1192, 1190, -1188, 1183, 1180, -1178, -1189], [-1161, 1172, -1162, 1199], [1173, -1175, 1190, -1167, 1189, 1165], [1197, -1164, -1193, -1170, -1175, -1182, 1194], [1181, 1191, -1200, 1177, 1175], [-1164, -1177, -1167, -1165], [1197, -1180, -1178], [-1193, -1164, -1172, -1196], [-1166, 1199, 1171], [-1162, -1171, -1172], [-1185, 1200, -1188], [1171, 1162, -1165, -1180], [1189, 1162, -1169, -1200, 1170, -1161, 1172, 1195], [1179, -1198, 1194], [1171, 1186], [1167, -1164, -1187], [-1174, -1162, -1185, -1197, 1165], [-1187, -1185, 1193, -1164, -1183, -1168, -1200], [1172, 1164, -1176, 1162], [1192, -1200, 1182, -1162, 1170, -1189], [1194, -1191, 1173], [-1181, 1194, 1200], [-1190, 1174, 1161, 1178], [1161, -1196, 1199, 1187, -1183, 1165], [-1194, -1170, 1168, 1182, -1184], [-1170, 1169, 1193, 1200, 1199, -1187, 1177, -1189, 1162], [-1172, -1200, -1199], [1166, 1196, -1182, 1187, -1192, -1164, 1172, -1184], [-1179, 1168, 1193, 1180, -1181, 1196, -1190, 1169, 1167, -1171, -1165], [1166, 1188, 1185], [1161, 1186, 1172, -1170, -1188, 1198, 1185], [-1183, 1177, 1190], [1177, 1161, -1188, 1169, -1166, -1200, -1173, 1190, -1193], [1197, 1182, 1173, 1184], [-1177, -1181, -1199], [1168, 1194, -1162], [-1196, -1181, 1164, 1184, 1195], [1183, -1196], [1172, 1166], [1168, -1193, 1161], [-1187, 1171, 1196, 1180], [1169, 1173], [-1166, -1176, -1171, 1188, 1182, -1198, -1185, -1183, 1170, 1168, 1179, 1193, 1191], [1196, -1197], [-1167, -1184, -1164, -1182], [-1166, 1178, 1190], [-1185, -1178, 1196, 1173, 1200, -1165], [-1196, -1180, 1199, 1192, 1161, -1166], [-1165, -1174, 1187, 1164], [1163, -1176, -1173], [-1196, -1170], [1183, -1174, 1178, 1168, -1167], [1183, -1192, -1170], [1200, 1176, -1164, 1175, -1162, -1163, -1167], [-1196, -1177], [-1199, 1166], [-1169, 1192, 1200, 1196], [-1168, 1196, 1187, -1174, 1177], [-1163, -1175, -1179, -1197, 1199], [-1177, 1186, -1196], [-1193, 1189], [-1185, -1177, -1166], [1178, 1198, 1168, 1186], [1198, 1180, -1174, -1167, -1170, -1172, -1164, -1181, 1176], [1197, 1195, -1191], [-1169, 1165], [-1187, -1197, 1178], [-1186, -1170, 1200, 1177], [1187, -1183, 1200], [-1177, 1180, 1181], [1188, -1180, 1199, 1194, 1195, -1166, -1200, -1185, 1164, -1172, 1163, -1167, 1182, 1198, 1193], [1185, 1175], [1189, 1171, 1196, -1178], [-1168, -1186, 1183, -1169, -1194, 1170], [1173, -1198, -1169, -1194], [-1180, 1198, -1191, -1193, 1169, -1178, 1165, -1196, 1175, -1199], [1171, -1184, 1199, -1178, 1168, 1170, -1188], [-1186, -1184, 1171, -1172, -1163, 1191], [-1168, -1169, 1188, -1167, -1164], [1177, 1182, -1178], [1186, 1190, 1172, -1168, -1187, -1176, -1195], [-1184, 1188, -1173], [-1189, -1164], [1173, 1162, -1194], [1185, 1193, -1173, -1181], [1167, 1174, 1184], [1173, -1181, -1161, 1178], [-1196, 1177], [-1163, -1176, 1184], [1216, 1230, 1222, 1226], [-1237, 1239, 1207, 1222], [1208, 1239, -1205], [1239, 1203, -1211, -1237, 1212, -1224, -1210, 1214], [1224, 1201, -1211], [1227, 1228, -1230], [1229, -1230, -1205], [-1215, -1229, -1239], [-1215, -1202, -1216, -1218, 1232, -1208, -1236, 1221], [1238, 1232, 1231, -1213], [1225, 1210, 1209, -1229], [1225, -1228, -1218, -1205, 1217, -1211], [-1201, -1203, -1213, 1233, -1204], [-1238, 1213, -1231], [1214, -1213], [1205, -1209, -1229, -1210, 1216], [1207, 1228, -1206], [1214, -1221, -1234], [-1232, -1222, -1205, -1226, 1224, 1227, -1228, 1207, -1212, 1202], [1227, -1235, 1201, -1221, 1211, 1208, 1206], [1221, -1233, -1220, 1226, -1228], [-1230, -1207, -1220, 1209], [-1217, -1207, -1235], [-1230, 1233, 1205], [1220, 1219, -1223], [-1228, -1239], [-1229, 1238], [1211, 1217, -1237, -1228, 1210, -1203], [1205, 1224, -1210, 1211, -1240, -1223, -1235, 1237, 1234, 1221, 1206, -1239, -1226, 1203, -1232, -1227, -1213, 1208], [-1212, 1201, -1206], [1235, -1215], [-1218, 1208, 1235, 1219, 1209], [1240, 1238, -1235], [1210, -1219, 1215, -1225], [-1235, 1221, 1217, 1225, 1206, 1213, -1223], [-1225, -1208, -1202], [1232, -1237, -1203, 1206, 1239, -1238, 1236, 1221, -1207, -1233], [1207, -1226, 1239], [-1238, -1212, -1216, 1217], [-1209, 1221, -1226], [-1222, -1220], [-1213, 1222, -1240, -1229, -1203], [-1229, -1208, -1239, -1223, -1236, 1231, 1230, 1209, -1207], [-1216, 1223, 1226, 1231, -1240, -1208], [1222, -1206, 1204, -1223], [-1232, -1233, -1235, -1209, 1221, 1225, 1228], [1225, 1220], [-1205, 1201, 1206, -1204, -1234, -1224], [1206, -1217, -1211], [-1226, 1239, -1223, -1224, 1238, 1225, 1233, 1209], [1222, 1220, -1225], [1234, -1233], [1203, 1208], [-1225, -1236, 1224], [-1219, -1237, 1221, -1228, -1213, 1201, 1223, -1233, 1208], [1220, -1218, 1224, 1236], [-1229, -1234, -1239, -1227, -1225, 1226, -1204], [1202, 1211, 1203, 1227], [1224, 1235, -1228], [1209, -1214, 1240, -1221, 1210], [-1208, -1203, 1229], [-1235, -1209, -1216, -1238], [1231, -1204], [1220, 1222, 1218, -1210, -1224], [-1204, -1212, 1220, 1227, 1234, -1236], [-1228, -1239, 1233, -1206, -1237, -1202], [1238, -1221, -1228, 1216, -1240], [1213, 1221, 1208, -1237, 1228, 1230], [1225, -1220, 1228, -1203], [-1202, -1215, 1220, -1207, -1208, -1219, 1230], [1240, 1217, 1231], [1233, -1218, -1234, -1203, 1237], [-1228, 1213, -1238, 1202], [1212, 1235, -1240], [-1223, 1222, -1233, 1201, -1216, 1203], [-1215, -1218, -1227, -1238], [1205, 1221, 1211, 1215, -1233, 1201, -1230, 1207, -1239, 1213, 1237], [1230, 1215, -1222, 1219], [-1214, 1227, 1228, 1233, 1211, 1205, -1235, -1238], [1208, 1207, -1229, 1201], [-1208, 1234, -1211], [-1228, -1229, -1230, -1237], [-1218, -1216, 1232, -1217, 1240, -1209], [-1239, -1234, -1221], [1238, 1237, -1213, -1232, 1227, -1216, -1208, 1236], [-1238, -1221, -1209, -1240], [-1226, 1203, -1221], [1222, -1219, -1231, 1233, 1234], [1210, 1205, -1208], [-1227, 1233, -1207, 1201, -1216, -1215, 1209], [-1209, -1222, -1205, -1240], [-1220, -1231, -1212, 1226, 1208], [-1215, 1219, -1210], [-1230, -1205, 1228], [1232, -1212, 1237, 1201], [-1222, 1225], [1220, 1213], [1231, 1215, -1207, -1226, -1229, -1203, 1206], [-1202, -1209], [-1204, 1235], [-1236, -1235, -1222], [1205, -1236, -1227], [1212, 1238, 1216, -1229], [-1218, 1217, -1238, -1239, 1221], [1239, 1220], [-1213, -1238, -1234], [1233, -1229, 1240, -1202, -1223, -1212, 1214], [-1234, 1239, 1238, -1201], [-1229, 1214, -1208, -1218, 1228, -1209, -1203, -1201, 1222, 1219, -1225], [-1223, 1203, 1230], [1232, 1224, 1202, 1229, -1218], [1206, 1236, 1232], [-1230, -1239, -1207, -1226, -1214, -1235, -1238, -1233], [1237, 1229, 1225, 1221, 1223, 1226, -1239, -1231, -1217], [-1231, 1210, 1217, 1214], [1227, 1201, 1240, 1233], [-1205, 1235, 1210], [-1236, 1229, 1220, -1235, -1203, 1211, -1219], [1206, 1202], [-1234, 1209, -1235, -1211, -1225, 1210], [1236, -1231, -1239], [1219, -1237, -1206, -1208], [1201, 1232], [1215, 1207, -1226], [-1222, 1210, 1203, 1221, -1228, 1237], [1230, -1237, 1205], [1221, 1226, 1201], [-1213, -1218], [-1216, -1210, -1234, 1236], [-1207, 1212, -1233], [1215, -1204, -1206, 1218, -1232], [-1205, 1229, 1234, -1235], [1214, 1209, -1240, 1208, 1231, -1217, -1203, 1215], [-1217, -1215, -1233], [1211, -1206, -1217, -1208, -1227], [-1240, 1236, 1217, -1219], [-1236, -1224, -1228, 1226, -1235, 1210], [-1215, -1212], [-1203, -1211, 1233, -1202, -1206], [1227, 1224, -1234], [-1236, -1206, -1205, -1210, 1226, -1228], [1219, 1226, -1223], [1236, -1234, -1208, 1229, 1217, 1206], [1220, 1215, 1210, -1207, -1233, 1227, 1213], [1219, 1221, -1239, -1223, 1217, 1220], [1218, -1207, -1240, -1205, -1226], [-1231, 1215, 1235, 1213, 1228, 1225, 1216], [-1219, 1217, -1228], [-1220, 1218, -1233, -1207, 1209, 1216], [-1215, 1230, 1209], [-1239, 1207, 1223, 1225, 1219], [-1238, -1218, -1207, -1208, 1204], [1220, 1229, 1212, 1219, 1227, 1233, -1207, 1208, 1218, -1214], [-1227, 1209], [1232, 1209, -1217, -1237, 1239, -1220], [1218, -1201], [1237, -1208, 1225, 1232], [1210, 1231, 1218, -1214], [1223, -1216, 1202, -1234, -1235, 1208], [-1223, 1221, -1240], [-1204, 1240, -1207, -1237, 1228], [-1236, 1202, 1208], [-1240, 1223, 1217, 1228, 1234, 1203, 1213, 1216], [-1235, 1229, 1209], [-1215, 1230], [-1237, -1215, -1221], [1217, 1206], [-1230, 1217, -1208], [1235, -1227, 1223, -1208, -1207, 1232], [1211, -1224, 1221], [-1201, 1233, 1236], [-1231, -1209, -1225, -1232], [1216, 1228, -1203, -1208, 1205], [-1222, 1230, 1231, 1212, -1210], [1218, -1216, 1240], [-1201, -1228, 1226, 1212, 1230, -1227, -1208, -1207], [-1213, -1212, 1239, -1201, 1211, 1236, -1207, 1232, 1209, -1228, -1226], [-1218, -1225, 1210], [-1209, -1211, -1217, -1208], [1208, 1225, 1228], [-1215, 1240, 1203], [1202, 1219, 1240], [1232, 1219], [-1232, -1222, 1211, 1203], [1217, 1228, 1205, 1240], [1229, -1217, -1234, 1230], [1208, -1219], [-1208, -1206], [1234, 1222, -1240, -1227, 1224], [1222, 1228], [-1216, 1235, -1228, 1224, 1219], [1217, 1229, 1206], [1208, 1235, -1221], [1212, 1206, 1214, -1237, 1202, -1236, 1211], [1222, -1228, 1223], [-1224, 1213, -1218, -1212], [-1234, -1212, -1218], [1215, 1224, 1216], [1212, -1217, 1207], [1234, -1211, -1240, -1238], [1215, 1219, -1235, 1232, -1210], [-1211, -1220, 1238, -1229, -1203, 1222, 1219, 1228, 1230], [-1226, -1224, 1214, -1240], [-1223, -1221, -1215], [-1205, 1236, 1240], [1227, 1203, -1201, -1232, 1218, 1205, -1221, 1224, 1239], [-1203, -1218], [1218, -1230, -1223], [-1202, 1217, 1237, -1204], [-1236, 1225], [1215, 1237, 1210, 1234], [-1226, 1211, 1205, -1233, 1217, 1212, -1215], [-1203, 1238], [1219, 1231, -1218], [1202, 1212, 1209, -1206], [1201, -1205, 1238], [-1228, 1217, -1215, 1238, 1231, -1213, 1220, -1229, 1210, -1234], [-1208, 1217, 1213, 1236], [-1221, -1214, -1218], [1215, -1224, -1237], [-1238, -1202, -1214, 1222], [1213, 1208, 1205, -1219], [-1218, -1217, 1224], [1220, 1235], [1217, -1205], [1211, 1203, -1208], [1211, -1237, 1219, 1221, 1236], [-1221, -1224, -1232], [-1234, 1209, -1237], [1220, 1240, 1218, -1228, -1233], [-1208, -1236, 1235, -1227, 1234, 1201], [1221, 1224, 1230, 1223], [1220, -1210, 1205, -1201], [1206, -1236, 1211, 1212, 1214, 1227], [-1213, -1226, 1222, -1202, -1240], [-1238, 1202, 1217], [-1226, 1224], [1212, -1233, -1201], [1217, -1227, 1240, -1221, -1225], [1201, 1213, 1216, 1229], [-1229, 1204, 1223], [1232, 1214, 1206, 1211], [1256, 1270, 1262, 1266], [-1277, 1279, 1247, 1262], [1248, 1279, -1245], [1279, 1243, -1251, -1277, 1252, -1264, -1250, 1254], [1264, 1241, -1251], [1267, 1268, -1270], [1269, -1270, -1245], [-1255, -1269, -1279], [-1255, -1242, -1256, -1258, 1272, -1248, -1276, 1261], [1278, 1272, 1271, -1253], [1265, 1250, 1249, -1269], [1265, -1268, -1258, -1245, 1257, -1251], [-1241, -1243, -1253, 1273, -1244], [-1278, 1253, -1271], [1254, -1253], [1245, -1249, -1269, -1250, 1256], [1247, 1268, -1246], [1254, -1261, -1274], [-1272, -1262, -1245, -1266, 1264, 1267, -1268, 1247, -1252, 1242], [1267, -1275, 1241, -1261, 1251, 1248, 1246], [1261, -1273, -1260, 1266, -1268], [-1270, -1247, -1260, 1249], [-1257, -1247, -1275], [-1270, 1273, 1245], [1260, 1259, -1263], [-1268, -1279], [-1269, 1278], [1251, 1257, -1277, -1268, 1250, -1243], [1245, 1264, -1250, 1251, -1280, -1263, -1275, 1277, 1274, 1261, 1246, -1279, -1266, 1243, -1272, -1267, -1253, 1248], [-1252, 1241, -1246], [1275, -1255], [-1258, 1248, 1275, 1259, 1249], [1280, 1278, -1275], [1250, -1259, 1255, -1265], [-1275, 1261, 1257, 1265, 1246, 1253, -1263], [-1265, -1248, -1242], [1272, -1277, -1243, 1246, 1279, -1278, 1276, 1261, -1247, -1273], [1247, -1266, 1279], [-1278, -1252, -1256, 1257], [-1249, 1261, -1266], [-1262, -1260], [-1253, 1262, -1280, -1269, -1243], [-1269, -1248, -1279, -1263, -1276, 1271, 1270, 1249, -1247], [-1256, 1263, 1266, 1271, -1280, -1248], [1262, -1246, 1244, -1263], [-1272, -1273, -1275, -1249, 1261, 1265, 1268], [1265, 1260], [-1245, 1241, 1246, -1244, -1274, -1264], [1246, -1257, -1251], [-1266, 1279, -1263, -1264, 1278, 1265, 1273, 1249], [1262, 1260, -1265], [1274, -1273], [1243, 1248], [-1265, -1276, 1264], [-1259, -1277, 1261, -1268, -1253, 1241, 1263, -1273, 1248], [1260, -1258, 1264, 1276], [-1269, -1274, -1279, -1267, -1265, 1266, -1244], [1242, 1251, 1243, 1267], [1264, 1275, -1268], [1249, -1254, 1280, -1261, 1250], [-1248, -1243, 1269], [-1275, -1249, -1256, -1278], [1271, -1244], [1260, 1262, 1258, -1250, -1264], [-1244, -1252, 1260, 1267, 1274, -1276], [-1268, -1279, 1273, -1246, -1277, -1242], [1278, -1261, -1268, 1256, -1280], [1253, 1261, 1248, -1277, 1268, 1270], [1265, -1260, 1268, -1243], [-1242, -1255, 1260, -1247, -1248, -1259, 1270], [1280, 1257, 1271], [1273, -1258, -1274, -1243, 1277], [-1268, 1253, -1278, 1242], [1252, 1275, -1280], [-1263, 1262, -1273, 1241, -1256, 1243], [-1255, -1258, -1267, -1278], [1245, 1261, 1251, 1255, -1273, 1241, -1270, 1247, -1279, 1253, 1277], [1270, 1255, -1262, 1259], [-1254, 1267, 1268, 1273, 1251, 1245, -1275, -1278], [1248, 1247, -1269, 1241], [-1248, 1274, -1251], [-1268, -1269, -1270, -1277], [-1258, -1256, 1272, -1257, 1280, -1249], [-1279, -1274, -1261], [1278, 1277, -1253, -1272, 1267, -1256, -1248, 1276], [-1278, -1261, -1249, -1280], [-1266, 1243, -1261], [1262, -1259, -1271, 1273, 1274], [1250, 1245, -1248], [-1267, 1273, -1247, 1241, -1256, -1255, 1249], [-1249, -1262, -1245, -1280], [-1260, -1271, -1252, 1266, 1248], [-1255, 1259, -1250], [-1270, -1245, 1268], [1272, -1252, 1277, 1241], [-1262, 1265], [1260, 1253], [1271, 1255, -1247, -1266, -1269, -1243, 1246], [-1242, -1249], [-1244, 1275], [-1276, -1275, -1262], [1245, -1276, -1267], [1252, 1278, 1256, -1269], [-1258, 1257, -1278, -1279, 1261], [1279, 1260], [-1253, -1278, -1274], [1273, -1269, 1280, -1242, -1263, -1252, 1254], [-1274, 1279, 1278, -1241], [-1269, 1254, -1248, -1258, 1268, -1249, -1243, -1241, 1262, 1259, -1265], [-1263, 1243, 1270], [1272, 1264, 1242, 1269, -1258], [1246, 1276, 1272], [-1270, -1279, -1247, -1266, -1254, -1275, -1278, -1273], [1277, 1269, 1265, 1261, 1263, 1266, -1279, -1271, -1257], [-1271, 1250, 1257, 1254], [1267, 1241, 1280, 1273], [-1245, 1275, 1250], [-1276, 1269, 1260, -1275, -1243, 1251, -1259], [1246, 1242], [-1274, 1249, -1275, -1251, -1265, 1250], [1276, -1271, -1279], [1259, -1277, -1246, -1248], [1241, 1272], [1255, 1247, -1266], [-1262, 1250, 1243, 1261, -1268, 1277], [1270, -1277, 1245], [1261, 1266, 1241], [-1253, -1258], [-1256, -1250, -1274, 1276], [-1247, 1252, -1273], [1255, -1244, -1246, 1258, -1272], [-1245, 1269, 1274, -1275], [1254, 1249, -1280, 1248, 1271, -1257, -1243, 1255], [-1257, -1255, -1273], [1251, -1246, -1257, -1248, -1267], [-1280, 1276, 1257, -1259], [-1276, -1264, -1268, 1266, -1275, 1250], [-1255, -1252], [-1243, -1251, 1273, -1242, -1246], [1267, 1264, -1274], [-1276, -1246, -1245, -1250, 1266, -1268], [1259, 1266, -1263], [1276, -1274, -1248, 1269, 1257, 1246], [1260, 1255, 1250, -1247, -1273, 1267, 1253], [1259, 1261, -1279, -1263, 1257, 1260], [1258, -1247, -1280, -1245, -1266], [-1271, 1255, 1275, 1253, 1268, 1265, 1256], [-1259, 1257, -1268], [-1260, 1258, -1273, -1247, 1249, 1256], [-1255, 1270, 1249], [-1279, 1247, 1263, 1265, 1259], [-1278, -1258, -1247, -1248, 1244], [1260, 1269, 1252, 1259, 1267, 1273, -1247, 1248, 1258, -1254], [-1267, 1249], [1272, 1249, -1257, -1277, 1279, -1260], [1258, -1241], [1277, -1248, 1265, 1272], [1250, 1271, 1258, -1254], [1263, -1256, 1242, -1274, -1275, 1248], [-1263, 1261, -1280], [-1244, 1280, -1247, -1277, 1268], [-1276, 1242, 1248], [-1280, 1263, 1257, 1268, 1274, 1243, 1253, 1256], [-1275, 1269, 1249], [-1255, 1270], [-1277, -1255, -1261], [1257, 1246], [-1270, 1257, -1248], [1275, -1267, 1263, -1248, -1247, 1272], [1251, -1264, 1261], [-1241, 1273, 1276], [-1271, -1249, -1265, -1272], [1256, 1268, -1243, -1248, 1245], [-1262, 1270, 1271, 1252, -1250], [1258, -1256, 1280], [-1241, -1268, 1266, 1252, 1270, -1267, -1248, -1247], [-1253, -1252, 1279, -1241, 1251, 1276, -1247, 1272, 1249, -1268, -1266], [-1258, -1265, 1250], [-1249, -1251, -1257, -1248], [1248, 1265, 1268], [-1255, 1280, 1243], [1242, 1259, 1280], [1272, 1259], [-1272, -1262, 1251, 1243], [1257, 1268, 1245, 1280], [1269, -1257, -1274, 1270], [1248, -1259], [-1248, -1246], [1274, 1262, -1280, -1267, 1264], [1262, 1268], [-1256, 1275, -1268, 1264, 1259], [1257, 1269, 1246], [1248, 1275, -1261], [1252, 1246, 1254, -1277, 1242, -1276, 1251], [1262, -1268, 1263], [-1264, 1253, -1258, -1252], [-1274, -1252, -1258], [1255, 1264, 1256], [1252, -1257, 1247], [1274, -1251, -1280, -1278], [1255, 1259, -1275, 1272, -1250], [-1251, -1260, 1278, -1269, -1243, 1262, 1259, 1268, 1270], [-1266, -1264, 1254, -1280], [-1263, -1261, -1255], [-1245, 1276, 1280], [1267, 1243, -1241, -1272, 1258, 1245, -1261, 1264, 1279], [-1243, -1258], [1258, -1270, -1263], [-1242, 1257, 1277, -1244], [-1276, 1265], [1255, 1277, 1250, 1274], [-1266, 1251, 1245, -1273, 1257, 1252, -1255], [-1243, 1278], [1259, 1271, -1258], [1242, 1252, 1249, -1246], [1241, -1245, 1278], [-1268, 1257, -1255, 1278, 1271, -1253, 1260, -1269, 1250, -1274], [-1248, 1257, 1253, 1276], [-1261, -1254, -1258], [1255, -1264, -1277], [-1278, -1242, -1254, 1262], [1253, 1248, 1245, -1259], [-1258, -1257, 1264], [1260, 1275], [1257, -1245], [1251, 1243, -1248], [1251, -1277, 1259, 1261, 1276], [-1261, -1264, -1272], [-1274, 1249, -1277], [1260, 1280, 1258, -1268, -1273], [-1248, -1276, 1275, -1267, 1274, 1241], [1261, 1264, 1270, 1263], [1260, -1250, 1245, -1241], [1246, -1276, 1251, 1252, 1254, 1267], [-1253, -1266, 1262, -1242, -1280], [-1278, 1242, 1257], [-1266, 1264], [1252, -1273, -1241], [1257, -1267, 1280, -1261, -1265], [1241, 1253, 1256, 1269], [-1269, 1244, 1263], [-1272, 1254, 1246, 1251], [-1295, 1301, -1306, -1313], [1295, -1316, -1297, -1313, 1301, -1285, -1296], [-1292, -1298, -1287], [1292, -1286, 1308, 1299], [-1303, 1316, -1285], [1319, -1289, 1295, -1314, -1286, 1318, 1290, -1303, 1320], [-1308, 1293, -1304, -1320, -1316], [1298, -1320, 1302, 1299, 1282, 1318], [-1296, -1309, 1297, 1316], [-1313, 1281, -1285, 1309, -1296, 1318, -1311], [-1290, -1317, 1314, 1315, 1313, -1295, -1319, -1300, -1287, -1293, 1291, 1299], [-1294, -1297], [-1285, -1320, 1297, 1308, 1302, 1313, -1316], [-1301, -1292, -1320, 1291, -1302, -1316], [1308, -1319, 1286, -1318, 1297], [-1308, -1307], [1302, -1314, 1316, -1309, 1291], [-1310, 1315, 1313], [1289, -1290, 1297, 1302], [1292, -1286, -1290, 1306, -1297], [1286, 1307, -1312, 1292], [1316, -1310, 1317, -1312], [-1310, -1302], [1281, 1291, -1287], [-1288, -1287, 1317, 1306, -1320, 1290], [-1305, 1294], [1296, 1285, 1292, -1306], [-1282, -1317, -1310], [1287, -1283, 1300, 1309, -1294], [1307, 1286, 1301, -1303], [-1314, -1298, 1307, 1295], [1318, 1310, -1290, -1293, 1313, 1312, 1311], [-1313, 1306, 1314], [-1281, -1312, 1283], [-1315, 1311, 1285], [-1315, -1305, -1300, -1304, -1285], [1317, -1281, -1304, 1305], [1290, -1318, 1317], [1295, -1304, 1288], [-1314, 1295, 1309], [-1309, 1319, 1287], [-1319, -1307, -1304], [1313, -1288, -1286], [-1282, -1284, -1300, 1317, -1283], [-1306, -1304, 1312, 1313], [-1287, -1286, 1293, 1288, 1303, -1281, -1298, -1318], [1301, -1291, -1305], [-1291, -1283, -1304], [1320, -1315, 1290, -1292, -1281, -1309], [1312, 1292, 1294, 1289], [-1301, -1296, 1310, 1297], [-1316, -1301, -1304, -1283], [1320, 1302, -1306, -1286, -1283, -1282, -1292, -1318, 1288], [-1304, -1295, 1317, 1284, -1288, -1312, -1298], [-1315, -1303], [1310, -1284, -1300, 1318], [1282, 1302], [-1312, -1300, 1311, -1320], [1305, 1309, 1286], [1312, -1292], [1306, -1305, 1299], [1311, 1314, 1305, 1315, 1302], [-1293, 1317, 1286, -1320, -1290, 1297, -1310, -1318, -1291, 1299, 1295, -1304], [1289, 1281, 1312], [-1307, -1309, -1297, 1317, -1285, -1306, -1281, -1302, 1312, 1287, -1311], [1298, 1292, 1320], [1302, 1283, -1312], [-1301, 1313, -1310, 1319], [1283, -1290, 1301, 1294, 1317, 1319, 1296, 1298, -1297, 1299, 1291, 1318], [-1318, 1313, 1305, 1320, -1315, -1301, -1310, -1316], [-1314, 1293, 1290], [1297, -1314, 1308], [-1317, 1305, 1281, -1296], [-1319, 1285, 1311, -1286, 1318], [1293, -1291, 1316, -1290], [-1315, 1287], [-1293, 1315, 1305, 1313, -1308, -1290], [1311, -1319, 1297], [-1296, -1311, 1285, 1303, 1310], [-1294, -1292, -1307], [1305, 1309, -1306, 1314, 1298], [-1291, 1313, -1319, 1311, -1293, 1303, -1300, 1290], [-1289, 1299, 1302, -1309, 1319], [1309, 1312, 1296, -1286], [1286, 1299, -1281], [-1313, 1294, 1285, 1302], [1295, 1319, -1308, -1289, -1285, -1290, 1299], [-1312, -1297, 1291, 1307, 1287, -1303], [1308, -1310, -1293], [1299, -1291, 1300, -1290, -1288, 1296, -1305], [-1316, -1281, -1306], [-1314, -1288, -1316, -1308, 1311, 1305, 1292, 1285], [-1292, -1283], [1318, 1289, -1312], [-1298, -1294], [1296, -1297, 1310, 1316, 1301, -1320, -1307], [1295, -1307, -1286, -1292, 1316], [-1312, 1282, -1288, -1300, -1306, -1293, -1285, -1315, 1308, -1303, -1304], [1307, -1303, 1295, 1302], [1284, -1320], [1301, -1291, 1302], [-1290, -1303, -1314], [-1313, -1281, 1317, 1306, 1301], [-1283, -1306, 1313, 1308, 1296], [1317, 1309, -1306], [1281, -1301, -1308], [-1315, 1305, -1296, 1302, -1309, -1291], [1282, 1304, -1306, -1302, 1298, 1293, 1283, -1314], [1305, 1303, 1289, -1311], [1311, 1282, -1305, -1291, -1290], [-1304, -1318], [1306, -1315, -1303], [1299, 1308], [1302, 1312, -1315, -1284, 1286], [-1281, 1302, -1284, 1303, 1301, -1286], [-1315, -1283], [-1304, -1301, -1287], [-1316, 1305, 1313, 1309, 1306, -1304, -1291], [-1306, -1297, -1310, 1291, 1312, -1296, -1311, 1314], [1313, 1296], [-1281, 1282, -1315, -1308], [1301, -1287], [-1298, 1291, 1307], [-1303, -1317, 1286, 1287, 1299], [1320, 1309, 1311, -1301], [1308, 1305, 1287, -1299, 1302], [-1291, -1295, 1292, -1310, -1289], [-1301, -1292, 1316, -1300, 1308, 1290, 1296], [-1291, -1285, -1289, -1301, 1281, -1313, 1315, -1304, -1318, -1305], [1288, 1315, -1292, -1320, -1281], [1299, -1310, 1313], [-1295, -1287, 1291, 1309], [1287, 1293, 1309, 1283, -1314, -1288], [-1312, 1319, 1289, 1308, 1303, -1297, -1304, -1291], [-1288, 1309, 1316, -1295, -1290, 1303, 1320], [-1309, 1319, -1286, 1308, -1315, -1300, -1307], [1308, -1291, 1300], [1298, -1315, -1296, -1312, -1303], [1295, -1301, -1290], [1317, -1286, -1297, 1293], [1288, -1293, 1320], [-1314, 1299, -1310, -1294, 1282, -1297, 1317], [-1290, -1314, -1283, 1308], [1299, -1281, -1298], [1284, -1308, -1301], [-1310, -1307, -1282, 1298, -1286, -1320], [1284, -1282, 1297, 1319], [-1305, -1315, 1298, 1312, -1293], [-1305, 1282, 1303, 1320], [1318, -1282], [1297, -1313, 1305, 1316], [-1297, -1314, 1306], [1285, 1290, 1300], [-1290, -1304, -1295, -1296], [1311, 1284, 1317, -1297, -1293], [-1310, -1317, 1311, -1309], [-1308, 1305, -1302, 1296], [-1289, -1284, -1306, -1301], [-1312, 1310, -1311, 1317], [-1308, -1291, 1283], [1291, 1298], [1296, 1286, -1288, 1282], [1320, 1319, -1307, 1309, -1296], [-1310, 1307, 1283], [1289, 1309], [-1305, 1287, -1319], [1297, -1285, 1299], [1316, -1297, 1285], [1306, 1303, 1310, -1281, -1319, -1313, -1289], [-1305, -1296, 1282], [-1293, 1298, 1317, 1294, -1297], [-1293, 1309, -1311, 1282, -1312, 1294, 1292, 1289, 1303, 1297, 1310], [1298, 1319, -1288, -1282, 1315, 1314], [-1311, 1285, -1303], [-1291, -1299, 1312, 1300, 1286, -1294], [1307, 1294, -1311], [-1300, 1289], [-1307, -1305, 1319], [1313, 1312, -1292, 1293, 1287, -1305, 1314, -1283], [-1289, -1300, -1295, -1282, -1286], [1311, 1309], [-1302, 1298, 1296, -1285], [1311, 1309, 1288, 1285, -1292, 1310, -1305, 1293], [-1314, 1310, 1281], [-1288, -1301, -1296], [1298, -1303, 1304, -1299, -1297], [1313, 1281, 1297], [-1282, -1318, 1320], [1286, -1285, -1289, 1305], [-1310, -1285, -1306, -1311, -1281], [1312, -1298, 1319, 1297, -1314], [1292, 1301, 1305, 1313, 1288, 1306], [-1287, -1305, 1301], [-1304, -1284, 1288, -1319, 1315, 1317], [-1281, -1309, 1293, 1283, -1297, 1300, -1299, 1282, -1306], [1295, 1305, 1296, 1307], [1311, -1317, 1286], [-1308, 1301, 1317], [-1291, 1307, 1297], [-1309, -1316, -1291, 1312], [-1285, 1287, -1283], [-1315, 1284, 1294, -1319, 1305, 1307], [-1288, -1293], [1281, 1313, 1283], [1285, -1306, -1284], [-1319, 1297, 1286, 1295, -1282, -1301], [-1307, -1291], [1301, -1290, 1313], [1282, 1283, -1304, -1300, -1288, -1289, -1301, 1295, 1284], [-1284, 1300, 1301, 1296, -1298, -1305, -1312, -1317], [-1302, 1320, 1314], [-1289, -1314, 1300], [-1316, -1298, 1299, 1295, 1301], [-1297, 1311, -1292, -1303, -1315, 1298, -1296], [1306, 1289, 1290], [-1290, 1296, 1289], [-1319, -1292, -1287, 1286], [1299, 1315, -1308, 1291, 1300, 1314, -1282], [1305, 1315, 1299, 1295, 1286], [-1285, -1302, -1282], [-1317, 1292, 1305], [1285, 1284, 1306], [1299, -1298, -1314, 1281], [1299, 1293, 1319], [1298, 1309, 1283, -1285, -1307], [1304, 1290, 1310, 1316, -1295], [-1284, 1296, -1311, 1310], [-1305, 1315, -1281], [-1312, -1289, -1295, 1311], [1320, 1308, -1315, -1302, -1314, -1281], [1297, -1307], [1294, -1311, 1320], [1313, 1315, -1300, -1302, 1282, -1311, -1316, -1306, 1318], [-1299, -1313, -1301], [-1301, 1317, 1298], [-1302, 1284, 1285, 1310, -1305, 1303], [1314, 1295], [1304, 1289, 1302, 1319, -1292, -1285, 1303, 1288, -1281, -1305, -1310, 1297], [-1318, -1287, -1297, -1309, 1307], [-1309, 1298, 1299, 1294, -1289, 1297], [1314, 1312, -1296], [-1293, -1318, -1294], [-1319, -1304, -1284, 1315], [1317, -1287, 1320, -1306, 1298], [1311, 1298, -1281], [-1315, -1281], [-1283, -1292, -1293, 1289], [-1293, -1297, -1288, 1286, -1296, -1308, 1307, 1284, 1305], [-1307, -1312, 1293], [-1303, 1287, 1319], [1317, 1292, 1283], [-1316, 1282, 1285], [1283, -1303, -1308], [1290, -1301], [-1284, -1298, 1295], [1318, 1310, -1304, 1288, -1319], [-1290, 1299, 1303, -1316, -1314], [1311, -1288, -1319, 1298, -1305, -1282], [1295, -1319, 1313], [-1296, -1295, -1313, -1282], [1288, 1320], [-1287, -1317, 1307, -1295, -1315, -1285, -1299, 1304, -1281, 1290, 1291, -1308, 1312, -1305], [-1310, -1296, -1285, 1292, 1297, 1318], [-1317, 1284, -1301, 1316, 1302], [-1294, -1287, -1292], [-1281, -1304, 1309], [1312, -1297, -1289, -1306, 1319, 1304], [-1298, -1311, -1291, 1303, 1290, 1308, -1304, 1287, 1300, -1309, -1310], [1318, 1286, 1317], [-1290, 1313, 1288, -1315, 1289, 1293], [-1298, 1293, -1291, -1315], [1298, -1288, 1313, 1309, 1307, 1290, 1320, 1315, -1294, 1302], [1295, 1291, -1292, -1314], [-1314, 1298], [1290, 1311, 1319], [1283, 1284, 1289], [-1288, 1286, 1298], [1294, -1296, 1316], [1284, 1307, -1312], [-1299, -1303, 1297, 1319], [1287, -1283, 1301], [1303, 1282, 1298], [-1314, -1286, -1318], [-1304, 1294, 1300, 1310, -1287], [1313, 1318, -1316, 1307], [-1301, -1292, -1281, -1299], [1284, 1291], [-1316, 1287, -1298], [1300, 1302, -1284], [1300, -1319, 1289, -1315], [1287, 1298, 1317], [1314, -1281, -1305, -1318], [-1295, -1294, -1316], [1307, 1298, 1305, -1293, -1296, -1316], [1299, 1294], [-1281, 1286, 1294, 1303, -1283, -1297, -1300, -1289], [1301, 1304, 1313, 1303, -1320], [-1286, 1296, 1316, -1315, -1308, 1309, -1313, -1290, -1287], [-1292, 1315, 1291], [1293, -1299, -1300], [1312, -1317], [-1289, 1310, 1307, -1311], [-1285, 1308, -1288], [1288, -1297, 1317, 1313, 1315, -1291, -1289, -1292, -1306], [-1289, -1310, 1297], [-1291, 1283, -1315], [1283, -1306, -1287, 1309], [1312, 1292, 1288], [1296, 1303, -1307, 1294, -1306], [1299, 1310], [-1296, 1293, -1283, -1309], [-1289, 1291, -1318, 1305], [1288, 1318, 1306, -1292], [1295, 1287, 1288, -1299, 1290], [1290, -1294, 1314, 1299, 1304, 1312, -1310, 1309, 1298], [1284, 1287, -1300, -1318, 1310, -1304, -1285], [-1285, -1312, 1291, -1311], [1298, -1311], [1316, -1291, 1296], [1304, 1297, -1316, -1290, 1305, 1311], [-1313, -1316, 1320, 1294, -1314], [1295, -1291, 1287], [1291, 1289, -1295, 1314, 1319, 1292, -1288, 1315], [1309, -1283, 1317, 1305], [-1316, 1296], [1316, 1309, -1306], [1308, 1285, -1295], [-1292, 1306], [-1335, 1341, -1346, -1353], [1335, -1356, -1337, -1353, 1341, -1325, -1336], [-1332, -1338, -1327], [1332, -1326, 1348, 1339], [-1343, 1356, -1325], [1359, -1329, 1335, -1354, -1326, 1358, 1330, -1343, 1360], [-1348, 1333, -1344, -1360, -1356], [1338, -1360, 1342, 1339, 1322, 1358], [-1336, -1349, 1337, 1356], [-1353, 1321, -1325, 1349, -1336, 1358, -1351], [-1330, -1357, 1354, 1355, 1353, -1335, -1359, -1340, -1327, -1333, 1331, 1339], [-1334, -1337], [-1325, -1360, 1337, 1348, 1342, 1353, -1356], [-1341, -1332, -1360, 1331, -1342, -1356], [1348, -1359, 1326, -1358, 1337], [-1348, -1347], [1342, -1354, 1356, -1349, 1331], [-1350, 1355, 1353], [1329, -1330, 1337, 1342], [1332, -1326, -1330, 1346, -1337], [1326, 1347, -1352, 1332], [1356, -1350, 1357, -1352], [-1350, -1342], [1321, 1331, -1327], [-1328, -1327, 1357, 1346, -1360, 1330], [-1345, 1334], [1336, 1325, 1332, -1346], [-1322, -1357, -1350], [1327, -1323, 1340, 1349, -1334], [1347, 1326, 1341, -1343], [-1354, -1338, 1347, 1335], [1358, 1350, -1330, -1333, 1353, 1352, 1351], [-1353, 1346, 1354], [-1321, -1352, 1323], [-1355, 1351, 1325], [-1355, -1345, -1340, -1344, -1325], [1357, -1321, -1344, 1345], [1330, -1358, 1357], [1335, -1344, 1328], [-1354, 1335, 1349], [-1349, 1359, 1327], [-1359, -1347, -1344], [1353, -1328, -1326], [-1322, -1324, -1340, 1357, -1323], [-1346, -1344, 1352, 1353], [-1327, -1326, 1333, 1328, 1343, -1321, -1338, -1358], [1341, -1331, -1345], [-1331, -1323, -1344], [1360, -1355, 1330, -1332, -1321, -1349], [1352, 1332, 1334, 1329], [-1341, -1336, 1350, 1337], [-1356, -1341, -1344, -1323], [1360, 1342, -1346, -1326, -1323, -1322, -1332, -1358, 1328], [-1344, -1335, 1357, 1324, -1328, -1352, -1338], [-1355, -1343], [1350, -1324, -1340, 1358], [1322, 1342], [-1352, -1340, 1351, -1360], [1345, 1349, 1326], [1352, -1332], [1346, -1345, 1339], [1351, 1354, 1345, 1355, 1342], [-1333, 1357, 1326, -1360, -1330, 1337, -1350, -1358, -1331, 1339, 1335, -1344], [1329, 1321, 1352], [-1347, -1349, -1337, 1357, -1325, -1346, -1321, -1342, 1352, 1327, -1351], [1338, 1332, 1360], [1342, 1323, -1352], [-1341, 1353, -1350, 1359], [1323, -1330, 1341, 1334, 1357, 1359, 1336, 1338, -1337, 1339, 1331, 1358], [-1358, 1353, 1345, 1360, -1355, -1341, -1350, -1356], [-1354, 1333, 1330], [1337, -1354, 1348], [-1357, 1345, 1321, -1336], [-1359, 1325, 1351, -1326, 1358], [1333, -1331, 1356, -1330], [-1355, 1327], [-1333, 1355, 1345, 1353, -1348, -1330], [1351, -1359, 1337], [-1336, -1351, 1325, 1343, 1350], [-1334, -1332, -1347], [1345, 1349, -1346, 1354, 1338], [-1331, 1353, -1359, 1351, -1333, 1343, -1340, 1330], [-1329, 1339, 1342, -1349, 1359], [1349, 1352, 1336, -1326], [1326, 1339, -1321], [-1353, 1334, 1325, 1342], [1335, 1359, -1348, -1329, -1325, -1330, 1339], [-1352, -1337, 1331, 1347, 1327, -1343], [1348, -1350, -1333], [1339, -1331, 1340, -1330, -1328, 1336, -1345], [-1356, -1321, -1346], [-1354, -1328, -1356, -1348, 1351, 1345, 1332, 1325], [-1332, -1323], [1358, 1329, -1352], [-1338, -1334], [1336, -1337, 1350, 1356, 1341, -1360, -1347], [1335, -1347, -1326, -1332, 1356], [-1352, 1322, -1328, -1340, -1346, -1333, -1325, -1355, 1348, -1343, -1344], [1347, -1343, 1335, 1342], [1324, -1360], [1341, -1331, 1342], [-1330, -1343, -1354], [-1353, -1321, 1357, 1346, 1341], [-1323, -1346, 1353, 1348, 1336], [1357, 1349, -1346], [1321, -1341, -1348], [-1355, 1345, -1336, 1342, -1349, -1331], [1322, 1344, -1346, -1342, 1338, 1333, 1323, -1354], [1345, 1343, 1329, -1351], [1351, 1322, -1345, -1331, -1330], [-1344, -1358], [1346, -1355, -1343], [1339, 1348], [1342, 1352, -1355, -1324, 1326], [-1321, 1342, -1324, 1343, 1341, -1326], [-1355, -1323], [-1344, -1341, -1327], [-1356, 1345, 1353, 1349, 1346, -1344, -1331], [-1346, -1337, -1350, 1331, 1352, -1336, -1351, 1354], [1353, 1336], [-1321, 1322, -1355, -1348], [1341, -1327], [-1338, 1331, 1347], [-1343, -1357, 1326, 1327, 1339], [1360, 1349, 1351, -1341], [1348, 1345, 1327, -1339, 1342], [-1331, -1335, 1332, -1350, -1329], [-1341, -1332, 1356, -1340, 1348, 1330, 1336], [-1331, -1325, -1329, -1341, 1321, -1353, 1355, -1344, -1358, -1345], [1328, 1355, -1332, -1360, -1321], [1339, -1350, 1353], [-1335, -1327, 1331, 1349], [1327, 1333, 1349, 1323, -1354, -1328], [-1352, 1359, 1329, 1348, 1343, -1337, -1344, -1331], [-1328, 1349, 1356, -1335, -1330, 1343, 1360], [-1349, 1359, -1326, 1348, -1355, -1340, -1347], [1348, -1331, 1340], [1338, -1355, -1336, -1352, -1343], [1335, -1341, -1330], [1357, -1326, -1337, 1333], [1328, -1333, 1360], [-1354, 1339, -1350, -1334, 1322, -1337, 1357], [-1330, -1354, -1323, 1348], [1339, -1321, -1338], [1324, -1348, -1341], [-1350, -1347, -1322, 1338, -1326, -1360], [1324, -1322, 1337, 1359], [-1345, -1355, 1338, 1352, -1333], [-1345, 1322, 1343, 1360], [1358, -1322], [1337, -1353, 1345, 1356], [-1337, -1354, 1346], [1325, 1330, 1340], [-1330, -1344, -1335, -1336], [1351, 1324, 1357, -1337, -1333], [-1350, -1357, 1351, -1349], [-1348, 1345, -1342, 1336], [-1329, -1324, -1346, -1341], [-1352, 1350, -1351, 1357], [-1348, -1331, 1323], [1331, 1338], [1336, 1326, -1328, 1322], [1360, 1359, -1347, 1349, -1336], [-1350, 1347, 1323], [1329, 1349], [-1345, 1327, -1359], [1337, -1325, 1339], [1356, -1337, 1325], [1346, 1343, 1350, -1321, -1359, -1353, -1329], [-1345, -1336, 1322], [-1333, 1338, 1357, 1334, -1337], [-1333, 1349, -1351, 1322, -1352, 1334, 1332, 1329, 1343, 1337, 1350], [1338, 1359, -1328, -1322, 1355, 1354], [-1351, 1325, -1343], [-1331, -1339, 1352, 1340, 1326, -1334], [1347, 1334, -1351], [-1340, 1329], [-1347, -1345, 1359], [1353, 1352, -1332, 1333, 1327, -1345, 1354, -1323], [-1329, -1340, -1335, -1322, -1326], [1351, 1349], [-1342, 1338, 1336, -1325], [1351, 1349, 1328, 1325, -1332, 1350, -1345, 1333], [-1354, 1350, 1321], [-1328, -1341, -1336], [1338, -1343, 1344, -1339, -1337], [1353, 1321, 1337], [-1322, -1358, 1360], [1326, -1325, -1329, 1345], [-1350, -1325, -1346, -1351, -1321], [1352, -1338, 1359, 1337, -1354], [1332, 1341, 1345, 1353, 1328, 1346], [-1327, -1345, 1341], [-1344, -1324, 1328, -1359, 1355, 1357], [-1321, -1349, 1333, 1323, -1337, 1340, -1339, 1322, -1346], [1335, 1345, 1336, 1347], [1351, -1357, 1326], [-1348, 1341, 1357], [-1331, 1347, 1337], [-1349, -1356, -1331, 1352], [-1325, 1327, -1323], [-1355, 1324, 1334, -1359, 1345, 1347], [-1328, -1333], [1321, 1353, 1323], [1325, -1346, -1324], [-1359, 1337, 1326, 1335, -1322, -1341], [-1347, -1331], [1341, -1330, 1353], [1322, 1323, -1344, -1340, -1328, -1329, -1341, 1335, 1324], [-1324, 1340, 1341, 1336, -1338, -1345, -1352, -1357], [-1342, 1360, 1354], [-1329, -1354, 1340], [-1356, -1338, 1339, 1335, 1341], [-1337, 1351, -1332, -1343, -1355, 1338, -1336], [1346, 1329, 1330], [-1330, 1336, 1329], [-1359, -1332, -1327, 1326], [1339, 1355, -1348, 1331, 1340, 1354, -1322], [1345, 1355, 1339, 1335, 1326], [-1325, -1342, -1322], [-1357, 1332, 1345], [1325, 1324, 1346], [1339, -1338, -1354, 1321], [1339, 1333, 1359], [1338, 1349, 1323, -1325, -1347], [1344, 1330, 1350, 1356, -1335], [-1324, 1336, -1351, 1350], [-1345, 1355, -1321], [-1352, -1329, -1335, 1351], [1360, 1348, -1355, -1342, -1354, -1321], [1337, -1347], [1334, -1351, 1360], [1353, 1355, -1340, -1342, 1322, -1351, -1356, -1346, 1358], [-1339, -1353, -1341], [-1341, 1357, 1338], [-1342, 1324, 1325, 1350, -1345, 1343], [1354, 1335], [1344, 1329, 1342, 1359, -1332, -1325, 1343, 1328, -1321, -1345, -1350, 1337], [-1358, -1327, -1337, -1349, 1347], [-1349, 1338, 1339, 1334, -1329, 1337], [1354, 1352, -1336], [-1333, -1358, -1334], [-1359, -1344, -1324, 1355], [1357, -1327, 1360, -1346, 1338], [1351, 1338, -1321], [-1355, -1321], [-1323, -1332, -1333, 1329], [-1333, -1337, -1328, 1326, -1336, -1348, 1347, 1324, 1345], [-1347, -1352, 1333], [-1343, 1327, 1359], [1357, 1332, 1323], [-1356, 1322, 1325], [1323, -1343, -1348], [1330, -1341], [-1324, -1338, 1335], [1358, 1350, -1344, 1328, -1359], [-1330, 1339, 1343, -1356, -1354], [1351, -1328, -1359, 1338, -1345, -1322], [1335, -1359, 1353], [-1336, -1335, -1353, -1322], [1328, 1360], [-1327, -1357, 1347, -1335, -1355, -1325, -1339, 1344, -1321, 1330, 1331, -1348, 1352, -1345], [-1350, -1336, -1325, 1332, 1337, 1358], [-1357, 1324, -1341, 1356, 1342], [-1334, -1327, -1332], [-1321, -1344, 1349], [1352, -1337, -1329, -1346, 1359, 1344], [-1338, -1351, -1331, 1343, 1330, 1348, -1344, 1327, 1340, -1349, -1350], [1358, 1326, 1357], [-1330, 1353, 1328, -1355, 1329, 1333], [-1338, 1333, -1331, -1355], [1338, -1328, 1353, 1349, 1347, 1330, 1360, 1355, -1334, 1342], [1335, 1331, -1332, -1354], [-1354, 1338], [1330, 1351, 1359], [1323, 1324, 1329], [-1328, 1326, 1338], [1334, -1336, 1356], [1324, 1347, -1352], [-1339, -1343, 1337, 1359], [1327, -1323, 1341], [1343, 1322, 1338], [-1354, -1326, -1358], [-1344, 1334, 1340, 1350, -1327], [1353, 1358, -1356, 1347], [-1341, -1332, -1321, -1339], [1324, 1331], [-1356, 1327, -1338], [1340, 1342, -1324], [1340, -1359, 1329, -1355], [1327, 1338, 1357], [1354, -1321, -1345, -1358], [-1335, -1334, -1356], [1347, 1338, 1345, -1333, -1336, -1356], [1339, 1334], [-1321, 1326, 1334, 1343, -1323, -1337, -1340, -1329], [1341, 1344, 1353, 1343, -1360], [-1326, 1336, 1356, -1355, -1348, 1349, -1353, -1330, -1327], [-1332, 1355, 1331], [1333, -1339, -1340], [1352, -1357], [-1329, 1350, 1347, -1351], [-1325, 1348, -1328], [1328, -1337, 1357, 1353, 1355, -1331, -1329, -1332, -1346], [-1329, -1350, 1337], [-1331, 1323, -1355], [1323, -1346, -1327, 1349], [1352, 1332, 1328], [1336, 1343, -1347, 1334, -1346], [1339, 1350], [-1336, 1333, -1323, -1349], [-1329, 1331, -1358, 1345], [1328, 1358, 1346, -1332], [1335, 1327, 1328, -1339, 1330], [1330, -1334, 1354, 1339, 1344, 1352, -1350, 1349, 1338], [1324, 1327, -1340, -1358, 1350, -1344, -1325], [-1325, -1352, 1331, -1351], [1338, -1351], [1356, -1331, 1336], [1344, 1337, -1356, -1330, 1345, 1351], [-1353, -1356, 1360, 1334, -1354], [1335, -1331, 1327], [1331, 1329, -1335, 1354, 1359, 1332, -1328, 1355], [1349, -1323, 1357, 1345], [-1356, 1336], [1356, 1349, -1346], [1348, 1325, -1335], [1332, 1346], [1383, -1389], [1371, 1377, -1376], [1365, 1384, -1388], [1392, 1391, -1361], [-1392, -1379, -1362, -1394, -1382, -1369, -1368], [-1395, -1399], [-1392, 1385, -1389, -1369, 1388, 1374], [-1375, 1377], [-1391, -1380], [-1361, -1374, -1379, 1386], [1385, 1389, -1362], [-1386, -1384, 1371], [-1374, -1364, 1392, 1391, -1368, 1373], [1377, -1395, -1389, 1382, -1364, 1369], [-1382, -1364, 1388, 1398], [-1396, 1363, 1398], [1363, -1397, -1389], [1377, -1378, 1362, 1391, -1390, 1373, -1364, 1363, -1370, 1381], [1398, 1397, -1371], [1373, 1400, 1399, 1391, 1384], [-1392, 1366, 1362], [-1375, -1400, 1391, 1380], [-1375, -1368, 1370], [-1391, -1390, -1380], [1399, 1383], [1400, -1392, 1366, 1394], [-1386, -1394], [1362, -1384, 1366, -1370], [-1363, 1385], [1366, 1397, 1385], [1373, -1361, -1366], [1367, 1387, 1381, 1368], [-1384, -1367, 1364, 1371], [1366, 1382], [-1391, 1386, 1394], [-1395, 1365], [-1399, -1368, 1377], [1371, 1388, -1387], [1386, 1373], [-1381, -1382, 1386], [1396, -1398, 1361, -1370, 1372], [1385, -1374, -1366, -1393], [-1371, 1392, -1363], [1377, 1394, 1366], [1397, -1384, 1390], [1391, -1399, -1379, -1390, 1367, -1375, -1384, -1383, 1392], [-1368, -1369, -1379], [1384, -1376, 1368], [-1389, -1377], [1396, -1376, 1370], [1389, -1383, 1380], [1397, -1367], [-1370, 1397, -1383, 1382], [-1389, 1388, -1394], [1400, -1378, 1386, -1363, 1381], [1386, 1373, 1368, 1367, -1365, 1389, -1395], [1370, -1393, -1369], [1369, 1399, 1389, -1376, -1391, -1400, 1385], [1379, 1398, 1378], [1393, 1382, 1391, -1383, -1365, 1386, 1372, 1373], [-1388, -1361, 1398], [-1384, 1398, 1378], [1371, -1377, -1399], [1374, 1392, 1398], [-1367, -1392, 1391, -1397], [1396, -1370, -1379, 1373], [1385, 1391, -1395, 1387], [-1398, 1374, 1361, -1395, -1378, -1394, -1384], [1371, 1393], [-1363, 1372, 1376], [-1369, 1394], [1394, 1389, -1368, -1388], [1377, -1389, -1387], [-1380, 1375, -1395, -1387], [1368, 1376, -1391], [1381, -1386, 1366], [-1396, -1388, -1397], [1392, -1389, 1369], [-1361, -1369, 1365, -1386], [1393, -1384, -1392, 1380, -1395, -1381, 1396], [-1381, -1365, -1366], [1378, 1384, -1361, 1391, 1397, 1400, 1375, -1372], [-1400, 1381, 1398, -1382, 1396, -1386], [-1368, -1366, -1399, 1371, -1378], [-1372, -1393, -1382], [-1393, -1371, 1390], [-1382, -1366], [-1400, -1388, -1390, 1386, -1394], [-1398, -1370, -1389], [-1361, -1388, -1398], [-1363, 1392, 1380], [1392, 1370, -1394, 1391, -1363], [1390, 1375, 1369, -1379], [1368, -1386, 1384, -1375], [-1363, 1361, 1396, -1376, -1390, 1383], [1392, -1387, -1400, 1388], [1365, 1380, 1391, 1385, 1389, 1369, -1379, -1378], [1365, 1378, 1372, 1364], [1375, 1361, 1378], [1392, 1375, 1362, -1367, 1363, -1400, 1396, -1391, 1397, -1387, -1390, 1364, -1386, -1361, -1377, 1389], [1371, 1382, -1362, 1376, 1374], [-1378, -1373, 1366], [-1389, -1382], [-1368, 1397, -1364, 1373, -1371, 1392, -1386, -1378, -1389, 1383], [1364, -1374, 1372, -1389, -1385, 1375], [1399, -1367, 1391, 1378], [1365, 1368, 1366, 1391, 1376], [-1373, 1382, 1374], [-1370, -1366, 1377, 1389], [-1392, 1380, 1396], [-1373, 1361, -1364, 1382, -1386, -1396], [-1390, 1365], [1387, 1363], [-1362, 1385, -1389, 1378, 1377, -1382, -1390, 1400, 1398], [1384, -1392, 1375, -1399, -1382], [-1364, -1387], [1374, -1396, -1376, 1391], [1375, -1373], [-1390, 1397, 1367, 1368], [1376, -1370, 1364], [1380, -1374, -1387], [-1362, -1390, -1374], [1382, -1368, 1377, -1387], [1368, 1373, 1395, -1385, 1375], [-1369, -1383, 1370, -1368, 1379, -1390, -1366], [1368, 1390, 1385], [-1365, 1376, 1377, 1395, 1368], [-1400, -1367, 1394], [1387, -1379], [-1374, 1397, 1379], [1385, 1373, 1379, -1382, 1361, -1391], [-1388, -1393], [1372, 1389, 1364], [-1381, -1386], [1370, 1382, -1383, -1380, -1397, 1373, 1398], [1388, 1391], [1385, -1378, 1365, 1382], [-1383, 1399], [-1368, 1379, -1388, 1370, 1395, 1392, 1362, -1369, -1371], [-1376, -1371, 1367], [-1391, 1400, 1375], [-1381, -1391], [-1364, 1390, 1396, -1378, 1393], [1366, -1377, -1389], [1382, -1378, -1368], [1374, 1392, 1390], [1400, -1375, 1379, 1398, 1386], [-1373, -1376, 1394], [1385, 1383, 1363], [1363, 1393, -1395, -1383, -1399, -1385, 1373, -1362, -1371, 1368, -1369], [-1363, -1368], [-1361, 1398, -1395, -1385], [1373, 1380, -1379], [-1387, -1394, 1386, -1383, -1376, -1370], [1396, 1397, -1376, -1398, 1373, -1371, -1380, -1389, -1391, -1390, -1365], [1391, 1383, -1390, -1361, -1385, 1378, 1368, -1370, -1389, 1365], [1392, -1366, 1380, -1376, -1394, 1387, -1393, 1362], [-1382, -1390, 1368, 1364], [-1397, 1375, 1362], [1377, -1380, 1378, -1393, -1368, -1398], [-1387, 1369, -1388, 1396, -1384], [1366, -1367, -1368, 1399, 1373, -1374, -1364], [1382, 1388, -1376], [-1366, 1378, 1364], [1397, -1382, 1364, 1373], [1396, -1388, -1393], [-1362, 1368, 1370, -1384, 1389, 1400], [1384, -1374, 1380], [-1389, 1367, -1377, -1390, -1364, -1392, -1365], [1399, 1382], [-1393, 1394, -1370], [1393, 1390, -1378, 1382], [-1371, 1387, 1397, -1370, -1369], [1372, -1379], [1376, -1399, 1391, 1381], [-1386, -1369, -1364, 1399, -1381, -1396, -1378, -1387, -1371], [-1364, -1392, -1400, -1387, 1378], [-1400, 1387, 1363], [1375, -1397, -1381, -1372, 1363], [1377, 1366], [-1373, 1390, -1386, -1370, -1385], [-1374, 1369, 1397, 1393, 1379], [-1375, -1399, 1384, -1385], [-1395, -1361, 1370], [-1391, -1383], [1367, -1376, -1381, 1373, 1393], [-1393, -1361, 1380, -1398, -1364], [1365, -1368], [-1372, -1399, 1394, -1376, 1388, -1375, 1391], [-1381, 1393, 1380], [-1391, -1390, 1369, -1365, 1395, 1363, 1383, 1367, 1380, -1386, 1387, -1400], [-1363, 1380, -1368], [-1371, -1395, 1367], [-1390, 1396, -1400, 1365], [-1364, 1369, 1365, 1366, 1386, -1379], [1375, 1395], [1423, -1429], [1411, 1417, -1416], [1405, 1424, -1428], [1432, 1431, -1401], [-1432, -1419, -1402, -1434, -1422, -1409, -1408], [-1435, -1439], [-1432, 1425, -1429, -1409, 1428, 1414], [-1415, 1417], [-1431, -1420], [-1401, -1414, -1419, 1426], [1425, 1429, -1402], [-1426, -1424, 1411], [-1414, -1404, 1432, 1431, -1408, 1413], [1417, -1435, -1429, 1422, -1404, 1409], [-1422, -1404, 1428, 1438], [-1436, 1403, 1438], [1403, -1437, -1429], [1417, -1418, 1402, 1431, -1430, 1413, -1404, 1403, -1410, 1421], [1438, 1437, -1411], [1413, 1440, 1439, 1431, 1424], [-1432, 1406, 1402], [-1415, -1440, 1431, 1420], [-1415, -1408, 1410], [-1431, -1430, -1420], [1439, 1423], [1440, -1432, 1406, 1434], [-1426, -1434], [1402, -1424, 1406, -1410], [-1403, 1425], [1406, 1437, 1425], [1413, -1401, -1406], [1407, 1427, 1421, 1408], [-1424, -1407, 1404, 1411], [1406, 1422], [-1431, 1426, 1434], [-1435, 1405], [-1439, -1408, 1417], [1411, 1428, -1427], [1426, 1413], [-1421, -1422, 1426], [1436, -1438, 1401, -1410, 1412], [1425, -1414, -1406, -1433], [-1411, 1432, -1403], [1417, 1434, 1406], [1437, -1424, 1430], [1431, -1439, -1419, -1430, 1407, -1415, -1424, -1423, 1432], [-1408, -1409, -1419], [1424, -1416, 1408], [-1429, -1417], [1436, -1416, 1410], [1429, -1423, 1420], [1437, -1407], [-1410, 1437, -1423, 1422], [-1429, 1428, -1434], [1440, -1418, 1426, -1403, 1421], [1426, 1413, 1408, 1407, -1405, 1429, -1435], [1410, -1433, -1409], [1409, 1439, 1429, -1416, -1431, -1440, 1425], [1419, 1438, 1418], [1433, 1422, 1431, -1423, -1405, 1426, 1412, 1413], [-1428, -1401, 1438], [-1424, 1438, 1418], [1411, -1417, -1439], [1414, 1432, 1438], [-1407, -1432, 1431, -1437], [1436, -1410, -1419, 1413], [1425, 1431, -1435, 1427], [-1438, 1414, 1401, -1435, -1418, -1434, -1424], [1411, 1433], [-1403, 1412, 1416], [-1409, 1434], [1434, 1429, -1408, -1428], [1417, -1429, -1427], [-1420, 1415, -1435, -1427], [1408, 1416, -1431], [1421, -1426, 1406], [-1436, -1428, -1437], [1432, -1429, 1409], [-1401, -1409, 1405, -1426], [1433, -1424, -1432, 1420, -1435, -1421, 1436], [-1421, -1405, -1406], [1418, 1424, -1401, 1431, 1437, 1440, 1415, -1412], [-1440, 1421, 1438, -1422, 1436, -1426], [-1408, -1406, -1439, 1411, -1418], [-1412, -1433, -1422], [-1433, -1411, 1430], [-1422, -1406], [-1440, -1428, -1430, 1426, -1434], [-1438, -1410, -1429], [-1401, -1428, -1438], [-1403, 1432, 1420], [1432, 1410, -1434, 1431, -1403], [1430, 1415, 1409, -1419], [1408, -1426, 1424, -1415], [-1403, 1401, 1436, -1416, -1430, 1423], [1432, -1427, -1440, 1428], [1405, 1420, 1431, 1425, 1429, 1409, -1419, -1418], [1405, 1418, 1412, 1404], [1415, 1401, 1418], [1432, 1415, 1402, -1407, 1403, -1440, 1436, -1431, 1437, -1427, -1430, 1404, -1426, -1401, -1417, 1429], [1411, 1422, -1402, 1416, 1414], [-1418, -1413, 1406], [-1429, -1422], [-1408, 1437, -1404, 1413, -1411, 1432, -1426, -1418, -1429, 1423], [1404, -1414, 1412, -1429, -1425, 1415], [1439, -1407, 1431, 1418], [1405, 1408, 1406, 1431, 1416], [-1413, 1422, 1414], [-1410, -1406, 1417, 1429], [-1432, 1420, 1436], [-1413, 1401, -1404, 1422, -1426, -1436], [-1430, 1405], [1427, 1403], [-1402, 1425, -1429, 1418, 1417, -1422, -1430, 1440, 1438], [1424, -1432, 1415, -1439, -1422], [-1404, -1427], [1414, -1436, -1416, 1431], [1415, -1413], [-1430, 1437, 1407, 1408], [1416, -1410, 1404], [1420, -1414, -1427], [-1402, -1430, -1414], [1422, -1408, 1417, -1427], [1408, 1413, 1435, -1425, 1415], [-1409, -1423, 1410, -1408, 1419, -1430, -1406], [1408, 1430, 1425], [-1405, 1416, 1417, 1435, 1408], [-1440, -1407, 1434], [1427, -1419], [-1414, 1437, 1419], [1425, 1413, 1419, -1422, 1401, -1431], [-1428, -1433], [1412, 1429, 1404], [-1421, -1426], [1410, 1422, -1423, -1420, -1437, 1413, 1438], [1428, 1431], [1425, -1418, 1405, 1422], [-1423, 1439], [-1408, 1419, -1428, 1410, 1435, 1432, 1402, -1409, -1411], [-1416, -1411, 1407], [-1431, 1440, 1415], [-1421, -1431], [-1404, 1430, 1436, -1418, 1433], [1406, -1417, -1429], [1422, -1418, -1408], [1414, 1432, 1430], [1440, -1415, 1419, 1438, 1426], [-1413, -1416, 1434], [1425, 1423, 1403], [1403, 1433, -1435, -1423, -1439, -1425, 1413, -1402, -1411, 1408, -1409], [-1403, -1408], [-1401, 1438, -1435, -1425], [1413, 1420, -1419], [-1427, -1434, 1426, -1423, -1416, -1410], [1436, 1437, -1416, -1438, 1413, -1411, -1420, -1429, -1431, -1430, -1405], [1431, 1423, -1430, -1401, -1425, 1418, 1408, -1410, -1429, 1405], [1432, -1406, 1420, -1416, -1434, 1427, -1433, 1402], [-1422, -1430, 1408, 1404], [-1437, 1415, 1402], [1417, -1420, 1418, -1433, -1408, -1438], [-1427, 1409, -1428, 1436, -1424], [1406, -1407, -1408, 1439, 1413, -1414, -1404], [1422, 1428, -1416], [-1406, 1418, 1404], [1437, -1422, 1404, 1413], [1436, -1428, -1433], [-1402, 1408, 1410, -1424, 1429, 1440], [1424, -1414, 1420], [-1429, 1407, -1417, -1430, -1404, -1432, -1405], [1439, 1422], [-1433, 1434, -1410], [1433, 1430, -1418, 1422], [-1411, 1427, 1437, -1410, -1409], [1412, -1419], [1416, -1439, 1431, 1421], [-1426, -1409, -1404, 1439, -1421, -1436, -1418, -1427, -1411], [-1404, -1432, -1440, -1427, 1418], [-1440, 1427, 1403], [1415, -1437, -1421, -1412, 1403], [1417, 1406], [-1413, 1430, -1426, -1410, -1425], [-1414, 1409, 1437, 1433, 1419], [-1415, -1439, 1424, -1425], [-1435, -1401, 1410], [-1431, -1423], [1407, -1416, -1421, 1413, 1433], [-1433, -1401, 1420, -1438, -1404], [1405, -1408], [-1412, -1439, 1434, -1416, 1428, -1415, 1431], [-1421, 1433, 1420], [-1431, -1430, 1409, -1405, 1435, 1403, 1423, 1407, 1420, -1426, 1427, -1440], [-1403, 1420, -1408], [-1411, -1435, 1407], [-1430, 1436, -1440, 1405], [-1404, 1409, 1405, 1406, 1426, -1419], [-1415, 1435]]\n"
     ]
    }
   ],
   "source": [
    "print(len(data[0].is_sat))\n",
    "print(data[0].clauses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'downstreamTask' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m vote_mean, iteration_votes_sorted, all_literal_embeddings, all_clause_embeddings \u001b[38;5;241m=\u001b[39m downstreamTask\u001b[38;5;241m.\u001b[39mforward(data[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(vote_mean)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(all_literal_embeddings[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'downstreamTask' is not defined"
     ]
    }
   ],
   "source": [
    "vote_mean, iteration_votes_sorted, all_literal_embeddings, all_clause_embeddings = downstreamTask.forward(data[0])\n",
    "\n",
    "print(vote_mean)\n",
    "\n",
    "print(all_literal_embeddings[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetName=\"NeuroSAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric import seed\n",
    "import utils\n",
    "import wandb\n",
    "import torch.nn.functional as fn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def trainExplainer (datasetName, opts, save_model=False, wandb_project=\"Explainer-NeuroSAT\", runSeed=None) :\n",
    "    if runSeed is not None: seed.seed_everything(runSeed)\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Check valid dataset name\n",
    "    configOG = utils.loadConfig(datasetName)\n",
    "    if configOG == -1:\n",
    "        return\n",
    "    \n",
    "    params = configOG['params']\n",
    "    graph_task = params['graph_task']\n",
    "    epochs = params['epochs']\n",
    "    t0 = params['t0']\n",
    "    tT = params['tT']\n",
    "    sampled_graphs = params['sampled_graphs']\n",
    "    coefficient_size_reg = params['coefficient_size_reg']\n",
    "    coefficient_entropy_reg = params['coefficient_entropy_reg']\n",
    "    coefficient_L2_reg = params['coefficient_L2_reg']\n",
    "    coefficient_consistency = params['coefficient_consistency']\n",
    "    num_explanation_edges = params['num_explanation_edges']\n",
    "    lr_mlp = params['lr_mlp']\n",
    "\n",
    "    wandb.init(project=wandb_project, config=params)\n",
    "\n",
    "    hidden_dim = 64 # Make loading possible\n",
    "    clip_grad_norm = 2 # Make loading possible\n",
    "    min_clip_value = -2\n",
    "    \n",
    "    \n",
    "    with open(opts['out_dir'], 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "    \n",
    "    # TODO: Split data into train and test\n",
    "    # TODO: !!! each data consist of multiple problems !!! -> Extract singular problems for calculating the loss!\n",
    "    dataset = data\n",
    "    \n",
    "    eval_problem = data[-1]\n",
    "    \n",
    "    # gt only needs to be calced once, not per epoch!\n",
    "    countClauses = 0\n",
    "    reals = []\n",
    "    gt_edges_per_problem = []\n",
    "    for current_batch_num in range(len(eval_problem.is_sat)):\n",
    "            # This can be repeated for each sub_problem\n",
    "            clauses_current_problem = eval_problem.get_clauses_for_problem(current_batch_num)\n",
    "\n",
    "            #print(f\"Length of sub_problem 0: {len(clauses_current_problem)}\")\n",
    "            # Sub problem 1 Contains literals 11-20, ...\n",
    "            #print(f\"Sub_problem 0 clauses: {clauses_current_problem}\")\n",
    "            \n",
    "            # Next part is only for calculating unsat_core -> Move to creation of data and save?\n",
    "            solver = Solver(name='m22')\n",
    "\n",
    "            offset = eval_problem.n_literals + 1\n",
    "\n",
    "            # Assumptions must be unique, therefore eval_problem.n_literals + 1\n",
    "            assumptions = [i + offset for i in range(len(clauses_current_problem))]\n",
    "\n",
    "            # Add the clauses with selector literals\n",
    "            for i, clause in enumerate(clauses_current_problem):\n",
    "                solver.add_clause(clause + [-assumptions[i]])  # Each clause gets a unique assumption\n",
    "                \n",
    "            # is_sat not needed right now as all problems should be unsat\n",
    "            is_sat = solver.solve(assumptions=assumptions)\n",
    "            \n",
    "            unsat_core = solver.get_core()\n",
    "\n",
    "            # Core contains clauses in reverse order, so we reverse it back\n",
    "            reversed_core = torch.tensor(unsat_core[::-1])\n",
    "            # Subtract offset from clauses in core to get original clauses\n",
    "            unsat_core_clauses = reversed_core - offset\n",
    "                \n",
    "            # Map back to original clauses\n",
    "            core_clause_literals = [clauses_current_problem[i] for i in range(len(clauses_current_problem)) if assumptions[i] in unsat_core]\n",
    "\n",
    "            solver.delete()\n",
    "            \n",
    "            # Calculate mask for current sub_problem in eval_problem\n",
    "            eval_batch_mask = utils.get_batch_mask(torch.tensor(eval_problem.batch_edges), batch_idx=current_batch_num, batch_size=opts['min_n'], n_variables=eval_problem.n_variables)\n",
    "            \n",
    "            \n",
    "            gt_mask = []\n",
    "            # TODO: THIS LOOKS WRONG\n",
    "            for i, idx in enumerate(unsat_core_clauses):\n",
    "                literals = core_clause_literals[i]\n",
    "                \n",
    "                # TODO: This only works if all sub_problems have the same amount of clauses!! WRONG!\n",
    "                # clause = len(clauses_current_problem)*current_batch_num + unsat_core_clauses[i]\n",
    "                clause = countClauses + unsat_core_clauses[i]\n",
    "                \n",
    "                # Sum of problemBatch.n_clauses_per_batch[] before current_batch_num? Or count while calculating gt for data?\n",
    "                #clause = problemBatch.n_clauses_per_batch[current_batch_num] + unsat_core_clauses[i]\n",
    "                \n",
    "                for value in literals:\n",
    "                    value = value -1 if value >= 1 else eval_problem.n_variables - (value + 1)\n",
    "                    gt_mask.append([value, clause])\n",
    "            \n",
    "            countClauses = countClauses + len(clauses_current_problem)\n",
    "                    \n",
    "            sub_problem_edges = eval_problem.batch_edges[eval_batch_mask]\n",
    "            \n",
    "            # TODO: VALIDATE THIS!!!\n",
    "            gt = torch.isin(torch.tensor(sub_problem_edges), torch.tensor(gt_mask))\n",
    "            # We only need the right column of the isin tensor\n",
    "            gt = gt[:,1].int()\n",
    "            \n",
    "            reals.append(gt.flatten().numpy())\n",
    "            gt_edges_per_problem.append(gt_mask)\n",
    "            #reals_unflattened.append(gt)\n",
    "    \n",
    "    allReals = np.concatenate(reals)  # Flatten the list of arrays\n",
    "    \n",
    "        \n",
    "\n",
    "    downstreamTask = NeuroSAT.NeuroSAT(opts=opts,device=device)\n",
    "    checkpoint = torch.load(f\"models/neurosat_sr10to40_ep1024_nr26_d128_last.pth.tar\", weights_only=True, map_location=device)\n",
    "    downstreamTask.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "    mlp = explainer_NeuroSAT.MLP(GraphTask=graph_task).to(device)\n",
    "    wandb.watch(mlp, log= \"all\", log_freq=2, log_graph=False)\n",
    "\n",
    "    mlp_optimizer = torch.optim.Adam(params = mlp.parameters(), lr = lr_mlp)\n",
    "\n",
    "    downstreamTask.eval()\n",
    "    for param in downstreamTask.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "\n",
    "    training_iterator = dataset\n",
    "    \n",
    "    for epoch in range(0, epochs) :\n",
    "        mlp.train()\n",
    "        mlp_optimizer.zero_grad()\n",
    "\n",
    "        temperature = t0*((tT/t0) ** ((epoch+1)/epochs))\n",
    "        \n",
    "        #sampledEdges = 0.0\n",
    "        #sumSampledEdges = 0.0\n",
    "        \n",
    "        #samplePredSum = 0\n",
    "\n",
    "        for index, content in enumerate(training_iterator):\n",
    "            # stop training before last batch, used for evaluation\n",
    "            if index == len(training_iterator)-2: break\n",
    "            node_to_predict = None\n",
    "            if graph_task: \n",
    "                # !! current_problem is really a batch of problems !!\n",
    "                current_problem = content\n",
    "\n",
    "            # MLP forward\n",
    "            # TODO: Implement embeddingCalculation for SAT\n",
    "            w_ij = mlp.forward(downstreamTask, current_problem, nodeToPred=node_to_predict)\n",
    "\n",
    "            sampleLoss = torch.FloatTensor([0]).to(device)\n",
    "            loss = torch.FloatTensor([0]).to(device)\n",
    "            \n",
    "            pOriginal, _, _, _ = downstreamTask.forward(current_problem)\n",
    "            pOriginal = fn.softmax(pOriginal, dim=0)\n",
    "            \n",
    "            for k in range(0, sampled_graphs):\n",
    "                edge_ij = mlp.sampleGraph(w_ij, temperature)\n",
    "                \n",
    "                #sampledEdges += torch.sum(edge_ij)\n",
    "            \n",
    "                # TODO: softmax needed for loss, beacuse negative values do not work witg log! Need for normalization?\n",
    "                pSample, _, _, _ = downstreamTask.forward(current_problem, edge_weights=edge_ij)\n",
    "                pSample = fn.softmax(pSample, dim=0)\n",
    "\n",
    "                #samplePredSum += torch.sum(torch.argmax(pSample, dim=1))\n",
    "                \n",
    "                if graph_task:\n",
    "                    for sub_problem_idx in range(len(current_problem.is_sat)):\n",
    "                        # batch_mask needed to differentiate sub_problems in batch of problems for loss\n",
    "                        # batch_edges cannot simply be divided since sub_problems have different number of edges/clauses\n",
    "                        # IMPORTANT: batch_size and n variables dependant on data!!\n",
    "                        batch_mask = utils.get_batch_mask(torch.tensor(current_problem.batch_edges), sub_problem_idx, opts['min_n'], current_problem.n_variables)\n",
    "                        currLoss = mlp.loss(pOriginal[sub_problem_idx], pSample[sub_problem_idx], edge_ij[batch_mask], current_problem.batch_edges[batch_mask], coefficient_size_reg, coefficient_entropy_reg, coefficient_consistency)\n",
    "                        sampleLoss.add_(currLoss)\n",
    "                    \n",
    "\n",
    "            loss += sampleLoss / sampled_graphs\n",
    "            \n",
    "            #sumSampledEdges += sampledEdges / sampled_graphs\n",
    "\n",
    "        #print(samplePredSum)\n",
    "        \n",
    "        loss = loss / len(training_iterator)\n",
    "        loss.backward()\n",
    "        \n",
    "        mlp_optimizer.step()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(mlp.parameters(), max_norm=clip_grad_norm)\n",
    "        \n",
    "        \"\"\"for param in mlp.parameters():\n",
    "            if param.grad is not None:\n",
    "                param.grad.data = torch.max(param.grad.data, min_clip_value * torch.ones_like(param.grad.data))\"\"\"\n",
    "\n",
    "        mlp.eval()\n",
    "        \n",
    "        \"\"\"if graph_task:\n",
    "            #TODO: Evaluation for SAT! Needs gt\n",
    "            meanAuc = evaluation.evaluateNeuroSATAUC(mlp, downstreamTask, data)\"\"\"\n",
    "            \n",
    "        # Get one sub problem for evaluation:\n",
    "        preds = []\n",
    "        highest_auc = 0\n",
    "        lowest_auc = 1\n",
    "        \n",
    "        # FIXME: THE EMBEDDINGS CALCULATED IN THIS FORWARD PATH ARE QUITE SMALL, BUT FIXED (GOOD)\n",
    "        \n",
    "        # Calculate weights and prediction for all sub_problems in eval_problem\n",
    "        w_ij_eval = mlp.forward(downstreamTask, eval_problem, nodeToPred=node_to_predict)\n",
    "        edge_ij_eval = mlp.sampleGraph(w_ij_eval, temperature).detach()\n",
    "        #pSample_eval, _, _, _ = downstreamTask.forward(eval_problem, edge_weights=edge_ij_eval)\n",
    "        #pOriginal_eval, _, _, _ = downstreamTask.forward(eval_problem)\n",
    "        \n",
    "        for current_batch_num in range(len(eval_problem.is_sat)):\n",
    "            # This can be repeated for each sub_problem\n",
    "            clauses_current_problem = eval_problem.get_clauses_for_problem(current_batch_num)\n",
    "\n",
    "            #print(f\"Length of sub_problem 0: {len(clauses_current_problem)}\")\n",
    "            # Sub problem 1 Contains literals 11-20, ...\n",
    "            #print(f\"Sub_problem 0 clauses: {clauses_current_problem}\")\n",
    "            \n",
    "            # Calculate mask for current sub_problem in eval_problem\n",
    "            eval_batch_mask = utils.get_batch_mask(torch.tensor(eval_problem.batch_edges), batch_idx=current_batch_num, batch_size=opts['min_n'], n_variables=eval_problem.n_variables)\n",
    "            \n",
    "            # Edge probabilites for current sub problem in eval_problems\n",
    "            edge_ij_eval_masked = edge_ij_eval[eval_batch_mask]\n",
    "            \n",
    "            sub_problem_edges = eval_problem.batch_edges[eval_batch_mask]\n",
    "            \n",
    "            motif_size = len(gt_mask)\n",
    "            \n",
    "            preds.append(edge_ij_eval_masked.cpu().flatten().numpy())\n",
    "            \n",
    "            if epoch == epochs-1:\n",
    "                curr_roc_auc = roc_auc_score(reals[current_batch_num], preds[-1])\n",
    "                if curr_roc_auc > highest_auc:\n",
    "                    highest_auc = curr_roc_auc\n",
    "                    highest_edge_ij = edge_ij_eval_masked\n",
    "                    highest_gt = reals[current_batch_num]\n",
    "                    sub_problem_edges_highest = sub_problem_edges\n",
    "                    highest_index = current_batch_num\n",
    "                if curr_roc_auc < lowest_auc:\n",
    "                    lowest_auc = curr_roc_auc\n",
    "                    lowest_edge_ij = edge_ij_eval_masked\n",
    "                    lowest_gt = reals[current_batch_num]\n",
    "                    sub_problem_edges_lowest = sub_problem_edges\n",
    "                    lowest_index = current_batch_num\n",
    "                \n",
    "        \n",
    "        \n",
    "        all_preds = np.concatenate(preds)  # Flatten the list of arrays\n",
    "    \n",
    "        roc_auc = roc_auc_score(allReals, all_preds)\n",
    "        \n",
    "        weights_eval_masked = w_ij_eval[eval_batch_mask]\n",
    "        print(f\"Edge weights for last sub_problem in last problem batch: {weights_eval_masked}\")\n",
    "        \n",
    "        print(f\"Edge probabilites for last sub_problem in last problem batch: {edge_ij_eval_masked}\")\n",
    "        \n",
    "        #print(f\"Prediction for sub_problem: {pOriginal_eval[0]}\")\n",
    "        #print(f\"Prediction for sampled sub_problem: {pSample_eval[0]}\")\n",
    "        \n",
    "        print(f\"roc_auc score: {roc_auc}\")\n",
    "    \n",
    "\n",
    "        # TODO: VISUALIZE TOPK!\n",
    "        # print sub_problem 0 with calculated edge weights\n",
    "        if (epoch+1) % 5 == 0:\n",
    "            pos = utils.visualize_edge_index_interactive(sub_problem_edges, edge_ij_eval_masked, f\"results/replication/seed{runSeed}_vis_edge_ij_{epoch+1}\", topK=len(gt_edges_per_problem[-1]))\n",
    "        \n",
    "        \n",
    "        #sumSampledEdges = sumSampledEdges / len(training_iterator)\n",
    "        #, \"val/mean_AUC\": meanAuc\n",
    "        wandb.log({\"train/Loss\": loss, \"val/temperature\": temperature, \"val/roc_auc\": roc_auc})\n",
    "\n",
    "        \"\"\"for name, param in mlp.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                print(f\"{name}: {param.grad}\")\"\"\"\n",
    "        \n",
    "    # print sub_problem 0 with calculated edge weights\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges, edge_ij_eval_masked, f\"results/replication/seed{runSeed}_vis_edge_ij_{epoch+1}\", topK=len(gt_edges_per_problem[-1]))\n",
    "    # print sub_problem 0 with gt\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges, reals[-1], f\"results/replication/seed{runSeed}_vis_gt\", pos)\n",
    "    \n",
    "    # TODO: Show difference between topK edges and gt edges -> logical and on topK and gt, visualize\n",
    "    \n",
    "    sorted_weights, topk_indices_highest = torch.topk(highest_edge_ij, len(gt_edges_per_problem[highest_index]))\n",
    "    mask_topK_highest = torch.zeros_like(highest_edge_ij, dtype=torch.float32)\n",
    "    mask_topK_highest[topk_indices_highest] = 1\n",
    "    \n",
    "    common_edges_highest = np.logical_and(highest_gt, mask_topK_highest.flatten().numpy())\n",
    "    print(f\"Highest individual auc: {highest_auc}\")\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_highest, highest_edge_ij, f\"results/replication/seed{runSeed}_highestAUC\", topK=len(gt_edges_per_problem[highest_index]))\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_highest, highest_gt, f\"results/replication/seed{runSeed}_highestAUC_gt\")\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_highest, common_edges_highest, f\"results/replication/seed{runSeed}_highestAUC_commonEdges\")\n",
    "    \n",
    "    # TO EVALUATE SATISFIABILITY OF EXPLANATION:\n",
    "    print(f\"Highest AUC example masked edges: {sub_problem_edges_highest[mask_topK_highest.bool()]}\")\n",
    "    \n",
    "    \n",
    "    sorted_weights, topk_indices_lowest = torch.topk(lowest_edge_ij, len(gt_edges_per_problem[lowest_index]))\n",
    "    mask_topK_lowest = torch.zeros_like(lowest_edge_ij, dtype=torch.float32)\n",
    "    mask_topK_lowest[topk_indices_lowest] = 1\n",
    "    \n",
    "    common_edges_lowest = np.logical_and(lowest_gt, mask_topK_lowest.flatten().numpy())\n",
    "    print(f\"Lowest individual auc: {lowest_auc}\")\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_lowest, lowest_edge_ij, f\"results/replication/seed{runSeed}_lowestAUC\", topK=len(gt_edges_per_problem[lowest_index]))\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_lowest, lowest_gt, f\"results/replication/seed{runSeed}_lowestAUC_gt\")\n",
    "    pos = utils.visualize_edge_index_interactive(sub_problem_edges_lowest, common_edges_lowest, f\"results/replication/seed{runSeed}_lowestAUC_commonEdges\")\n",
    "    \n",
    "        \n",
    "    #if save_model == \"True\":\n",
    "    #    torch.save(mlp.state_dict(), f\"models/explainer_{dataset}_{meanAuc}_{wandb.run.name}\")\n",
    "\n",
    "    wandb.finish()\n",
    "    \n",
    "    return mlp, downstreamTask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtristan-schulz2001\u001b[0m (\u001b[33mtristan-schulz2001-tu-dortmund\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\trist\\Git_repos\\BT-ML-PGESAT\\code\\PGExplainer NeuroSAT\\wandb\\run-20250407_195935-a98prypb</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR/runs/a98prypb' target=\"_blank\">absurd-frog-1</a></strong> to <a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR' target=\"_blank\">https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR/runs/a98prypb' target=\"_blank\">https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR/runs/a98prypb</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 1, Loss: 3.2832558155059814\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-0.4973, -0.5713, -0.4564, -0.2737, -0.6204, -0.7220, -0.2868, -0.3077,\n",
      "        -0.3309, -0.7305, -0.3926, -0.5093, -0.2476, -0.1903, -0.4388, -0.4245,\n",
      "        -0.3969, -0.6623, -0.2749, -0.3379, -0.3019, -0.5271, -0.5577, -0.6581,\n",
      "        -0.5361, -0.2593, -0.3520, -0.3576, -0.6619, -0.5547, -0.3339, -0.3722,\n",
      "        -0.3000, -0.4140, -0.2353, -0.2757, -0.7112, -0.3016, -0.6002, -0.3064,\n",
      "        -0.2776, -0.3068, -0.6697, -0.3859, -0.4220, -0.5506, -0.2283, -0.3198,\n",
      "        -0.5775, -0.3405, -0.2623, -0.6054, -0.3315, -0.2936, -0.5841, -0.5166,\n",
      "        -0.3524, -0.5323, -0.3006, -0.3436, -0.3857, -0.3993, -0.5196, -0.5339,\n",
      "        -0.2835, -0.3464, -0.2662, -0.3807, -0.6915, -0.2953, -0.5995, -0.3410,\n",
      "        -0.3203, -0.4747, -0.4172, -0.5551, -0.2569, -0.3362, -0.3833, -0.5173,\n",
      "        -0.6454, -0.2968, -0.6514, -0.2485, -0.6658, -0.3585, -0.3323, -0.5175,\n",
      "        -0.7624, -0.7077, -0.2396, -0.3005, -0.3861, -0.2786, -0.2476, -0.2742,\n",
      "        -0.5980, -0.3247, -0.2076, -0.5508, -0.2696, -0.5914, -0.5165, -0.2398,\n",
      "        -0.2382, -0.3343, -0.6835, -0.2732, -0.3441, -0.5183, -0.2872, -0.2811,\n",
      "        -0.5177, -0.2481, -0.3220, -0.5643, -0.5513, -0.4140, -0.6916, -0.3364,\n",
      "        -0.2904, -0.3001, -0.4032, -0.4203, -0.6473, -0.3396, -0.5530, -0.7217,\n",
      "        -0.5868, -0.3525, -0.2793, -0.3934, -0.5400, -0.3509, -0.2573, -0.2724,\n",
      "        -0.3308, -0.2664, -0.5986, -0.2773, -0.5299, -0.3604, -0.5503, -0.4744,\n",
      "        -0.5965, -0.3137, -0.7280, -0.2969, -0.3424, -0.3304, -0.7109, -0.5439,\n",
      "        -0.3910, -0.6599, -0.3825, -0.3894, -0.3387, -0.6919, -0.4153, -0.5382,\n",
      "        -0.4531, -0.5806, -0.5432, -0.2210, -0.2779, -0.3565, -0.6345, -0.4187,\n",
      "        -0.3458, -0.6538, -0.3431, -0.3141, -0.3514, -0.6217, -0.6274, -0.3563,\n",
      "        -0.3168, -0.5600, -0.3040, -0.2697, -0.5708, -0.3562, -0.3176, -0.3732,\n",
      "        -0.2698, -0.3352, -0.3658, -0.4042, -0.6952, -0.7323, -0.3210, -0.2778,\n",
      "        -0.5958, -0.3380, -0.2729, -0.1846, -0.2333, -0.2524, -0.4372, -0.6248,\n",
      "        -0.5636, -0.4766, -0.2288, -0.5449, -0.3555, -0.3787, -0.6730, -0.3954,\n",
      "        -0.5177, -0.2481, -0.5302, -0.3472, -0.6676, -0.2899, -0.2749, -0.3539],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.3782, 0.3609, 0.3878, 0.4320, 0.3497, 0.3269, 0.4288, 0.4237, 0.4180,\n",
      "        0.3251, 0.4031, 0.3754, 0.4384, 0.4526, 0.3920, 0.3955, 0.4021, 0.3402,\n",
      "        0.4317, 0.4163, 0.4251, 0.3712, 0.3641, 0.3412, 0.3691, 0.4355, 0.4129,\n",
      "        0.4115, 0.3403, 0.3648, 0.4173, 0.4080, 0.4256, 0.3980, 0.4414, 0.4315,\n",
      "        0.3293, 0.4252, 0.3543, 0.4240, 0.4310, 0.4239, 0.3386, 0.4047, 0.3960,\n",
      "        0.3657, 0.4432, 0.4207, 0.3595, 0.4157, 0.4348, 0.3531, 0.4179, 0.4271,\n",
      "        0.3580, 0.3737, 0.4128, 0.3700, 0.4254, 0.4149, 0.4047, 0.4015, 0.3729,\n",
      "        0.3696, 0.4296, 0.4142, 0.4338, 0.4060, 0.3337, 0.4267, 0.3545, 0.4156,\n",
      "        0.4206, 0.3835, 0.3972, 0.3647, 0.4361, 0.4167, 0.4053, 0.3735, 0.3440,\n",
      "        0.4263, 0.3427, 0.4382, 0.3394, 0.4113, 0.4177, 0.3734, 0.3181, 0.3301,\n",
      "        0.4404, 0.4254, 0.4047, 0.4308, 0.4384, 0.4319, 0.3548, 0.4195, 0.4483,\n",
      "        0.3657, 0.4330, 0.3563, 0.3737, 0.4403, 0.4407, 0.4172, 0.3355, 0.4321,\n",
      "        0.4148, 0.3732, 0.4287, 0.4302, 0.3734, 0.4383, 0.4202, 0.3625, 0.3656,\n",
      "        0.3979, 0.3337, 0.4167, 0.4279, 0.4255, 0.4006, 0.3964, 0.3436, 0.4159,\n",
      "        0.3652, 0.3270, 0.3574, 0.4128, 0.4306, 0.4029, 0.3682, 0.4132, 0.4360,\n",
      "        0.4323, 0.4180, 0.4338, 0.3547, 0.4311, 0.3705, 0.4109, 0.3658, 0.3836,\n",
      "        0.3551, 0.4222, 0.3256, 0.4263, 0.4152, 0.4181, 0.3294, 0.3673, 0.4035,\n",
      "        0.3408, 0.4055, 0.4039, 0.4161, 0.3336, 0.3976, 0.3686, 0.3886, 0.3588,\n",
      "        0.3674, 0.4450, 0.4310, 0.4118, 0.3465, 0.3968, 0.4144, 0.3421, 0.4150,\n",
      "        0.4221, 0.4131, 0.3494, 0.3481, 0.4119, 0.4215, 0.3635, 0.4246, 0.4330,\n",
      "        0.3611, 0.4119, 0.4213, 0.4078, 0.4329, 0.4170, 0.4096, 0.4003, 0.3329,\n",
      "        0.3247, 0.4204, 0.4310, 0.3553, 0.4163, 0.4322, 0.4540, 0.4419, 0.4372,\n",
      "        0.3924, 0.3487, 0.3627, 0.3831, 0.4430, 0.3670, 0.4121, 0.4064, 0.3378,\n",
      "        0.4024, 0.3734, 0.4383, 0.3705, 0.4141, 0.3390, 0.4280, 0.4317, 0.4124])\n",
      "roc_auc score: 0.5378934051206211\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 2, Loss: 3.2315661907196045\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-0.6546, -0.7655, -0.6355, -0.4372, -0.8496, -0.9138, -0.4822, -0.5323,\n",
      "        -0.5344, -0.9982, -0.6250, -0.6632, -0.4181, -0.3470, -0.5988, -0.6181,\n",
      "        -0.6021, -0.8967, -0.3774, -0.5400, -0.4445, -0.7503, -0.7607, -0.9008,\n",
      "        -0.7730, -0.3904, -0.5988, -0.5797, -0.8344, -0.7747, -0.6236, -0.6214,\n",
      "        -0.5586, -0.6636, -0.4376, -0.4909, -0.9291, -0.4832, -0.7808, -0.5120,\n",
      "        -0.4581, -0.5164, -0.8673, -0.6029, -0.5712, -0.6777, -0.3683, -0.4737,\n",
      "        -0.7641, -0.4728, -0.3628, -0.8242, -0.5666, -0.4939, -0.7459, -0.7097,\n",
      "        -0.5096, -0.7080, -0.4743, -0.4918, -0.6325, -0.5843, -0.7108, -0.7098,\n",
      "        -0.4518, -0.5835, -0.4365, -0.5839, -0.8970, -0.4714, -0.7489, -0.5761,\n",
      "        -0.4501, -0.6343, -0.5588, -0.7707, -0.3792, -0.5413, -0.5936, -0.7501,\n",
      "        -0.8235, -0.4549, -0.8899, -0.3855, -0.8667, -0.5414, -0.5675, -0.6868,\n",
      "        -1.0501, -0.9410, -0.4288, -0.5383, -0.6251, -0.4923, -0.4345, -0.4557,\n",
      "        -0.7779, -0.5636, -0.3565, -0.7479, -0.4408, -0.7618, -0.7590, -0.3894,\n",
      "        -0.4003, -0.5825, -0.8604, -0.4815, -0.5605, -0.6728, -0.4516, -0.4356,\n",
      "        -0.6841, -0.4194, -0.4693, -0.7365, -0.7291, -0.5942, -0.8990, -0.5190,\n",
      "        -0.4661, -0.4775, -0.6185, -0.6249, -0.8324, -0.5032, -0.7214, -0.9456,\n",
      "        -0.8127, -0.5414, -0.4083, -0.6121, -0.7167, -0.5279, -0.3887, -0.3747,\n",
      "        -0.5033, -0.4442, -0.7678, -0.4310, -0.7197, -0.5174, -0.7185, -0.6789,\n",
      "        -0.8034, -0.5464, -0.9709, -0.4764, -0.5095, -0.5308, -0.8882, -0.7308,\n",
      "        -0.5774, -0.8573, -0.6325, -0.5972, -0.4935, -0.9244, -0.5627, -0.6669,\n",
      "        -0.6313, -0.7732, -0.6734, -0.3677, -0.4661, -0.5963, -0.8709, -0.6568,\n",
      "        -0.5574, -0.8757, -0.5322, -0.5033, -0.5840, -0.8270, -0.8252, -0.5852,\n",
      "        -0.4946, -0.7298, -0.4344, -0.4449, -0.7755, -0.6194, -0.5212, -0.6002,\n",
      "        -0.4193, -0.5456, -0.6444, -0.6369, -0.9194, -1.0019, -0.5500, -0.4442,\n",
      "        -0.8510, -0.5154, -0.4490, -0.3623, -0.4370, -0.3939, -0.6405, -0.7685,\n",
      "        -0.7736, -0.6691, -0.3332, -0.7654, -0.5619, -0.6234, -0.9032, -0.5983,\n",
      "        -0.6841, -0.4194, -0.7265, -0.5137, -0.8623, -0.4604, -0.3849, -0.5896],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.3420, 0.3175, 0.3463, 0.3924, 0.2995, 0.2862, 0.3817, 0.3700, 0.3695,\n",
      "        0.2693, 0.3486, 0.3400, 0.3970, 0.4141, 0.3546, 0.3502, 0.3539, 0.2897,\n",
      "        0.4067, 0.3682, 0.3907, 0.3208, 0.3185, 0.2889, 0.3158, 0.4036, 0.3546,\n",
      "        0.3590, 0.3027, 0.3155, 0.3490, 0.3495, 0.3639, 0.3399, 0.3923, 0.3797,\n",
      "        0.2831, 0.3815, 0.3141, 0.3747, 0.3874, 0.3737, 0.2958, 0.3537, 0.3610,\n",
      "        0.3368, 0.4090, 0.3837, 0.3178, 0.3840, 0.4103, 0.3049, 0.3620, 0.3790,\n",
      "        0.3217, 0.3297, 0.3753, 0.3300, 0.3836, 0.3795, 0.3469, 0.3579, 0.3294,\n",
      "        0.3296, 0.3889, 0.3581, 0.3926, 0.3580, 0.2897, 0.3843, 0.3211, 0.3598,\n",
      "        0.3893, 0.3465, 0.3638, 0.3163, 0.4063, 0.3679, 0.3558, 0.3208, 0.3050,\n",
      "        0.3882, 0.2911, 0.4048, 0.2959, 0.3679, 0.3618, 0.3347, 0.2592, 0.2807,\n",
      "        0.3944, 0.3686, 0.3486, 0.3794, 0.3930, 0.3880, 0.3148, 0.3627, 0.4118,\n",
      "        0.3213, 0.3916, 0.3183, 0.3189, 0.4039, 0.4012, 0.3584, 0.2973, 0.3819,\n",
      "        0.3634, 0.3379, 0.3890, 0.3928, 0.3354, 0.3967, 0.3848, 0.3238, 0.3254,\n",
      "        0.3557, 0.2893, 0.3731, 0.3856, 0.3828, 0.3501, 0.3487, 0.3031, 0.3768,\n",
      "        0.3271, 0.2798, 0.3073, 0.3679, 0.3993, 0.3516, 0.3281, 0.3710, 0.4040,\n",
      "        0.4074, 0.3768, 0.3907, 0.3170, 0.3939, 0.3275, 0.3735, 0.3277, 0.3365,\n",
      "        0.3093, 0.3667, 0.2747, 0.3831, 0.3753, 0.3703, 0.2915, 0.3250, 0.3595,\n",
      "        0.2979, 0.3470, 0.3550, 0.3791, 0.2841, 0.3629, 0.3392, 0.3472, 0.3158,\n",
      "        0.3377, 0.4091, 0.3855, 0.3552, 0.2951, 0.3415, 0.3641, 0.2941, 0.3700,\n",
      "        0.3768, 0.3580, 0.3043, 0.3047, 0.3577, 0.3788, 0.3252, 0.3931, 0.3906,\n",
      "        0.3153, 0.3499, 0.3726, 0.3543, 0.3967, 0.3669, 0.3442, 0.3459, 0.2851,\n",
      "        0.2686, 0.3659, 0.3907, 0.2992, 0.3739, 0.3896, 0.4104, 0.3924, 0.4028,\n",
      "        0.3451, 0.3168, 0.3157, 0.3387, 0.4175, 0.3175, 0.3631, 0.3490, 0.2884,\n",
      "        0.3547, 0.3354, 0.3967, 0.3260, 0.3743, 0.2969, 0.3869, 0.4049, 0.3567])\n",
      "roc_auc score: 0.5946839279662323\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 3, Loss: 3.173893451690674\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-0.8214, -0.9461, -0.8132, -0.6094, -1.0870, -1.1126, -0.6847, -0.7748,\n",
      "        -0.7427, -1.2703, -0.8672, -0.8152, -0.5951, -0.5004, -0.7502, -0.8017,\n",
      "        -0.8032, -1.1178, -0.4819, -0.7467, -0.6004, -0.9662, -0.9738, -1.1572,\n",
      "        -1.0018, -0.5308, -0.8549, -0.7988, -1.0202, -1.0022, -0.9097, -0.8726,\n",
      "        -0.8210, -0.9263, -0.6419, -0.7040, -1.1494, -0.6683, -0.9512, -0.7288,\n",
      "        -0.6420, -0.7255, -1.0557, -0.8103, -0.7138, -0.8116, -0.5191, -0.6276,\n",
      "        -0.9362, -0.6094, -0.4720, -1.0374, -0.7955, -0.6993, -0.9125, -0.8928,\n",
      "        -0.6638, -0.8764, -0.6480, -0.6448, -0.8704, -0.7628, -0.8954, -0.8786,\n",
      "        -0.6203, -0.8194, -0.6180, -0.7788, -1.1168, -0.6528, -0.8967, -0.8025,\n",
      "        -0.5847, -0.7995, -0.6968, -0.9845, -0.5065, -0.7473, -0.8042, -0.9759,\n",
      "        -1.0045, -0.6158, -1.1232, -0.5239, -1.0605, -0.7322, -0.7880, -0.8553,\n",
      "        -1.3223, -1.1720, -0.6240, -0.7725, -0.8713, -0.7041, -0.6249, -0.6359,\n",
      "        -0.9366, -0.8017, -0.5102, -0.9362, -0.6217, -0.9300, -0.9970, -0.5380,\n",
      "        -0.5635, -0.8469, -1.0431, -0.6976, -0.7709, -0.8228, -0.6291, -0.5937,\n",
      "        -0.8469, -0.5794, -0.6127, -0.9022, -0.9101, -0.7681, -1.1151, -0.7001,\n",
      "        -0.6497, -0.6555, -0.8257, -0.8307, -1.0237, -0.6589, -0.8836, -1.1790,\n",
      "        -1.0344, -0.7330, -0.5341, -0.8269, -0.8879, -0.7107, -0.5201, -0.4769,\n",
      "        -0.6724, -0.6227, -0.9323, -0.5856, -0.9039, -0.6700, -0.8772, -0.8789,\n",
      "        -1.0097, -0.7755, -1.2173, -0.6627, -0.6912, -0.7364, -1.0730, -0.9055,\n",
      "        -0.7626, -1.0464, -0.8773, -0.8092, -0.6522, -1.1560, -0.7041, -0.8011,\n",
      "        -0.8109, -0.9610, -0.7990, -0.5220, -0.6508, -0.8348, -1.1127, -0.8866,\n",
      "        -0.7699, -1.1043, -0.7261, -0.7023, -0.8006, -1.0186, -1.0220, -0.8013,\n",
      "        -0.6884, -0.8990, -0.5658, -0.6083, -0.9622, -0.8769, -0.7317, -0.8350,\n",
      "        -0.5754, -0.7547, -0.9241, -0.8747, -1.1332, -1.2844, -0.7745, -0.6143,\n",
      "        -1.1032, -0.6816, -0.6298, -0.5428, -0.6358, -0.5341, -0.8519, -0.9213,\n",
      "        -0.9967, -0.8657, -0.4357, -0.9813, -0.7674, -0.8582, -1.1297, -0.8065,\n",
      "        -0.8469, -0.5794, -0.9183, -0.6765, -1.0474, -0.6314, -0.4962, -0.8251],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.3055, 0.2797, 0.3072, 0.3522, 0.2522, 0.2474, 0.3352, 0.3154, 0.3224,\n",
      "        0.2192, 0.2958, 0.3068, 0.3555, 0.3774, 0.3208, 0.3097, 0.3093, 0.2464,\n",
      "        0.3818, 0.3216, 0.3543, 0.2756, 0.2741, 0.2392, 0.2686, 0.3703, 0.2984,\n",
      "        0.3103, 0.2650, 0.2685, 0.2871, 0.2947, 0.3055, 0.2837, 0.3448, 0.3309,\n",
      "        0.2406, 0.3389, 0.2786, 0.3255, 0.3448, 0.3262, 0.2581, 0.3078, 0.3288,\n",
      "        0.3075, 0.3731, 0.3481, 0.2817, 0.3522, 0.3841, 0.2617, 0.3110, 0.3320,\n",
      "        0.2865, 0.2905, 0.3399, 0.2939, 0.3434, 0.3442, 0.2952, 0.3180, 0.2900,\n",
      "        0.2935, 0.3497, 0.3059, 0.3502, 0.3146, 0.2466, 0.3424, 0.2897, 0.3095,\n",
      "        0.3579, 0.3101, 0.3325, 0.2720, 0.3760, 0.3214, 0.3091, 0.2737, 0.2680,\n",
      "        0.3507, 0.2454, 0.3719, 0.2572, 0.3247, 0.3126, 0.2983, 0.2104, 0.2365,\n",
      "        0.3489, 0.3159, 0.2950, 0.3309, 0.3487, 0.3462, 0.2816, 0.3097, 0.3752,\n",
      "        0.2817, 0.3494, 0.2829, 0.2695, 0.3687, 0.3627, 0.3001, 0.2605, 0.3323,\n",
      "        0.3163, 0.3052, 0.3477, 0.3558, 0.3001, 0.3591, 0.3515, 0.2886, 0.2870,\n",
      "        0.3169, 0.2469, 0.3318, 0.3431, 0.3417, 0.3046, 0.3035, 0.2643, 0.3410,\n",
      "        0.2924, 0.2352, 0.2622, 0.3245, 0.3696, 0.3043, 0.2915, 0.3294, 0.3728,\n",
      "        0.3830, 0.3380, 0.3492, 0.2825, 0.3576, 0.2883, 0.3385, 0.2937, 0.2934,\n",
      "        0.2670, 0.3153, 0.2284, 0.3401, 0.3338, 0.3238, 0.2548, 0.2879, 0.3181,\n",
      "        0.2599, 0.2937, 0.3081, 0.3425, 0.2394, 0.3309, 0.3098, 0.3077, 0.2767,\n",
      "        0.3102, 0.3724, 0.3428, 0.3026, 0.2474, 0.2918, 0.3165, 0.2489, 0.3261,\n",
      "        0.3313, 0.3099, 0.2653, 0.2646, 0.3098, 0.3344, 0.2893, 0.3622, 0.3525,\n",
      "        0.2764, 0.2938, 0.3248, 0.3026, 0.3600, 0.3198, 0.2841, 0.2943, 0.2436,\n",
      "        0.2168, 0.3155, 0.3511, 0.2491, 0.3359, 0.3476, 0.3675, 0.3462, 0.3696,\n",
      "        0.2990, 0.2847, 0.2696, 0.2962, 0.3928, 0.2726, 0.3171, 0.2977, 0.2442,\n",
      "        0.3086, 0.3001, 0.3591, 0.2853, 0.3371, 0.2597, 0.3472, 0.3784, 0.3047])\n",
      "roc_auc score: 0.6353069060603259\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 4, Loss: 3.112302541732788\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-0.9882, -1.1312, -0.9969, -0.7977, -1.3214, -1.3079, -0.8921, -1.0070,\n",
      "        -0.9514, -1.5324, -1.1119, -0.9841, -0.7841, -0.6524, -0.8934, -0.9882,\n",
      "        -1.0053, -1.3346, -0.5983, -0.9510, -0.7607, -1.1770, -1.1810, -1.3961,\n",
      "        -1.2229, -0.6750, -1.0923, -1.0197, -1.1925, -1.2321, -1.2113, -1.1382,\n",
      "        -1.0701, -1.1943, -0.8435, -0.9217, -1.3716, -0.8563, -1.1155, -0.9488,\n",
      "        -0.8325, -0.9415, -1.2528, -1.0313, -0.8540, -0.9477, -0.6699, -0.7769,\n",
      "        -1.1153, -0.7555, -0.5859, -1.2609, -1.0193, -0.9061, -1.0912, -1.0814,\n",
      "        -0.8227, -1.0475, -0.8168, -0.8104, -1.1193, -0.9376, -1.0873, -1.0502,\n",
      "        -0.7978, -1.0423, -0.7924, -0.9802, -1.3489, -0.8443, -1.0506, -1.0227,\n",
      "        -0.7235, -0.9710, -0.8352, -1.1978, -0.6330, -0.9651, -1.0154, -1.1922,\n",
      "        -1.1866, -0.7902, -1.3670, -0.6654, -1.2604, -0.9308, -1.0043, -1.0351,\n",
      "        -1.5834, -1.3940, -0.8251, -1.0122, -1.1218, -0.9145, -0.8144, -0.8206,\n",
      "        -1.1219, -1.0343, -0.6685, -1.1239, -0.8151, -1.0985, -1.2322, -0.6860,\n",
      "        -0.7310, -1.1005, -1.2187, -0.9102, -0.9991, -0.9755, -0.7955, -0.7448,\n",
      "        -1.0106, -0.7319, -0.7541, -1.0689, -1.0957, -0.9422, -1.3431, -0.8714,\n",
      "        -0.8374, -0.8332, -1.0327, -1.0428, -1.2231, -0.8224, -1.0484, -1.4144,\n",
      "        -1.2504, -0.9230, -0.6560, -1.0376, -1.0630, -0.9050, -0.6570, -0.5785,\n",
      "        -0.8413, -0.7912, -1.1060, -0.7445, -1.0908, -0.8264, -1.0510, -1.0799,\n",
      "        -1.2215, -0.9977, -1.4574, -0.8565, -0.8724, -0.9460, -1.2513, -1.0807,\n",
      "        -0.9425, -1.2551, -1.1159, -1.0160, -0.8192, -1.3889, -0.8439, -0.9407,\n",
      "        -0.9882, -1.1381, -0.9402, -0.6812, -0.8360, -1.0742, -1.3482, -1.1109,\n",
      "        -0.9839, -1.3192, -0.9192, -0.8983, -1.0138, -1.2088, -1.2140, -1.0158,\n",
      "        -0.8841, -1.0744, -0.7090, -0.7772, -1.1283, -1.1344, -0.9497, -1.0648,\n",
      "        -0.7271, -0.9632, -1.2025, -1.1207, -1.3502, -1.5536, -1.0143, -0.7819,\n",
      "        -1.3539, -0.8520, -0.8183, -0.7300, -0.8378, -0.6749, -1.0525, -1.0733,\n",
      "        -1.2085, -1.0661, -0.5352, -1.1943, -0.9813, -1.0947, -1.3556, -1.0095,\n",
      "        -1.0106, -0.7319, -1.1097, -0.8393, -1.2464, -0.8159, -0.6083, -1.0548],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.2713, 0.2439, 0.2696, 0.3105, 0.2106, 0.2128, 0.2907, 0.2676, 0.2786,\n",
      "        0.1776, 0.2475, 0.2721, 0.3134, 0.3424, 0.2904, 0.2713, 0.2679, 0.2084,\n",
      "        0.3547, 0.2787, 0.3185, 0.2356, 0.2349, 0.1984, 0.2274, 0.3374, 0.2512,\n",
      "        0.2651, 0.2328, 0.2258, 0.2295, 0.2427, 0.2554, 0.2325, 0.3008, 0.2846,\n",
      "        0.2024, 0.2981, 0.2468, 0.2791, 0.3031, 0.2806, 0.2222, 0.2628, 0.2986,\n",
      "        0.2793, 0.3385, 0.3150, 0.2469, 0.3196, 0.3576, 0.2208, 0.2652, 0.2878,\n",
      "        0.2514, 0.2532, 0.3052, 0.2597, 0.3064, 0.3078, 0.2461, 0.2814, 0.2521,\n",
      "        0.2592, 0.3105, 0.2607, 0.3117, 0.2729, 0.2061, 0.3006, 0.2591, 0.2645,\n",
      "        0.3266, 0.2747, 0.3025, 0.2319, 0.3468, 0.2759, 0.2659, 0.2329, 0.2339,\n",
      "        0.3121, 0.2031, 0.3395, 0.2209, 0.2828, 0.2681, 0.2621, 0.1703, 0.1988,\n",
      "        0.3047, 0.2666, 0.2457, 0.2861, 0.3070, 0.3056, 0.2457, 0.2622, 0.3388,\n",
      "        0.2453, 0.3068, 0.2500, 0.2258, 0.3349, 0.3250, 0.2496, 0.2282, 0.2870,\n",
      "        0.2691, 0.2738, 0.3110, 0.3219, 0.2669, 0.3248, 0.3199, 0.2556, 0.2505,\n",
      "        0.2805, 0.2070, 0.2950, 0.3021, 0.3030, 0.2626, 0.2606, 0.2274, 0.3053,\n",
      "        0.2595, 0.1955, 0.2226, 0.2844, 0.3416, 0.2616, 0.2567, 0.2880, 0.3414,\n",
      "        0.3593, 0.3013, 0.3119, 0.2486, 0.3220, 0.2515, 0.3044, 0.2590, 0.2535,\n",
      "        0.2277, 0.2694, 0.1889, 0.2981, 0.2947, 0.2797, 0.2225, 0.2534, 0.2804,\n",
      "        0.2218, 0.2468, 0.2658, 0.3059, 0.1996, 0.3007, 0.2808, 0.2713, 0.2427,\n",
      "        0.2809, 0.3360, 0.3024, 0.2546, 0.2062, 0.2477, 0.2721, 0.2109, 0.2851,\n",
      "        0.2894, 0.2662, 0.2299, 0.2290, 0.2658, 0.2923, 0.2546, 0.3298, 0.3149,\n",
      "        0.2445, 0.2433, 0.2789, 0.2564, 0.3258, 0.2762, 0.2310, 0.2459, 0.2058,\n",
      "        0.1746, 0.2661, 0.3139, 0.2052, 0.2990, 0.3061, 0.3252, 0.3020, 0.3374,\n",
      "        0.2587, 0.2548, 0.2300, 0.2561, 0.3693, 0.2325, 0.2726, 0.2507, 0.2050,\n",
      "        0.2671, 0.2669, 0.3248, 0.2479, 0.3017, 0.2233, 0.3066, 0.3525, 0.2583])\n",
      "roc_auc score: 0.6629807803401424\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 5, Loss: 3.052201271057129\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-1.1666, -1.3252, -1.1698, -0.9825, -1.5405, -1.4994, -1.1017, -1.2357,\n",
      "        -1.1545, -1.7860, -1.3600, -1.1628, -0.9743, -0.8193, -1.0387, -1.1677,\n",
      "        -1.2058, -1.5657, -0.7170, -1.1593, -0.9233, -1.3854, -1.3756, -1.6359,\n",
      "        -1.4486, -0.8132, -1.3421, -1.2465, -1.3676, -1.4465, -1.5121, -1.4109,\n",
      "        -1.3153, -1.4586, -1.0479, -1.1435, -1.5911, -1.0447, -1.2847, -1.1565,\n",
      "        -1.0266, -1.1498, -1.4609, -1.2657, -0.9938, -1.0944, -0.8260, -0.9218,\n",
      "        -1.2984, -0.9043, -0.6954, -1.4856, -1.2421, -1.1133, -1.2836, -1.2673,\n",
      "        -0.9912, -1.2204, -0.9841, -0.9770, -1.3491, -1.1155, -1.2748, -1.2252,\n",
      "        -0.9895, -1.2636, -0.9788, -1.1872, -1.5685, -1.0319, -1.2026, -1.2405,\n",
      "        -0.8702, -1.1502, -0.9733, -1.4076, -0.7599, -1.1847, -1.2270, -1.4096,\n",
      "        -1.3831, -0.9669, -1.6048, -0.8050, -1.4711, -1.1297, -1.2113, -1.2283,\n",
      "        -1.8530, -1.6289, -1.0254, -1.2496, -1.3771, -1.1271, -1.0098, -1.0090,\n",
      "        -1.3204, -1.2629, -0.8233, -1.3137, -1.0013, -1.2614, -1.4613, -0.8313,\n",
      "        -0.9067, -1.3518, -1.3989, -1.1283, -1.2395, -1.1338, -0.9682, -0.8954,\n",
      "        -1.1813, -0.8984, -0.8885, -1.2417, -1.2797, -1.1192, -1.5611, -1.0445,\n",
      "        -1.0244, -1.0114, -1.2412, -1.2576, -1.4238, -0.9892, -1.2173, -1.6429,\n",
      "        -1.4614, -1.1163, -0.7860, -1.2501, -1.2397, -1.0980, -0.7897, -0.6836,\n",
      "        -1.0126, -0.9702, -1.2984, -0.9042, -1.2768, -0.9906, -1.2427, -1.2760,\n",
      "        -1.4330, -1.2163, -1.6967, -1.0515, -1.0557, -1.1527, -1.4198, -1.2598,\n",
      "        -1.1289, -1.4659, -1.3508, -1.2340, -0.9833, -1.6171, -0.9833, -1.0859,\n",
      "        -1.1609, -1.3254, -1.0902, -0.8380, -1.0275, -1.3014, -1.5830, -1.3342,\n",
      "        -1.1921, -1.5284, -1.1044, -1.0905, -1.2260, -1.4155, -1.4191, -1.2273,\n",
      "        -1.0798, -1.2663, -0.8564, -0.9593, -1.3017, -1.3993, -1.1635, -1.2957,\n",
      "        -0.8794, -1.1643, -1.4746, -1.3712, -1.5809, -1.8052, -1.2497, -0.9439,\n",
      "        -1.5986, -1.0240, -0.9989, -0.9274, -1.0337, -0.8224, -1.2481, -1.2237,\n",
      "        -1.4351, -1.2670, -0.6428, -1.4084, -1.1942, -1.3255, -1.5820, -1.2145,\n",
      "        -1.1813, -0.8984, -1.2984, -1.0128, -1.4557, -1.0084, -0.7170, -1.2781],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.2375, 0.2100, 0.2369, 0.2724, 0.1765, 0.1825, 0.2494, 0.2252, 0.2397,\n",
      "        0.1436, 0.2042, 0.2382, 0.2740, 0.3059, 0.2614, 0.2373, 0.2304, 0.1728,\n",
      "        0.3281, 0.2388, 0.2843, 0.2001, 0.2017, 0.1630, 0.1902, 0.3072, 0.2072,\n",
      "        0.2233, 0.2030, 0.1905, 0.1806, 0.1961, 0.2116, 0.1887, 0.2596, 0.2417,\n",
      "        0.1692, 0.2602, 0.2167, 0.2393, 0.2637, 0.2405, 0.1883, 0.2200, 0.2702,\n",
      "        0.2508, 0.3045, 0.2846, 0.2144, 0.2882, 0.3328, 0.1846, 0.2241, 0.2473,\n",
      "        0.2169, 0.2197, 0.2707, 0.2279, 0.2721, 0.2735, 0.2060, 0.2469, 0.2184,\n",
      "        0.2270, 0.2710, 0.2203, 0.2731, 0.2338, 0.1724, 0.2627, 0.2310, 0.2243,\n",
      "        0.2952, 0.2405, 0.2742, 0.1966, 0.3187, 0.2342, 0.2267, 0.1963, 0.2005,\n",
      "        0.2755, 0.1673, 0.3090, 0.1868, 0.2442, 0.2295, 0.2265, 0.1355, 0.1640,\n",
      "        0.2640, 0.2228, 0.2015, 0.2447, 0.2670, 0.2672, 0.2108, 0.2205, 0.3051,\n",
      "        0.2119, 0.2687, 0.2207, 0.1883, 0.3034, 0.2877, 0.2056, 0.1980, 0.2445,\n",
      "        0.2245, 0.2435, 0.2752, 0.2900, 0.2348, 0.2894, 0.2914, 0.2241, 0.2176,\n",
      "        0.2462, 0.1735, 0.2603, 0.2642, 0.2667, 0.2242, 0.2214, 0.1941, 0.2711,\n",
      "        0.2284, 0.1621, 0.1883, 0.2467, 0.3130, 0.2227, 0.2245, 0.2501, 0.3122,\n",
      "        0.3355, 0.2665, 0.2748, 0.2144, 0.2882, 0.2181, 0.2708, 0.2240, 0.2182,\n",
      "        0.1926, 0.2286, 0.1549, 0.2589, 0.2581, 0.2400, 0.1947, 0.2210, 0.2444,\n",
      "        0.1876, 0.2057, 0.2255, 0.2722, 0.1656, 0.2722, 0.2524, 0.2385, 0.2099,\n",
      "        0.2516, 0.3020, 0.2636, 0.2139, 0.1704, 0.2085, 0.2329, 0.1782, 0.2489,\n",
      "        0.2515, 0.2269, 0.1954, 0.1948, 0.2267, 0.2535, 0.2199, 0.2981, 0.2770,\n",
      "        0.2139, 0.1979, 0.2380, 0.2149, 0.2933, 0.2379, 0.1863, 0.2024, 0.1707,\n",
      "        0.1412, 0.2227, 0.2801, 0.1682, 0.2643, 0.2692, 0.2835, 0.2624, 0.3053,\n",
      "        0.2230, 0.2273, 0.1923, 0.2198, 0.3446, 0.1965, 0.2325, 0.2099, 0.1705,\n",
      "        0.2289, 0.2348, 0.2894, 0.2144, 0.2664, 0.1891, 0.2673, 0.3281, 0.2179])\n",
      "roc_auc score: 0.6824712311673907\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_5.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_5.html. Open it in your browser.\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 6, Loss: 2.9772145748138428\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-1.3501, -1.5319, -1.3445, -1.1635, -1.7634, -1.6858, -1.3113, -1.4607,\n",
      "        -1.3473, -2.0375, -1.6002, -1.3493, -1.1532, -0.9934, -1.1823, -1.3452,\n",
      "        -1.4123, -1.7946, -0.8365, -1.3722, -1.0912, -1.5857, -1.5654, -1.8733,\n",
      "        -1.6596, -0.9493, -1.5956, -1.4682, -1.5420, -1.6582, -1.8232, -1.6808,\n",
      "        -1.5659, -1.7138, -1.2543, -1.3712, -1.8021, -1.2324, -1.4688, -1.3634,\n",
      "        -1.2182, -1.3676, -1.6865, -1.4992, -1.1414, -1.2476, -0.9932, -1.0646,\n",
      "        -1.4753, -1.0525, -0.8054, -1.7124, -1.4604, -1.3134, -1.4793, -1.4504,\n",
      "        -1.1622, -1.3915, -1.1486, -1.1439, -1.5738, -1.2989, -1.4600, -1.3969,\n",
      "        -1.1747, -1.4802, -1.1659, -1.3953, -1.7866, -1.2245, -1.3591, -1.4555,\n",
      "        -1.0219, -1.3389, -1.1196, -1.6043, -0.8927, -1.4048, -1.4438, -1.6186,\n",
      "        -1.5992, -1.1439, -1.8394, -0.9431, -1.6875, -1.3206, -1.4209, -1.4207,\n",
      "        -2.1123, -1.8660, -1.2235, -1.4768, -1.6378, -1.3431, -1.2031, -1.2027,\n",
      "        -1.5266, -1.4873, -0.9795, -1.4992, -1.1892, -1.4409, -1.6783, -0.9739,\n",
      "        -1.0877, -1.6143, -1.5844, -1.3516, -1.4681, -1.2913, -1.1542, -1.0431,\n",
      "        -1.3500, -1.0770, -1.0223, -1.4134, -1.4628, -1.2993, -1.7796, -1.2318,\n",
      "        -1.2168, -1.1790, -1.4552, -1.4658, -1.6296, -1.1592, -1.3871, -1.8738,\n",
      "        -1.6670, -1.3111, -0.9211, -1.4667, -1.4117, -1.2812, -0.9184, -0.7944,\n",
      "        -1.1898, -1.1561, -1.4991, -1.0860, -1.4565, -1.1599, -1.4432, -1.4600,\n",
      "        -1.6569, -1.4335, -1.9360, -1.2386, -1.2432, -1.3621, -1.5896, -1.4356,\n",
      "        -1.3260, -1.6771, -1.5874, -1.4501, -1.1637, -1.8486, -1.1301, -1.2383,\n",
      "        -1.3346, -1.5253, -1.2459, -0.9924, -1.2191, -1.5241, -1.8070, -1.5628,\n",
      "        -1.4060, -1.7329, -1.2964, -1.2825, -1.4357, -1.6227, -1.6327, -1.4344,\n",
      "        -1.2696, -1.4638, -1.0052, -1.1384, -1.4933, -1.6745, -1.3768, -1.5302,\n",
      "        -1.0275, -1.3566, -1.7570, -1.6131, -1.8178, -2.0568, -1.4813, -1.1016,\n",
      "        -1.8238, -1.2026, -1.1828, -1.1254, -1.2260, -0.9752, -1.4425, -1.3779,\n",
      "        -1.6582, -1.4622, -0.7541, -1.6119, -1.4109, -1.5573, -1.8123, -1.4279,\n",
      "        -1.3500, -1.0770, -1.4816, -1.1885, -1.6665, -1.1950, -0.8359, -1.4970],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.2059, 0.1777, 0.2068, 0.2380, 0.1464, 0.1563, 0.2123, 0.1884, 0.2063,\n",
      "        0.1153, 0.1679, 0.2060, 0.2399, 0.2702, 0.2346, 0.2067, 0.1959, 0.1425,\n",
      "        0.3023, 0.2023, 0.2514, 0.1700, 0.1729, 0.1332, 0.1598, 0.2790, 0.1686,\n",
      "        0.1872, 0.1762, 0.1600, 0.1390, 0.1570, 0.1728, 0.1527, 0.2220, 0.2024,\n",
      "        0.1416, 0.2258, 0.1871, 0.2037, 0.2283, 0.2030, 0.1562, 0.1825, 0.2421,\n",
      "        0.2231, 0.2703, 0.2564, 0.1861, 0.2588, 0.3089, 0.1529, 0.1884, 0.2119,\n",
      "        0.1855, 0.1899, 0.2383, 0.1992, 0.2407, 0.2416, 0.1717, 0.2143, 0.1885,\n",
      "        0.1983, 0.2360, 0.1854, 0.2376, 0.1986, 0.1435, 0.2271, 0.2044, 0.1892,\n",
      "        0.2646, 0.2077, 0.2461, 0.1674, 0.2906, 0.1971, 0.1910, 0.1654, 0.1681,\n",
      "        0.2416, 0.1371, 0.2803, 0.1561, 0.2107, 0.1945, 0.1946, 0.1079, 0.1340,\n",
      "        0.2273, 0.1859, 0.1628, 0.2070, 0.2309, 0.2310, 0.1785, 0.1843, 0.2730,\n",
      "        0.1825, 0.2334, 0.1914, 0.1573, 0.2741, 0.2521, 0.1660, 0.1702, 0.2056,\n",
      "        0.1872, 0.2156, 0.2397, 0.2606, 0.2059, 0.2541, 0.2646, 0.1957, 0.1880,\n",
      "        0.2143, 0.1444, 0.2259, 0.2285, 0.2352, 0.1892, 0.1876, 0.1639, 0.2388,\n",
      "        0.1999, 0.1331, 0.1588, 0.2123, 0.2847, 0.1874, 0.1960, 0.2173, 0.2853,\n",
      "        0.3112, 0.2333, 0.2394, 0.1826, 0.2524, 0.1890, 0.2387, 0.1910, 0.1885,\n",
      "        0.1602, 0.1925, 0.1261, 0.2247, 0.2239, 0.2039, 0.1694, 0.1922, 0.2098,\n",
      "        0.1575, 0.1697, 0.1900, 0.2380, 0.1360, 0.2441, 0.2247, 0.2084, 0.1787,\n",
      "        0.2234, 0.2704, 0.2281, 0.1789, 0.1410, 0.1732, 0.1969, 0.1502, 0.2148,\n",
      "        0.2171, 0.1922, 0.1648, 0.1635, 0.1924, 0.2193, 0.1879, 0.2679, 0.2426,\n",
      "        0.1834, 0.1578, 0.2015, 0.1780, 0.2636, 0.2048, 0.1472, 0.1662, 0.1397,\n",
      "        0.1134, 0.1852, 0.2494, 0.1390, 0.2310, 0.2346, 0.2450, 0.2269, 0.2738,\n",
      "        0.1912, 0.2013, 0.1600, 0.1881, 0.3199, 0.1663, 0.1961, 0.1740, 0.1404,\n",
      "        0.1934, 0.2059, 0.2541, 0.1852, 0.2335, 0.1589, 0.2324, 0.3024, 0.1829])\n",
      "roc_auc score: 0.6954484972425053\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 7, Loss: 2.899169683456421\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-1.5374, -1.7469, -1.5193, -1.3364, -1.9938, -1.8853, -1.5135, -1.6948,\n",
      "        -1.5410, -2.2928, -1.8513, -1.5408, -1.3310, -1.1694, -1.3269, -1.5222,\n",
      "        -1.6175, -2.0267, -0.9633, -1.5790, -1.2613, -1.7866, -1.7531, -2.1150,\n",
      "        -1.8678, -1.0860, -1.8599, -1.6968, -1.7250, -1.8836, -2.1340, -1.9547,\n",
      "        -1.8164, -1.9743, -1.4633, -1.5998, -2.0124, -1.4154, -1.6681, -1.5814,\n",
      "        -1.3994, -1.5814, -1.9156, -1.7208, -1.2950, -1.4030, -1.1630, -1.2097,\n",
      "        -1.6547, -1.1987, -0.9255, -1.9374, -1.6906, -1.5046, -1.6863, -1.6281,\n",
      "        -1.3257, -1.5702, -1.3090, -1.3137, -1.8104, -1.4793, -1.6379, -1.5754,\n",
      "        -1.3657, -1.7011, -1.3542, -1.5988, -2.0092, -1.4280, -1.5297, -1.6772,\n",
      "        -1.1735, -1.5223, -1.2733, -1.8013, -1.0252, -1.6181, -1.6568, -1.8291,\n",
      "        -1.8156, -1.3211, -2.0758, -1.0766, -1.8982, -1.5048, -1.6359, -1.6126,\n",
      "        -2.3684, -2.1038, -1.4258, -1.7105, -1.8973, -1.5468, -1.3928, -1.4037,\n",
      "        -1.7343, -1.7243, -1.1333, -1.6832, -1.3896, -1.6359, -1.9021, -1.1171,\n",
      "        -1.2660, -1.8828, -1.7682, -1.5731, -1.6885, -1.4575, -1.3394, -1.1912,\n",
      "        -1.5263, -1.2558, -1.1608, -1.5900, -1.6391, -1.4766, -1.9980, -1.4275,\n",
      "        -1.4190, -1.3422, -1.6639, -1.6760, -1.8506, -1.3231, -1.5656, -2.1095,\n",
      "        -1.8697, -1.5197, -1.0591, -1.6840, -1.5865, -1.4631, -1.0453, -0.9075,\n",
      "        -1.3655, -1.3421, -1.6997, -1.2627, -1.6339, -1.3226, -1.6436, -1.6467,\n",
      "        -1.8785, -1.6572, -2.1732, -1.4149, -1.4256, -1.5767, -1.7738, -1.6156,\n",
      "        -1.5158, -1.8923, -1.8399, -1.6682, -1.3426, -2.0840, -1.2839, -1.3947,\n",
      "        -1.5080, -1.7324, -1.4022, -1.1401, -1.4156, -1.7481, -2.0167, -1.7984,\n",
      "        -1.6212, -1.9472, -1.4871, -1.4697, -1.6506, -1.8322, -1.8449, -1.6503,\n",
      "        -1.4479, -1.6617, -1.1550, -1.3291, -1.6949, -1.9552, -1.5928, -1.7697,\n",
      "        -1.1809, -1.5550, -2.0467, -1.8635, -2.0483, -2.3142, -1.7054, -1.2665,\n",
      "        -2.0538, -1.3876, -1.3818, -1.3210, -1.4225, -1.1373, -1.6465, -1.5340,\n",
      "        -1.8815, -1.6458, -0.8672, -1.8133, -1.6227, -1.8018, -2.0455, -1.6397,\n",
      "        -1.5263, -1.2558, -1.6625, -1.3529, -1.8792, -1.3942, -0.9578, -1.7268],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.1769, 0.1484, 0.1796, 0.2081, 0.1199, 0.1318, 0.1804, 0.1551, 0.1764,\n",
      "        0.0917, 0.1357, 0.1764, 0.2090, 0.2370, 0.2097, 0.1791, 0.1656, 0.1164,\n",
      "        0.2762, 0.1709, 0.2207, 0.1435, 0.1477, 0.1076, 0.1338, 0.2524, 0.1347,\n",
      "        0.1549, 0.1512, 0.1320, 0.1058, 0.1240, 0.1399, 0.1219, 0.1880, 0.1680,\n",
      "        0.1179, 0.1954, 0.1587, 0.1706, 0.1979, 0.1706, 0.1284, 0.1518, 0.2150,\n",
      "        0.1973, 0.2381, 0.2298, 0.1605, 0.2317, 0.2838, 0.1259, 0.1557, 0.1817,\n",
      "        0.1563, 0.1641, 0.2099, 0.1722, 0.2127, 0.2119, 0.1406, 0.1855, 0.1627,\n",
      "        0.1715, 0.2033, 0.1543, 0.2052, 0.1682, 0.1182, 0.1934, 0.1780, 0.1575,\n",
      "        0.2362, 0.1791, 0.2187, 0.1417, 0.2640, 0.1655, 0.1602, 0.1384, 0.1400,\n",
      "        0.2106, 0.1115, 0.2541, 0.1303, 0.1817, 0.1630, 0.1662, 0.0856, 0.1087,\n",
      "        0.1938, 0.1531, 0.1304, 0.1755, 0.1990, 0.1972, 0.1500, 0.1513, 0.2436,\n",
      "        0.1567, 0.1995, 0.1630, 0.1299, 0.2466, 0.2199, 0.1321, 0.1458, 0.1718,\n",
      "        0.1560, 0.1889, 0.2076, 0.2330, 0.1785, 0.2217, 0.2385, 0.1694, 0.1626,\n",
      "        0.1859, 0.1194, 0.1935, 0.1948, 0.2071, 0.1592, 0.1576, 0.1358, 0.2103,\n",
      "        0.1729, 0.1082, 0.1336, 0.1795, 0.2575, 0.1566, 0.1699, 0.1880, 0.2601,\n",
      "        0.2875, 0.2033, 0.2072, 0.1545, 0.2205, 0.1633, 0.2104, 0.1620, 0.1616,\n",
      "        0.1326, 0.1601, 0.1022, 0.1955, 0.1938, 0.1713, 0.1451, 0.1658, 0.1801,\n",
      "        0.1310, 0.1371, 0.1587, 0.2071, 0.1107, 0.2169, 0.1987, 0.1812, 0.1503,\n",
      "        0.1975, 0.2423, 0.1954, 0.1483, 0.1175, 0.1421, 0.1650, 0.1249, 0.1844,\n",
      "        0.1870, 0.1610, 0.1380, 0.1365, 0.1611, 0.1903, 0.1595, 0.2396, 0.2093,\n",
      "        0.1551, 0.1240, 0.1690, 0.1456, 0.2349, 0.1744, 0.1144, 0.1343, 0.1142,\n",
      "        0.0900, 0.1538, 0.2199, 0.1137, 0.1998, 0.2007, 0.2106, 0.1943, 0.2428,\n",
      "        0.1616, 0.1774, 0.1322, 0.1617, 0.2958, 0.1402, 0.1648, 0.1416, 0.1145,\n",
      "        0.1625, 0.1785, 0.2217, 0.1594, 0.2054, 0.1325, 0.1987, 0.2773, 0.1510])\n",
      "roc_auc score: 0.703660805065163\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 8, Loss: 2.826077938079834\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-1.7230, -1.9586, -1.6933, -1.5064, -2.2276, -2.0975, -1.7064, -1.9286,\n",
      "        -1.7380, -2.5391, -2.0967, -1.7328, -1.5048, -1.3508, -1.4803, -1.7028,\n",
      "        -1.8193, -2.2559, -1.0963, -1.7814, -1.4301, -1.9959, -1.9554, -2.3527,\n",
      "        -2.0884, -1.2299, -2.1245, -1.9236, -1.9168, -2.1154, -2.4377, -2.2231,\n",
      "        -2.0629, -2.2289, -1.6781, -1.8186, -2.2371, -1.5987, -1.8686, -1.8068,\n",
      "        -1.5785, -1.7857, -2.1462, -1.9304, -1.4551, -1.5650, -1.3279, -1.3566,\n",
      "        -1.8477, -1.3405, -1.0543, -2.1570, -1.9295, -1.7005, -1.8979, -1.8002,\n",
      "        -1.4821, -1.7615, -1.4691, -1.4821, -2.0509, -1.6569, -1.8140, -1.7690,\n",
      "        -1.5679, -1.9311, -1.5350, -1.8062, -2.2373, -1.6363, -1.6993, -1.8996,\n",
      "        -1.3240, -1.7005, -1.4350, -2.0156, -1.1598, -1.8267, -1.8701, -2.0438,\n",
      "        -2.0411, -1.4924, -2.3112, -1.2007, -2.1199, -1.6827, -1.8475, -1.7950,\n",
      "        -2.6195, -2.3409, -1.6176, -1.9504, -2.1476, -1.7499, -1.5838, -1.6058,\n",
      "        -1.9505, -1.9754, -1.2759, -1.8818, -1.5952, -1.8460, -2.1245, -1.2618,\n",
      "        -1.4449, -2.1569, -1.9618, -1.7844, -1.9084, -1.6313, -1.5157, -1.3387,\n",
      "        -1.7152, -1.4398, -1.3008, -1.7845, -1.8055, -1.6483, -2.2261, -1.6245,\n",
      "        -1.6265, -1.5052, -1.8781, -1.8816, -2.0652, -1.4840, -1.7567, -2.3384,\n",
      "        -2.0694, -1.7229, -1.2004, -1.8997, -1.7801, -1.6390, -1.1633, -1.0329,\n",
      "        -1.5475, -1.5227, -1.8961, -1.4340, -1.8069, -1.4809, -1.8490, -1.8425,\n",
      "        -2.0971, -1.8831, -2.4083, -1.5903, -1.6094, -1.7910, -1.9719, -1.8098,\n",
      "        -1.6987, -2.1144, -2.0850, -1.8802, -1.5187, -2.3125, -1.4439, -1.5574,\n",
      "        -1.6826, -1.9416, -1.5614, -1.2839, -1.6026, -1.9855, -2.2327, -2.0293,\n",
      "        -1.8334, -2.1688, -1.6704, -1.6491, -1.8568, -2.0501, -2.0638, -1.8644,\n",
      "        -1.6236, -1.8526, -1.3031, -1.5262, -1.9009, -2.2212, -1.8139, -2.0025,\n",
      "        -1.3354, -1.7529, -2.3296, -2.1139, -2.2814, -2.5641, -1.9319, -1.4334,\n",
      "        -2.2974, -1.5719, -1.5825, -1.5106, -1.6222, -1.2990, -1.8496, -1.7128,\n",
      "        -2.1070, -1.8234, -0.9808, -2.0167, -1.8217, -2.0462, -2.2748, -1.8433,\n",
      "        -1.7152, -1.4398, -1.8380, -1.5213, -2.0998, -1.5985, -1.0842, -1.9610],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.1515, 0.1236, 0.1553, 0.1815, 0.0973, 0.1093, 0.1536, 0.1269, 0.1496,\n",
      "        0.0732, 0.1094, 0.1502, 0.1817, 0.2057, 0.1854, 0.1541, 0.1395, 0.0948,\n",
      "        0.2504, 0.1441, 0.1931, 0.1196, 0.1240, 0.0869, 0.1102, 0.2262, 0.1067,\n",
      "        0.1275, 0.1282, 0.1076, 0.0803, 0.0977, 0.1128, 0.0972, 0.1574, 0.1396,\n",
      "        0.0965, 0.1682, 0.1337, 0.1410, 0.1710, 0.1436, 0.1047, 0.1267, 0.1892,\n",
      "        0.1729, 0.2095, 0.2048, 0.1361, 0.2074, 0.2584, 0.1037, 0.1268, 0.1544,\n",
      "        0.1303, 0.1418, 0.1851, 0.1466, 0.1871, 0.1851, 0.1140, 0.1602, 0.1402,\n",
      "        0.1457, 0.1725, 0.1266, 0.1773, 0.1411, 0.0964, 0.1630, 0.1546, 0.1302,\n",
      "        0.2101, 0.1544, 0.1923, 0.1176, 0.2387, 0.1386, 0.1335, 0.1147, 0.1150,\n",
      "        0.1836, 0.0902, 0.2314, 0.1072, 0.1567, 0.1362, 0.1425, 0.0679, 0.0878,\n",
      "        0.1655, 0.1245, 0.1046, 0.1481, 0.1703, 0.1672, 0.1245, 0.1218, 0.2182,\n",
      "        0.1322, 0.1687, 0.1363, 0.1067, 0.2207, 0.1908, 0.1037, 0.1233, 0.1438,\n",
      "        0.1292, 0.1637, 0.1801, 0.2077, 0.1525, 0.1916, 0.2140, 0.1437, 0.1412,\n",
      "        0.1613, 0.0974, 0.1646, 0.1643, 0.1817, 0.1326, 0.1322, 0.1125, 0.1848,\n",
      "        0.1472, 0.0880, 0.1121, 0.1515, 0.2314, 0.1301, 0.1443, 0.1626, 0.2381,\n",
      "        0.2625, 0.1754, 0.1791, 0.1306, 0.1925, 0.1410, 0.1853, 0.1360, 0.1368,\n",
      "        0.1094, 0.1320, 0.0825, 0.1693, 0.1667, 0.1429, 0.1222, 0.1407, 0.1546,\n",
      "        0.1077, 0.1106, 0.1324, 0.1797, 0.0901, 0.1909, 0.1740, 0.1567, 0.1255,\n",
      "        0.1735, 0.2169, 0.1676, 0.1207, 0.0969, 0.1162, 0.1378, 0.1026, 0.1584,\n",
      "        0.1612, 0.1351, 0.1140, 0.1127, 0.1342, 0.1647, 0.1356, 0.2136, 0.1785,\n",
      "        0.1300, 0.0979, 0.1402, 0.1189, 0.2083, 0.1477, 0.0887, 0.1078, 0.0927,\n",
      "        0.0715, 0.1265, 0.1926, 0.0913, 0.1719, 0.1704, 0.1808, 0.1649, 0.2143,\n",
      "        0.1359, 0.1528, 0.1084, 0.1390, 0.2727, 0.1175, 0.1392, 0.1144, 0.0932,\n",
      "        0.1367, 0.1525, 0.1916, 0.1373, 0.1793, 0.1091, 0.1682, 0.2527, 0.1234])\n",
      "roc_auc score: 0.7092667798933974\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 9, Loss: 2.732024908065796\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-1.9037, -2.1724, -1.8833, -1.6756, -2.4618, -2.3200, -1.9023, -2.1642,\n",
      "        -1.9320, -2.7877, -2.3377, -1.9238, -1.6843, -1.5269, -1.6438, -1.8993,\n",
      "        -2.0161, -2.4865, -1.2329, -1.9817, -1.5937, -2.2033, -2.1569, -2.5879,\n",
      "        -2.3156, -1.3773, -2.3764, -2.1410, -2.1277, -2.3481, -2.7343, -2.4796,\n",
      "        -2.3112, -2.4775, -1.8830, -2.0340, -2.4679, -1.7804, -2.0721, -2.0295,\n",
      "        -1.7581, -1.9877, -2.3737, -2.1505, -1.6199, -1.7287, -1.4982, -1.5030,\n",
      "        -2.0487, -1.4901, -1.1886, -2.3811, -2.1654, -1.8935, -2.1023, -1.9875,\n",
      "        -1.6467, -1.9567, -1.6311, -1.6446, -2.2861, -1.8292, -2.0022, -1.9654,\n",
      "        -1.7694, -2.1610, -1.7210, -2.0085, -2.4718, -1.8434, -1.8753, -2.1177,\n",
      "        -1.4734, -1.8757, -1.6027, -2.2206, -1.2987, -2.0398, -2.0857, -2.2646,\n",
      "        -2.2620, -1.6632, -2.5426, -1.3219, -2.3435, -1.8610, -2.0506, -1.9849,\n",
      "        -2.8767, -2.5834, -1.8061, -2.1836, -2.3950, -1.9559, -1.7761, -1.8172,\n",
      "        -2.1624, -2.2200, -1.4143, -2.0909, -1.8006, -2.0525, -2.3549, -1.4120,\n",
      "        -1.6164, -2.4212, -2.1748, -1.9891, -2.1303, -1.8059, -1.6952, -1.4858,\n",
      "        -1.9066, -1.6251, -1.4375, -1.9809, -1.9915, -1.8206, -2.4598, -1.8168,\n",
      "        -1.8340, -1.6689, -2.0884, -2.0781, -2.2811, -1.6467, -1.9503, -2.5755,\n",
      "        -2.2816, -1.9159, -1.3422, -2.1109, -1.9774, -1.8171, -1.2783, -1.1608,\n",
      "        -1.7282, -1.7068, -2.0971, -1.6028, -1.9904, -1.6412, -2.0465, -2.0544,\n",
      "        -2.3249, -2.1125, -2.6453, -1.7674, -1.7914, -2.0032, -2.1845, -2.0102,\n",
      "        -1.8844, -2.3386, -2.3263, -2.0907, -1.6885, -2.5448, -1.6103, -1.7207,\n",
      "        -1.8718, -2.1526, -1.7200, -1.4231, -1.7924, -2.2201, -2.4574, -2.2549,\n",
      "        -2.0496, -2.3957, -1.8539, -1.8341, -2.0588, -2.2699, -2.2845, -2.0665,\n",
      "        -1.8049, -2.0521, -1.4515, -1.7227, -2.1140, -2.4773, -2.0352, -2.2340,\n",
      "        -1.4923, -1.9451, -2.5967, -2.3597, -2.5196, -2.8164, -2.1574, -1.6053,\n",
      "        -2.5431, -1.7514, -1.7826, -1.6948, -1.8253, -1.4535, -2.0544, -1.8944,\n",
      "        -2.3338, -2.0201, -1.1013, -2.2263, -2.0213, -2.2805, -2.5119, -2.0461,\n",
      "        -1.9066, -1.6251, -2.0237, -1.6908, -2.3224, -1.8003, -1.2150, -2.1905],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.1297, 0.1023, 0.1320, 0.1577, 0.0786, 0.0895, 0.1298, 0.1030, 0.1265,\n",
      "        0.0580, 0.0881, 0.1274, 0.1565, 0.1785, 0.1619, 0.1302, 0.1175, 0.0768,\n",
      "        0.2257, 0.1211, 0.1689, 0.0995, 0.1037, 0.0699, 0.0898, 0.2014, 0.0850,\n",
      "        0.1052, 0.1064, 0.0872, 0.0610, 0.0773, 0.0902, 0.0774, 0.1320, 0.1157,\n",
      "        0.0781, 0.1442, 0.1118, 0.1161, 0.1470, 0.1205, 0.0852, 0.1043, 0.1652,\n",
      "        0.1508, 0.1827, 0.1820, 0.1142, 0.1839, 0.2335, 0.0846, 0.1029, 0.1308,\n",
      "        0.1089, 0.1205, 0.1615, 0.1238, 0.1637, 0.1618, 0.0923, 0.1383, 0.1190,\n",
      "        0.1229, 0.1456, 0.1033, 0.1517, 0.1183, 0.0779, 0.1367, 0.1329, 0.1074,\n",
      "        0.1864, 0.1329, 0.1676, 0.0979, 0.2144, 0.1151, 0.1105, 0.0941, 0.0943,\n",
      "        0.1593, 0.0729, 0.2105, 0.0876, 0.1346, 0.1140, 0.1208, 0.0533, 0.0702,\n",
      "        0.1411, 0.1012, 0.0836, 0.1239, 0.1448, 0.1398, 0.1032, 0.0980, 0.1956,\n",
      "        0.1100, 0.1418, 0.1138, 0.0867, 0.1959, 0.1657, 0.0816, 0.1020, 0.1204,\n",
      "        0.1062, 0.1411, 0.1551, 0.1846, 0.1294, 0.1645, 0.1919, 0.1212, 0.1201,\n",
      "        0.1394, 0.0787, 0.1398, 0.1378, 0.1586, 0.1102, 0.1112, 0.0927, 0.1616,\n",
      "        0.1245, 0.0707, 0.0927, 0.1283, 0.2072, 0.1080, 0.1216, 0.1398, 0.2178,\n",
      "        0.2385, 0.1508, 0.1536, 0.1094, 0.1676, 0.1202, 0.1623, 0.1144, 0.1136,\n",
      "        0.0891, 0.1079, 0.0663, 0.1459, 0.1429, 0.1189, 0.1011, 0.1181, 0.1319,\n",
      "        0.0880, 0.0890, 0.1100, 0.1560, 0.0728, 0.1665, 0.1518, 0.1333, 0.1041,\n",
      "        0.1519, 0.1942, 0.1428, 0.0980, 0.0789, 0.0949, 0.1141, 0.0835, 0.1354,\n",
      "        0.1378, 0.1132, 0.0936, 0.0924, 0.1124, 0.1413, 0.1138, 0.1898, 0.1515,\n",
      "        0.1077, 0.0775, 0.1156, 0.0967, 0.1836, 0.1251, 0.0694, 0.0863, 0.0745,\n",
      "        0.0564, 0.1036, 0.1672, 0.0729, 0.1479, 0.1440, 0.1551, 0.1388, 0.1895,\n",
      "        0.1136, 0.1307, 0.0884, 0.1171, 0.2495, 0.0974, 0.1170, 0.0928, 0.0750,\n",
      "        0.1145, 0.1294, 0.1645, 0.1167, 0.1557, 0.0893, 0.1418, 0.2288, 0.1006])\n",
      "roc_auc score: 0.7124481525556889\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 10, Loss: 2.622401237487793\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-2.0886, -2.3882, -2.0901, -1.8470, -2.6924, -2.5510, -2.0978, -2.4067,\n",
      "        -2.1221, -3.0580, -2.5701, -2.1142, -1.8624, -1.7020, -1.8051, -2.1113,\n",
      "        -2.2115, -2.7251, -1.3722, -2.1897, -1.7564, -2.4158, -2.3614, -2.8453,\n",
      "        -2.5431, -1.5263, -2.6210, -2.3573, -2.3404, -2.5755, -3.0142, -2.7364,\n",
      "        -2.5667, -2.7202, -2.0815, -2.2487, -2.7204, -1.9576, -2.2760, -2.2576,\n",
      "        -1.9387, -2.1994, -2.6034, -2.3689, -1.7852, -1.8977, -1.6722, -1.6458,\n",
      "        -2.2453, -1.6372, -1.3226, -2.6206, -2.3936, -2.0884, -2.3130, -2.1901,\n",
      "        -1.8118, -2.1478, -1.7905, -1.8051, -2.5079, -2.0034, -2.2046, -2.1594,\n",
      "        -1.9685, -2.3791, -1.9068, -2.2128, -2.7100, -2.0553, -2.0681, -2.3361,\n",
      "        -1.6222, -2.0644, -1.7713, -2.4280, -1.4482, -2.2538, -2.2848, -2.4851,\n",
      "        -2.4836, -1.8321, -2.7856, -1.4440, -2.5677, -2.0403, -2.2504, -2.1842,\n",
      "        -3.1459, -2.8350, -1.9869, -2.4137, -2.6403, -2.1598, -1.9701, -2.0301,\n",
      "        -2.3771, -2.4467, -1.5451, -2.3099, -2.0099, -2.2546, -2.5932, -1.5726,\n",
      "        -1.7777, -2.6742, -2.3891, -2.1937, -2.3574, -1.9782, -1.8762, -1.6305,\n",
      "        -2.0966, -1.8083, -1.5722, -2.1747, -2.1866, -1.9893, -2.6946, -2.0027,\n",
      "        -2.0451, -1.8323, -2.2919, -2.2752, -2.4998, -1.8096, -2.1416, -2.8170,\n",
      "        -2.5054, -2.1138, -1.4870, -2.3163, -2.1689, -1.9928, -1.3948, -1.2905,\n",
      "        -1.9066, -1.8908, -2.3039, -1.7717, -2.1894, -1.8057, -2.2533, -2.2598,\n",
      "        -2.5631, -2.3331, -2.9013, -1.9468, -1.9674, -2.2091, -2.4011, -2.2085,\n",
      "        -2.0669, -2.5656, -2.5599, -2.3004, -1.8561, -2.7983, -1.7734, -1.8879,\n",
      "        -2.0726, -2.3679, -1.8855, -1.5550, -1.9717, -2.4350, -2.6926, -2.4739,\n",
      "        -2.2648, -2.6250, -2.0352, -2.0215, -2.2566, -2.4994, -2.5105, -2.2694,\n",
      "        -1.9822, -2.2566, -1.6039, -1.9166, -2.3184, -2.7231, -2.2495, -2.4635,\n",
      "        -1.6545, -2.1364, -2.8514, -2.5899, -2.7661, -3.0803, -2.3817, -1.7780,\n",
      "        -2.7861, -1.9307, -1.9802, -1.8779, -2.0259, -1.6087, -2.2699, -2.0741,\n",
      "        -2.5651, -2.2218, -1.2329, -2.4413, -2.2224, -2.5090, -2.7530, -2.2475,\n",
      "        -2.0966, -1.8083, -2.2308, -1.8626, -2.5432, -2.0015, -1.3499, -2.4117],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.1102, 0.0841, 0.1101, 0.1362, 0.0634, 0.0724, 0.1093, 0.0827, 0.1070,\n",
      "        0.0449, 0.0711, 0.1077, 0.1344, 0.1542, 0.1412, 0.1080, 0.0987, 0.0615,\n",
      "        0.2023, 0.1007, 0.1472, 0.0820, 0.0862, 0.0549, 0.0729, 0.1785, 0.0678,\n",
      "        0.0865, 0.0878, 0.0707, 0.0468, 0.0609, 0.0713, 0.0618, 0.1109, 0.0955,\n",
      "        0.0618, 0.1237, 0.0931, 0.0947, 0.1258, 0.0998, 0.0689, 0.0856, 0.1437,\n",
      "        0.1304, 0.1581, 0.1617, 0.0958, 0.1628, 0.2104, 0.0678, 0.0837, 0.1102,\n",
      "        0.0900, 0.1006, 0.1404, 0.1045, 0.1430, 0.1412, 0.0753, 0.1188, 0.0993,\n",
      "        0.1035, 0.1226, 0.0848, 0.1293, 0.0986, 0.0624, 0.1135, 0.1122, 0.0882,\n",
      "        0.1649, 0.1126, 0.1454, 0.0811, 0.1903, 0.0950, 0.0924, 0.0769, 0.0770,\n",
      "        0.1380, 0.0581, 0.1909, 0.0712, 0.1150, 0.0953, 0.1012, 0.0413, 0.0555,\n",
      "        0.1206, 0.0821, 0.0666, 0.1034, 0.1224, 0.1161, 0.0849, 0.0797, 0.1758,\n",
      "        0.0903, 0.1182, 0.0949, 0.0696, 0.1718, 0.1446, 0.0645, 0.0840, 0.1003,\n",
      "        0.0865, 0.1215, 0.1328, 0.1638, 0.1094, 0.1408, 0.1719, 0.1020, 0.1010,\n",
      "        0.1203, 0.0633, 0.1189, 0.1145, 0.1380, 0.0918, 0.0932, 0.0759, 0.1407,\n",
      "        0.1051, 0.0564, 0.0755, 0.1078, 0.1844, 0.0898, 0.1026, 0.1200, 0.1986,\n",
      "        0.2158, 0.1294, 0.1312, 0.0908, 0.1453, 0.1007, 0.1412, 0.0951, 0.0945,\n",
      "        0.0716, 0.0884, 0.0521, 0.1249, 0.1227, 0.0989, 0.0831, 0.0990, 0.1124,\n",
      "        0.0714, 0.0718, 0.0911, 0.1352, 0.0574, 0.1451, 0.1315, 0.1118, 0.0857,\n",
      "        0.1318, 0.1744, 0.1222, 0.0805, 0.0634, 0.0777, 0.0941, 0.0675, 0.1156,\n",
      "        0.1170, 0.0948, 0.0759, 0.0751, 0.0937, 0.1211, 0.0948, 0.1674, 0.1282,\n",
      "        0.0896, 0.0616, 0.0954, 0.0785, 0.1605, 0.1056, 0.0546, 0.0698, 0.0592,\n",
      "        0.0439, 0.0846, 0.1446, 0.0581, 0.1267, 0.1213, 0.1326, 0.1165, 0.1668,\n",
      "        0.0937, 0.1116, 0.0714, 0.0978, 0.2257, 0.0801, 0.0978, 0.0752, 0.0599,\n",
      "        0.0956, 0.1094, 0.1408, 0.0970, 0.1344, 0.0729, 0.1190, 0.2059, 0.0823])\n",
      "roc_auc score: 0.7144919624426164\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_10.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_10.html. Open it in your browser.\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 11, Loss: 2.5348618030548096\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-2.2809, -2.6098, -2.2930, -2.0234, -2.9304, -2.7862, -2.2935, -2.6446,\n",
      "        -2.3147, -3.3324, -2.7930, -2.3108, -2.0445, -1.8720, -1.9653, -2.3192,\n",
      "        -2.4029, -2.9683, -1.5202, -2.3924, -1.9200, -2.6348, -2.5749, -3.1027,\n",
      "        -2.7740, -1.6855, -2.8516, -2.5598, -2.5605, -2.8027, -3.2796, -2.9846,\n",
      "        -2.8235, -2.9659, -2.2764, -2.4634, -2.9729, -2.1365, -2.4802, -2.4855,\n",
      "        -2.1225, -2.4110, -2.8321, -2.5892, -1.9482, -2.0782, -1.8468, -1.7897,\n",
      "        -2.4481, -1.7847, -1.4584, -2.8621, -2.6110, -2.2848, -2.5319, -2.3981,\n",
      "        -1.9796, -2.3410, -1.9490, -1.9643, -2.7270, -2.1819, -2.4132, -2.3561,\n",
      "        -2.1653, -2.5903, -2.0833, -2.4137, -2.9534, -2.2681, -2.2670, -2.5496,\n",
      "        -1.7748, -2.2576, -1.9351, -2.6438, -1.6028, -2.4646, -2.4828, -2.7112,\n",
      "        -2.6996, -2.0019, -3.0392, -1.5666, -2.7994, -2.2209, -2.4560, -2.3925,\n",
      "        -3.4262, -3.0895, -2.1641, -2.6443, -2.8803, -2.3598, -2.1601, -2.2431,\n",
      "        -2.5970, -2.6670, -1.6757, -2.5193, -2.2161, -2.4591, -2.8282, -1.7334,\n",
      "        -1.9329, -2.9094, -2.6035, -2.4033, -2.5780, -2.1516, -2.0465, -1.7763,\n",
      "        -2.2816, -1.9846, -1.7093, -2.3681, -2.3917, -2.1658, -2.9373, -2.1917,\n",
      "        -2.2572, -1.9936, -2.4959, -2.4807, -2.7153, -1.9708, -2.3282, -3.0677,\n",
      "        -2.7341, -2.3126, -1.6378, -2.5206, -2.3651, -2.1762, -1.5118, -1.4172,\n",
      "        -2.0864, -2.0682, -2.5095, -1.9418, -2.3969, -1.9720, -2.4707, -2.4750,\n",
      "        -2.8010, -2.5462, -3.1588, -2.1237, -2.1409, -2.4153, -2.6198, -2.4098,\n",
      "        -2.2529, -2.7984, -2.7834, -2.5024, -2.0244, -3.0490, -1.9364, -2.0659,\n",
      "        -2.2753, -2.5886, -2.0552, -1.6876, -2.1487, -2.6499, -2.9305, -2.6941,\n",
      "        -2.4775, -2.8612, -2.2124, -2.2040, -2.4596, -2.7278, -2.7398, -2.4716,\n",
      "        -2.1602, -2.4639, -1.7542, -2.1067, -2.5243, -2.9603, -2.4612, -2.6803,\n",
      "        -1.8134, -2.3295, -3.0959, -2.8095, -3.0137, -3.3481, -2.5997, -1.9492,\n",
      "        -3.0319, -2.1112, -2.1816, -2.0595, -2.2233, -1.7647, -2.4852, -2.2630,\n",
      "        -2.8035, -2.4281, -1.3657, -2.6666, -2.4267, -2.7302, -3.0002, -2.4450,\n",
      "        -2.2816, -1.9846, -2.4425, -2.0344, -2.7744, -2.2027, -1.4839, -2.6282],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0927, 0.0685, 0.0917, 0.1168, 0.0507, 0.0581, 0.0917, 0.0663, 0.0899,\n",
      "        0.0345, 0.0577, 0.0902, 0.1146, 0.1333, 0.1229, 0.0895, 0.0830, 0.0489,\n",
      "        0.1794, 0.0838, 0.1279, 0.0669, 0.0708, 0.0430, 0.0587, 0.1564, 0.0546,\n",
      "        0.0718, 0.0717, 0.0572, 0.0363, 0.0481, 0.0561, 0.0490, 0.0931, 0.0785,\n",
      "        0.0487, 0.1056, 0.0773, 0.0769, 0.1069, 0.0823, 0.0556, 0.0698, 0.1247,\n",
      "        0.1112, 0.1363, 0.1431, 0.0796, 0.1437, 0.1887, 0.0541, 0.0684, 0.0924,\n",
      "        0.0737, 0.0833, 0.1214, 0.0878, 0.1247, 0.1230, 0.0614, 0.1014, 0.0822,\n",
      "        0.0866, 0.1029, 0.0698, 0.1107, 0.0821, 0.0496, 0.0938, 0.0939, 0.0725,\n",
      "        0.1449, 0.0947, 0.1262, 0.0664, 0.1676, 0.0784, 0.0771, 0.0623, 0.0630,\n",
      "        0.1190, 0.0457, 0.1727, 0.0574, 0.0979, 0.0790, 0.0837, 0.0315, 0.0435,\n",
      "        0.1030, 0.0663, 0.0531, 0.0863, 0.1034, 0.0959, 0.0693, 0.0649, 0.1577,\n",
      "        0.0745, 0.0983, 0.0788, 0.0558, 0.1501, 0.1264, 0.0517, 0.0689, 0.0829,\n",
      "        0.0706, 0.1042, 0.1144, 0.1448, 0.0927, 0.1208, 0.1533, 0.0856, 0.0838,\n",
      "        0.1029, 0.0503, 0.1005, 0.0947, 0.1199, 0.0761, 0.0772, 0.0621, 0.1223,\n",
      "        0.0888, 0.0445, 0.0610, 0.0901, 0.1628, 0.0744, 0.0859, 0.1019, 0.1807,\n",
      "        0.1951, 0.1104, 0.1122, 0.0752, 0.1254, 0.0834, 0.1222, 0.0779, 0.0776,\n",
      "        0.0573, 0.0727, 0.0407, 0.1068, 0.1052, 0.0820, 0.0679, 0.0824, 0.0951,\n",
      "        0.0574, 0.0582, 0.0757, 0.1167, 0.0453, 0.1260, 0.1125, 0.0932, 0.0699,\n",
      "        0.1135, 0.1561, 0.1045, 0.0660, 0.0507, 0.0633, 0.0775, 0.0541, 0.0986,\n",
      "        0.0994, 0.0787, 0.0614, 0.0607, 0.0779, 0.1034, 0.0784, 0.1475, 0.1084,\n",
      "        0.0742, 0.0493, 0.0786, 0.0641, 0.1402, 0.0887, 0.0433, 0.0568, 0.0468,\n",
      "        0.0340, 0.0692, 0.1246, 0.0460, 0.1080, 0.1014, 0.1131, 0.0977, 0.1462,\n",
      "        0.0769, 0.0942, 0.0571, 0.0811, 0.2033, 0.0650, 0.0812, 0.0612, 0.0474,\n",
      "        0.0798, 0.0927, 0.1208, 0.0800, 0.1156, 0.0587, 0.0995, 0.1848, 0.0673])\n",
      "roc_auc score: 0.7154935279754753\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 12, Loss: 2.420518159866333\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-2.4775, -2.8344, -2.4982, -2.2114, -3.1764, -3.0227, -2.4835, -2.8723,\n",
      "        -2.5061, -3.6051, -3.0120, -2.5050, -2.2359, -2.0494, -2.1315, -2.5277,\n",
      "        -2.5955, -3.2164, -1.6687, -2.5920, -2.0844, -2.8684, -2.7909, -3.3612,\n",
      "        -3.0132, -1.8442, -3.0829, -2.7635, -2.7829, -3.0397, -3.5385, -3.2187,\n",
      "        -3.0672, -3.2157, -2.4621, -2.6702, -3.2224, -2.3159, -2.6854, -2.6984,\n",
      "        -2.3011, -2.6192, -3.0660, -2.8112, -2.1119, -2.2612, -2.0234, -1.9345,\n",
      "        -2.6532, -1.9320, -1.5931, -3.1051, -2.8263, -2.4834, -2.7496, -2.6101,\n",
      "        -2.1438, -2.5393, -2.1112, -2.1225, -2.9483, -2.3596, -2.6247, -2.5551,\n",
      "        -2.3575, -2.8037, -2.2617, -2.6090, -3.1997, -2.4700, -2.4683, -2.7593,\n",
      "        -1.9249, -2.4608, -2.0967, -2.8657, -1.7580, -2.6731, -2.6819, -2.9454,\n",
      "        -2.9264, -2.1727, -3.2934, -1.6903, -3.0343, -2.4188, -2.6593, -2.5990,\n",
      "        -3.7043, -3.3400, -2.3410, -2.8753, -3.1091, -2.5561, -2.3440, -2.4421,\n",
      "        -2.8174, -2.8867, -1.8061, -2.7320, -2.4153, -2.6650, -3.0691, -1.8979,\n",
      "        -2.0870, -3.1428, -2.8295, -2.6171, -2.7972, -2.3296, -2.2237, -1.9223,\n",
      "        -2.4761, -2.1546, -1.8465, -2.5672, -2.6027, -2.3422, -3.1828, -2.3822,\n",
      "        -2.4613, -2.1555, -2.6934, -2.6837, -2.9344, -2.1350, -2.5250, -3.3212,\n",
      "        -2.9733, -2.5043, -1.7866, -2.7183, -2.5645, -2.3650, -1.6314, -1.5446,\n",
      "        -2.2608, -2.2418, -2.7178, -2.1143, -2.6095, -2.1362, -2.6842, -2.6945,\n",
      "        -3.0403, -2.7578, -3.4192, -2.2996, -2.3168, -2.6136, -2.8421, -2.6135,\n",
      "        -2.4334, -3.0338, -3.0092, -2.7005, -2.1927, -3.3022, -2.0995, -2.2504,\n",
      "        -2.4803, -2.8103, -2.2400, -1.8199, -2.3278, -2.8670, -3.1810, -2.9068,\n",
      "        -2.6889, -3.1002, -2.3871, -2.3837, -2.6627, -2.9588, -2.9718, -2.6757,\n",
      "        -2.3521, -2.6789, -1.9023, -2.2982, -2.7290, -3.1976, -2.6657, -2.8903,\n",
      "        -1.9694, -2.5268, -3.3429, -3.0298, -3.2602, -3.6223, -2.8066, -2.1135,\n",
      "        -3.2803, -2.2949, -2.3785, -2.2424, -2.4178, -1.9214, -2.7060, -2.4564,\n",
      "        -3.0499, -2.6368, -1.4997, -2.9022, -2.6310, -2.9527, -3.2501, -2.6397,\n",
      "        -2.4761, -2.1546, -2.6603, -2.2014, -3.0082, -2.3975, -1.6151, -2.8435],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0775, 0.0555, 0.0760, 0.0987, 0.0401, 0.0464, 0.0770, 0.0535, 0.0754,\n",
      "        0.0265, 0.0469, 0.0755, 0.0966, 0.1141, 0.1061, 0.0739, 0.0694, 0.0386,\n",
      "        0.1586, 0.0697, 0.1106, 0.0537, 0.0578, 0.0335, 0.0468, 0.1365, 0.0438,\n",
      "        0.0593, 0.0583, 0.0457, 0.0282, 0.0385, 0.0445, 0.0386, 0.0786, 0.0648,\n",
      "        0.0383, 0.0898, 0.0638, 0.0631, 0.0910, 0.0679, 0.0445, 0.0567, 0.1079,\n",
      "        0.0944, 0.1168, 0.1263, 0.0658, 0.1265, 0.1689, 0.0429, 0.0559, 0.0770,\n",
      "        0.0601, 0.0685, 0.1049, 0.0731, 0.1080, 0.1069, 0.0498, 0.0863, 0.0676,\n",
      "        0.0721, 0.0865, 0.0571, 0.0943, 0.0686, 0.0392, 0.0780, 0.0781, 0.0596,\n",
      "        0.1273, 0.0787, 0.1094, 0.0539, 0.1470, 0.0646, 0.0640, 0.0500, 0.0509,\n",
      "        0.1022, 0.0358, 0.1557, 0.0459, 0.0817, 0.0654, 0.0692, 0.0240, 0.0342,\n",
      "        0.0878, 0.0534, 0.0427, 0.0720, 0.0875, 0.0800, 0.0564, 0.0528, 0.1411,\n",
      "        0.0611, 0.0820, 0.0651, 0.0444, 0.1304, 0.1104, 0.0414, 0.0558, 0.0680,\n",
      "        0.0575, 0.0887, 0.0976, 0.1276, 0.0776, 0.1039, 0.1363, 0.0713, 0.0690,\n",
      "        0.0877, 0.0398, 0.0845, 0.0786, 0.1038, 0.0634, 0.0639, 0.0505, 0.1057,\n",
      "        0.0741, 0.0348, 0.0486, 0.0756, 0.1435, 0.0619, 0.0715, 0.0859, 0.1636,\n",
      "        0.1759, 0.0944, 0.0961, 0.0619, 0.1077, 0.0685, 0.1056, 0.0639, 0.0633,\n",
      "        0.0456, 0.0596, 0.0317, 0.0912, 0.0897, 0.0683, 0.0551, 0.0683, 0.0807,\n",
      "        0.0459, 0.0470, 0.0629, 0.1004, 0.0355, 0.1091, 0.0953, 0.0772, 0.0568,\n",
      "        0.0962, 0.1394, 0.0888, 0.0538, 0.0399, 0.0518, 0.0636, 0.0431, 0.0842,\n",
      "        0.0844, 0.0652, 0.0493, 0.0487, 0.0644, 0.0869, 0.0642, 0.1299, 0.0913,\n",
      "        0.0613, 0.0393, 0.0650, 0.0526, 0.1225, 0.0740, 0.0341, 0.0461, 0.0370,\n",
      "        0.0260, 0.0570, 0.1078, 0.0363, 0.0915, 0.0848, 0.0960, 0.0818, 0.1277,\n",
      "        0.0626, 0.0790, 0.0452, 0.0668, 0.1825, 0.0520, 0.0672, 0.0496, 0.0373,\n",
      "        0.0666, 0.0776, 0.1039, 0.0654, 0.0996, 0.0471, 0.0834, 0.1659, 0.0550])\n",
      "roc_auc score: 0.7159482161013032\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 13, Loss: 2.3098928928375244\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-2.6744, -3.0612, -2.7059, -2.3992, -3.4251, -3.2581, -2.6724, -3.0898,\n",
      "        -2.7002, -3.8803, -3.2314, -2.7073, -2.4262, -2.2289, -2.3066, -2.7386,\n",
      "        -2.7889, -3.4588, -1.8146, -2.7947, -2.2498, -3.1049, -3.0115, -3.6223,\n",
      "        -3.2602, -1.9990, -3.3118, -2.9679, -3.0019, -3.2836, -3.7995, -3.4512,\n",
      "        -3.2977, -3.4618, -2.6473, -2.8734, -3.4737, -2.4950, -2.8913, -2.9057,\n",
      "        -2.4778, -2.8264, -3.2988, -3.0269, -2.2753, -2.4517, -2.1971, -2.0779,\n",
      "        -2.8625, -2.0799, -1.7251, -3.3504, -3.0390, -2.6862, -2.9644, -2.8283,\n",
      "        -2.2977, -2.7400, -2.2716, -2.2821, -3.1712, -2.5345, -2.8436, -2.7564,\n",
      "        -2.5483, -3.0185, -2.4490, -2.7969, -3.4484, -2.6648, -2.6712, -2.9705,\n",
      "        -2.0783, -2.6583, -2.2579, -3.0947, -1.9097, -2.8803, -2.8817, -3.1880,\n",
      "        -3.1524, -2.3480, -3.5449, -1.8146, -3.2716, -2.6145, -2.8642, -2.8020,\n",
      "        -3.9836, -3.5912, -2.5176, -3.0941, -3.3289, -2.7521, -2.5382, -2.6341,\n",
      "        -3.0360, -3.1081, -1.9372, -2.9518, -2.6091, -2.8704, -3.3204, -2.0574,\n",
      "        -2.2380, -3.3692, -3.0566, -2.8214, -3.0126, -2.5130, -2.4073, -2.0661,\n",
      "        -2.6728, -2.3250, -1.9797, -2.7686, -2.8190, -2.5128, -3.4308, -2.5625,\n",
      "        -2.6529, -2.3176, -2.8851, -2.8889, -3.1547, -2.2924, -2.7241, -3.5733,\n",
      "        -3.2152, -2.6899, -1.9361, -2.9135, -2.7662, -2.5547, -1.7517, -1.6725,\n",
      "        -2.4308, -2.4259, -2.9261, -2.2886, -2.8273, -2.2922, -2.8957, -2.9199,\n",
      "        -3.2820, -2.9709, -3.6822, -2.4730, -2.4886, -2.8063, -3.0626, -2.8195,\n",
      "        -2.6122, -3.2716, -3.2365, -2.8993, -2.3575, -3.5578, -2.2627, -2.4402,\n",
      "        -2.6878, -3.0343, -2.4249, -1.9518, -2.5187, -3.0857, -3.4354, -3.1175,\n",
      "        -2.8893, -3.3389, -2.5677, -2.5590, -2.8675, -3.1922, -3.2061, -2.8814,\n",
      "        -2.5449, -2.8885, -2.0542, -2.4802, -2.9420, -3.4368, -2.8628, -3.0984,\n",
      "        -2.1303, -2.7245, -3.5870, -3.2489, -3.5028, -3.8949, -3.0138, -2.2826,\n",
      "        -3.5370, -2.4740, -2.5630, -2.4177, -2.6101, -2.0786, -2.9314, -2.6623,\n",
      "        -3.2928, -2.8478, -1.6388, -3.1405, -2.8390, -3.1768, -3.4975, -2.8318,\n",
      "        -2.6728, -2.3250, -2.8824, -2.3617, -3.2444, -2.5903, -1.7469, -3.0602],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0645, 0.0447, 0.0626, 0.0832, 0.0315, 0.0370, 0.0646, 0.0435, 0.0630,\n",
      "        0.0202, 0.0380, 0.0625, 0.0812, 0.0972, 0.0906, 0.0607, 0.0579, 0.0305,\n",
      "        0.1401, 0.0576, 0.0954, 0.0429, 0.0469, 0.0260, 0.0370, 0.1193, 0.0352,\n",
      "        0.0489, 0.0473, 0.0361, 0.0219, 0.0307, 0.0357, 0.0304, 0.0662, 0.0535,\n",
      "        0.0301, 0.0762, 0.0526, 0.0519, 0.0774, 0.0559, 0.0356, 0.0462, 0.0932,\n",
      "        0.0793, 0.1000, 0.1113, 0.0540, 0.1111, 0.1512, 0.0339, 0.0457, 0.0638,\n",
      "        0.0491, 0.0558, 0.0913, 0.0607, 0.0935, 0.0926, 0.0403, 0.0735, 0.0550,\n",
      "        0.0597, 0.0725, 0.0466, 0.0795, 0.0575, 0.0308, 0.0651, 0.0647, 0.0488,\n",
      "        0.1112, 0.0655, 0.0947, 0.0433, 0.1290, 0.0531, 0.0531, 0.0396, 0.0410,\n",
      "        0.0872, 0.0281, 0.1401, 0.0366, 0.0682, 0.0539, 0.0572, 0.0183, 0.0268,\n",
      "        0.0746, 0.0433, 0.0346, 0.0600, 0.0732, 0.0670, 0.0458, 0.0428, 0.1260,\n",
      "        0.0497, 0.0686, 0.0536, 0.0349, 0.1133, 0.0964, 0.0333, 0.0449, 0.0562,\n",
      "        0.0469, 0.0749, 0.0826, 0.1124, 0.0646, 0.0891, 0.1213, 0.0590, 0.0563,\n",
      "        0.0750, 0.0313, 0.0716, 0.0658, 0.0897, 0.0529, 0.0527, 0.0409, 0.0918,\n",
      "        0.0616, 0.0273, 0.0386, 0.0636, 0.1261, 0.0515, 0.0592, 0.0721, 0.1478,\n",
      "        0.1581, 0.0809, 0.0812, 0.0509, 0.0921, 0.0559, 0.0918, 0.0524, 0.0512,\n",
      "        0.0362, 0.0488, 0.0245, 0.0778, 0.0767, 0.0570, 0.0447, 0.0563, 0.0684,\n",
      "        0.0366, 0.0378, 0.0522, 0.0865, 0.0277, 0.0943, 0.0802, 0.0637, 0.0459,\n",
      "        0.0813, 0.1244, 0.0746, 0.0437, 0.0312, 0.0424, 0.0527, 0.0343, 0.0712,\n",
      "        0.0718, 0.0538, 0.0395, 0.0389, 0.0531, 0.0728, 0.0527, 0.1136, 0.0773,\n",
      "        0.0501, 0.0312, 0.0540, 0.0432, 0.1062, 0.0615, 0.0269, 0.0374, 0.0292,\n",
      "        0.0199, 0.0468, 0.0926, 0.0283, 0.0777, 0.0716, 0.0818, 0.0685, 0.1112,\n",
      "        0.0506, 0.0652, 0.0358, 0.0548, 0.1626, 0.0415, 0.0553, 0.0400, 0.0294,\n",
      "        0.0556, 0.0646, 0.0891, 0.0530, 0.0861, 0.0375, 0.0698, 0.1484, 0.0448])\n",
      "roc_auc score: 0.7157893520658101\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 14, Loss: 2.1965298652648926\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-2.8703, -3.2873, -2.9153, -2.5830, -3.6762, -3.4920, -2.8546, -3.3005,\n",
      "        -2.8879, -4.1474, -3.4408, -2.9070, -2.6133, -2.4027, -2.4822, -2.9514,\n",
      "        -2.9773, -3.7018, -1.9586, -3.0047, -2.4146, -3.3433, -3.2324, -3.8778,\n",
      "        -3.5093, -2.1538, -3.5349, -3.1644, -3.2219, -3.5317, -4.0496, -3.6760,\n",
      "        -3.5217, -3.6984, -2.8245, -3.0709, -3.7284, -2.6771, -3.0974, -3.1031,\n",
      "        -2.6382, -3.0330, -3.5316, -3.2510, -2.4462, -2.6394, -2.3676, -2.2207,\n",
      "        -3.0770, -2.2286, -1.8572, -3.5952, -3.2470, -2.8919, -3.1781, -3.0485,\n",
      "        -2.4541, -2.9423, -2.4269, -2.4425, -3.3928, -2.7052, -3.0644, -2.9592,\n",
      "        -2.7267, -3.2337, -2.6436, -2.9778, -3.6966, -2.8474, -2.8708, -3.1734,\n",
      "        -2.2324, -2.8523, -2.4196, -3.3244, -2.0600, -3.0945, -3.0757, -3.4327,\n",
      "        -3.3771, -2.5207, -3.7938, -1.9338, -3.5031, -2.8062, -3.0694, -3.0043,\n",
      "        -4.2561, -3.8394, -2.6904, -3.3054, -3.5447, -2.9430, -2.7404, -2.8186,\n",
      "        -3.2538, -3.3251, -2.0684, -3.1734, -2.7924, -3.0851, -3.5737, -2.2167,\n",
      "        -2.3912, -3.5965, -3.2807, -3.0208, -3.2342, -2.6929, -2.5967, -2.2088,\n",
      "        -2.8712, -2.4869, -2.1120, -2.9753, -3.0374, -2.6775, -3.6788, -2.7380,\n",
      "        -2.8336, -2.4737, -3.0709, -3.0966, -3.3766, -2.4471, -2.9274, -3.8218,\n",
      "        -3.4593, -2.8721, -2.0858, -3.1046, -2.9698, -2.7426, -1.8689, -1.8005,\n",
      "        -2.5978, -2.6169, -3.1359, -2.4647, -3.0471, -2.4482, -3.1059, -3.1496,\n",
      "        -3.5229, -3.1761, -3.9402, -2.6415, -2.6640, -2.9985, -3.2845, -3.0292,\n",
      "        -2.7846, -3.5049, -3.4575, -3.0876, -2.5222, -3.8074, -2.4327, -2.6269,\n",
      "        -2.9018, -3.2597, -2.6092, -2.0813, -2.7176, -3.3042, -3.6890, -3.3218,\n",
      "        -3.0875, -3.5739, -2.7495, -2.7327, -3.0719, -3.4251, -3.4369, -3.0835,\n",
      "        -2.7400, -3.0959, -2.2073, -2.6531, -3.1587, -3.6695, -3.0596, -3.2995,\n",
      "        -2.2913, -2.9182, -3.8255, -3.4556, -3.7469, -4.1634, -3.2206, -2.4562,\n",
      "        -3.8018, -2.6446, -2.7424, -2.5990, -2.8139, -2.2393, -3.1622, -2.8690,\n",
      "        -3.5284, -3.0603, -1.7759, -3.3809, -3.0454, -3.3939, -3.7424, -3.0215,\n",
      "        -2.8712, -2.4869, -3.1065, -2.5221, -3.4751, -2.7701, -1.8788, -3.2772],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0536, 0.0360, 0.0514, 0.0702, 0.0247, 0.0295, 0.0544, 0.0356, 0.0528,\n",
      "        0.0156, 0.0310, 0.0518, 0.0683, 0.0830, 0.0771, 0.0497, 0.0485, 0.0241,\n",
      "        0.1236, 0.0472, 0.0821, 0.0341, 0.0380, 0.0203, 0.0290, 0.1040, 0.0283,\n",
      "        0.0405, 0.0383, 0.0284, 0.0171, 0.0247, 0.0287, 0.0242, 0.0560, 0.0443,\n",
      "        0.0235, 0.0643, 0.0432, 0.0430, 0.0667, 0.0460, 0.0284, 0.0373, 0.0797,\n",
      "        0.0666, 0.0857, 0.0979, 0.0441, 0.0972, 0.1350, 0.0267, 0.0374, 0.0526,\n",
      "        0.0400, 0.0453, 0.0791, 0.0501, 0.0811, 0.0800, 0.0325, 0.0627, 0.0446,\n",
      "        0.0493, 0.0614, 0.0379, 0.0664, 0.0484, 0.0242, 0.0548, 0.0536, 0.0402,\n",
      "        0.0969, 0.0546, 0.0817, 0.0347, 0.1130, 0.0433, 0.0441, 0.0313, 0.0330,\n",
      "        0.0744, 0.0220, 0.1263, 0.0292, 0.0570, 0.0444, 0.0472, 0.0140, 0.0211,\n",
      "        0.0635, 0.0354, 0.0281, 0.0501, 0.0606, 0.0563, 0.0372, 0.0347, 0.1122,\n",
      "        0.0402, 0.0577, 0.0437, 0.0273, 0.0983, 0.0838, 0.0267, 0.0362, 0.0465,\n",
      "        0.0379, 0.0634, 0.0693, 0.0990, 0.0536, 0.0768, 0.1079, 0.0486, 0.0458,\n",
      "        0.0643, 0.0246, 0.0608, 0.0555, 0.0777, 0.0443, 0.0432, 0.0330, 0.0796,\n",
      "        0.0508, 0.0214, 0.0305, 0.0536, 0.1105, 0.0429, 0.0488, 0.0605, 0.1337,\n",
      "        0.1418, 0.0693, 0.0681, 0.0416, 0.0784, 0.0453, 0.0796, 0.0429, 0.0411,\n",
      "        0.0287, 0.0401, 0.0191, 0.0665, 0.0651, 0.0475, 0.0361, 0.0461, 0.0582,\n",
      "        0.0292, 0.0305, 0.0436, 0.0743, 0.0217, 0.0807, 0.0674, 0.0521, 0.0370,\n",
      "        0.0686, 0.1109, 0.0619, 0.0354, 0.0244, 0.0348, 0.0436, 0.0273, 0.0601,\n",
      "        0.0611, 0.0443, 0.0315, 0.0312, 0.0438, 0.0607, 0.0433, 0.0991, 0.0658,\n",
      "        0.0408, 0.0249, 0.0448, 0.0356, 0.0918, 0.0513, 0.0213, 0.0306, 0.0230,\n",
      "        0.0153, 0.0384, 0.0790, 0.0218, 0.0663, 0.0605, 0.0692, 0.0566, 0.0963,\n",
      "        0.0406, 0.0537, 0.0285, 0.0448, 0.1448, 0.0329, 0.0454, 0.0325, 0.0231,\n",
      "        0.0465, 0.0536, 0.0768, 0.0428, 0.0743, 0.0300, 0.0590, 0.1325, 0.0364])\n",
      "roc_auc score: 0.7151305477246819\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 15, Loss: 2.1052727699279785\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-3.0654, -3.5062, -3.1272, -2.7656, -3.9245, -3.7245, -3.0210, -3.5094,\n",
      "        -3.0739, -4.4134, -3.6424, -3.1052, -2.8004, -2.5694, -2.6530, -3.1679,\n",
      "        -3.1594, -3.9435, -2.1016, -3.2085, -2.5785, -3.5762, -3.4491, -4.1298,\n",
      "        -3.7566, -2.3075, -3.7579, -3.3528, -3.4407, -3.7769, -4.2991, -3.8940,\n",
      "        -3.7444, -3.9441, -3.0003, -3.2583, -3.9792, -2.8602, -3.3110, -3.2997,\n",
      "        -2.7968, -3.2411, -3.7635, -3.4688, -2.6177, -2.8255, -2.5308, -2.3598,\n",
      "        -3.2843, -2.3771, -1.9902, -3.8318, -3.4549, -3.0970, -3.3910, -3.2673,\n",
      "        -2.6102, -3.1418, -2.5778, -2.6026, -3.6067, -2.8661, -3.2839, -3.1583,\n",
      "        -2.9017, -3.4397, -2.8385, -3.1561, -3.9368, -3.0279, -3.0693, -3.3761,\n",
      "        -2.3850, -3.0456, -2.5898, -3.5471, -2.2093, -3.3065, -3.2626, -3.6741,\n",
      "        -3.6009, -2.6908, -4.0417, -2.0525, -3.7321, -2.9974, -3.2661, -3.2059,\n",
      "        -4.5273, -4.0865, -2.8584, -3.5155, -3.7566, -3.1264, -2.9384, -2.9999,\n",
      "        -3.4709, -3.5371, -2.1972, -3.3913, -2.9706, -3.2994, -3.8250, -2.3748,\n",
      "        -2.5413, -3.8236, -3.5036, -3.2288, -3.4573, -2.8747, -2.7867, -2.3509,\n",
      "        -3.0730, -2.6485, -2.2444, -3.1782, -3.2543, -2.8360, -3.9184, -2.9101,\n",
      "        -3.0135, -2.6276, -3.2563, -3.3036, -3.5977, -2.6018, -3.1298, -4.0690,\n",
      "        -3.7016, -3.0518, -2.2344, -3.2877, -3.1706, -2.9300, -1.9837, -1.9317,\n",
      "        -2.7630, -2.8046, -3.3450, -2.6441, -3.2654, -2.6039, -3.3153, -3.3777,\n",
      "        -3.7561, -3.3806, -4.1942, -2.7974, -2.8399, -3.1858, -3.5052, -3.2401,\n",
      "        -2.9502, -3.7344, -3.6754, -3.2703, -2.6864, -4.0542, -2.6053, -2.8126,\n",
      "        -3.1149, -3.4814, -2.7923, -2.2096, -2.9166, -3.5141, -3.9351, -3.5206,\n",
      "        -3.2829, -3.8080, -2.9309, -2.8999, -3.2684, -3.6503, -3.6630, -3.2808,\n",
      "        -2.9283, -3.3026, -2.3597, -2.8242, -3.3743, -3.8983, -3.2554, -3.4932,\n",
      "        -2.4546, -3.1056, -4.0636, -3.6585, -3.9901, -4.4308, -3.4230, -2.6303,\n",
      "        -4.0632, -2.8133, -2.9163, -2.7803, -3.0170, -2.3938, -3.3913, -3.0734,\n",
      "        -3.7625, -3.2682, -1.9111, -3.6187, -3.2511, -3.6087, -3.9861, -3.2036,\n",
      "        -3.0730, -2.6485, -3.3292, -2.6822, -3.7030, -2.9470, -2.0153, -3.4850],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0446, 0.0291, 0.0420, 0.0592, 0.0194, 0.0236, 0.0465, 0.0290, 0.0442,\n",
      "        0.0120, 0.0255, 0.0429, 0.0573, 0.0711, 0.0658, 0.0404, 0.0407, 0.0190,\n",
      "        0.1089, 0.0388, 0.0705, 0.0272, 0.0308, 0.0158, 0.0228, 0.0905, 0.0228,\n",
      "        0.0338, 0.0310, 0.0224, 0.0134, 0.0200, 0.0231, 0.0190, 0.0474, 0.0370,\n",
      "        0.0184, 0.0542, 0.0352, 0.0356, 0.0575, 0.0376, 0.0227, 0.0302, 0.0680,\n",
      "        0.0560, 0.0737, 0.0863, 0.0361, 0.0849, 0.1202, 0.0212, 0.0306, 0.0432,\n",
      "        0.0326, 0.0367, 0.0685, 0.0414, 0.0706, 0.0690, 0.0264, 0.0539, 0.0361,\n",
      "        0.0408, 0.0521, 0.0311, 0.0553, 0.0409, 0.0191, 0.0462, 0.0444, 0.0331,\n",
      "        0.0843, 0.0454, 0.0698, 0.0280, 0.0989, 0.0354, 0.0369, 0.0247, 0.0266,\n",
      "        0.0635, 0.0173, 0.1138, 0.0234, 0.0475, 0.0368, 0.0389, 0.0107, 0.0165,\n",
      "        0.0542, 0.0289, 0.0228, 0.0420, 0.0503, 0.0474, 0.0302, 0.0283, 0.1000,\n",
      "        0.0326, 0.0488, 0.0356, 0.0214, 0.0851, 0.0730, 0.0214, 0.0292, 0.0381,\n",
      "        0.0306, 0.0534, 0.0580, 0.0870, 0.0442, 0.0661, 0.0958, 0.0400, 0.0372,\n",
      "        0.0554, 0.0195, 0.0517, 0.0468, 0.0674, 0.0371, 0.0354, 0.0267, 0.0690,\n",
      "        0.0419, 0.0168, 0.0241, 0.0451, 0.0967, 0.0360, 0.0403, 0.0507, 0.1209,\n",
      "        0.1266, 0.0594, 0.0571, 0.0341, 0.0664, 0.0368, 0.0689, 0.0350, 0.0330,\n",
      "        0.0228, 0.0329, 0.0149, 0.0575, 0.0552, 0.0397, 0.0292, 0.0377, 0.0497,\n",
      "        0.0233, 0.0247, 0.0366, 0.0638, 0.0171, 0.0688, 0.0566, 0.0425, 0.0298,\n",
      "        0.0577, 0.0989, 0.0513, 0.0289, 0.0192, 0.0287, 0.0362, 0.0217, 0.0506,\n",
      "        0.0522, 0.0367, 0.0253, 0.0250, 0.0362, 0.0508, 0.0355, 0.0863, 0.0560,\n",
      "        0.0331, 0.0199, 0.0371, 0.0295, 0.0791, 0.0429, 0.0169, 0.0251, 0.0182,\n",
      "        0.0118, 0.0316, 0.0672, 0.0169, 0.0566, 0.0514, 0.0584, 0.0467, 0.0836,\n",
      "        0.0326, 0.0442, 0.0227, 0.0367, 0.1289, 0.0261, 0.0373, 0.0264, 0.0182,\n",
      "        0.0390, 0.0442, 0.0661, 0.0346, 0.0640, 0.0241, 0.0499, 0.1176, 0.0297])\n",
      "roc_auc score: 0.7144917217395322\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_15.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_15.html. Open it in your browser.\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 16, Loss: 2.005584239959717\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-3.2562, -3.7205, -3.3347, -2.9476, -4.1656, -3.9525, -3.1839, -3.7143,\n",
      "        -3.2576, -4.6743, -3.8441, -3.2992, -2.9859, -2.7343, -2.8288, -3.3786,\n",
      "        -3.3375, -4.1805, -2.2418, -3.4077, -2.7352, -3.7995, -3.6613, -4.3768,\n",
      "        -3.9956, -2.4581, -3.9797, -3.5409, -3.6549, -4.0142, -4.5470, -4.1083,\n",
      "        -3.9648, -4.1847, -3.1747, -3.4400, -4.2251, -3.0328, -3.5204, -3.4946,\n",
      "        -2.9512, -3.4447, -3.9908, -3.6852, -2.7927, -3.0073, -2.6952, -2.4908,\n",
      "        -3.4861, -2.5262, -2.1251, -4.0639, -3.6616, -3.2912, -3.5996, -3.4750,\n",
      "        -2.7665, -3.3368, -2.7192, -2.7643, -3.8195, -3.0276, -3.4920, -3.3538,\n",
      "        -3.0756, -3.6446, -3.0223, -3.3326, -4.1722, -3.2070, -3.2633, -3.5777,\n",
      "        -2.5318, -3.2346, -2.7628, -3.7651, -2.3556, -3.5115, -3.4465, -3.9032,\n",
      "        -3.8202, -2.8528, -4.2845, -2.1706, -3.9567, -3.1842, -3.4618, -3.4034,\n",
      "        -4.7934, -4.3346, -3.0237, -3.7247, -3.9638, -3.2984, -3.1270, -3.1798,\n",
      "        -3.6836, -3.7480, -2.3279, -3.5998, -3.1476, -3.5090, -4.0624, -2.5296,\n",
      "        -2.6905, -4.0493, -3.7218, -3.4327, -3.6732, -3.0602, -2.9657, -2.4853,\n",
      "        -3.2666, -2.8091, -2.3745, -3.3740, -3.4633, -2.9951, -4.1531, -3.0794,\n",
      "        -3.1919, -2.7757, -3.4376, -3.5060, -3.8145, -2.7564, -3.3232, -4.3113,\n",
      "        -3.9302, -3.2288, -2.3794, -3.4711, -3.3666, -3.1130, -2.0981, -2.0625,\n",
      "        -2.9281, -2.9850, -3.5498, -2.8163, -3.4710, -2.7597, -3.5205, -3.6000,\n",
      "        -3.9848, -3.5839, -4.4433, -2.9466, -3.0135, -3.3721, -3.7213, -3.4403,\n",
      "        -3.1163, -3.9594, -3.8922, -3.4535, -2.8524, -4.2963, -2.7792, -2.9938,\n",
      "        -3.3222, -3.6931, -2.9712, -2.3398, -3.1037, -3.7229, -4.1757, -3.7161,\n",
      "        -3.4746, -4.0378, -3.1100, -3.0558, -3.4639, -3.8712, -3.8846, -3.4772,\n",
      "        -3.1120, -3.5050, -2.5057, -2.9937, -3.5857, -4.1259, -3.4472, -3.6871,\n",
      "        -2.6124, -3.2875, -4.3004, -3.8616, -4.2286, -4.6929, -3.6221, -2.8010,\n",
      "        -4.3115, -2.9776, -3.0888, -2.9618, -3.2149, -2.5405, -3.6111, -3.2701,\n",
      "        -3.9920, -3.4729, -2.0434, -3.8439, -3.4521, -3.8223, -4.2249, -3.3835,\n",
      "        -3.2666, -2.8091, -3.5455, -2.8412, -3.9265, -3.1227, -2.1486, -3.6918],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0371, 0.0236, 0.0344, 0.0498, 0.0153, 0.0188, 0.0398, 0.0238, 0.0371,\n",
      "        0.0092, 0.0210, 0.0356, 0.0481, 0.0610, 0.0558, 0.0330, 0.0343, 0.0151,\n",
      "        0.0961, 0.0321, 0.0609, 0.0219, 0.0251, 0.0124, 0.0181, 0.0788, 0.0183,\n",
      "        0.0282, 0.0252, 0.0177, 0.0105, 0.0162, 0.0186, 0.0150, 0.0401, 0.0311,\n",
      "        0.0144, 0.0460, 0.0287, 0.0295, 0.0497, 0.0309, 0.0182, 0.0245, 0.0577,\n",
      "        0.0471, 0.0633, 0.0765, 0.0297, 0.0740, 0.1067, 0.0169, 0.0250, 0.0359,\n",
      "        0.0266, 0.0300, 0.0592, 0.0343, 0.0618, 0.0593, 0.0215, 0.0462, 0.0295,\n",
      "        0.0338, 0.0441, 0.0255, 0.0464, 0.0345, 0.0152, 0.0389, 0.0369, 0.0272,\n",
      "        0.0737, 0.0379, 0.0594, 0.0226, 0.0866, 0.0290, 0.0309, 0.0198, 0.0215,\n",
      "        0.0545, 0.0136, 0.1024, 0.0188, 0.0398, 0.0304, 0.0322, 0.0082, 0.0129,\n",
      "        0.0464, 0.0236, 0.0186, 0.0356, 0.0420, 0.0399, 0.0245, 0.0230, 0.0888,\n",
      "        0.0266, 0.0412, 0.0291, 0.0169, 0.0738, 0.0635, 0.0171, 0.0236, 0.0313,\n",
      "        0.0248, 0.0448, 0.0490, 0.0769, 0.0367, 0.0568, 0.0851, 0.0331, 0.0304,\n",
      "        0.0476, 0.0155, 0.0440, 0.0395, 0.0586, 0.0311, 0.0291, 0.0216, 0.0597,\n",
      "        0.0348, 0.0132, 0.0193, 0.0381, 0.0848, 0.0301, 0.0334, 0.0426, 0.1093,\n",
      "        0.1128, 0.0508, 0.0481, 0.0279, 0.0565, 0.0301, 0.0595, 0.0287, 0.0266,\n",
      "        0.0183, 0.0270, 0.0116, 0.0499, 0.0468, 0.0332, 0.0236, 0.0311, 0.0424,\n",
      "        0.0187, 0.0200, 0.0307, 0.0546, 0.0134, 0.0585, 0.0477, 0.0348, 0.0243,\n",
      "        0.0487, 0.0879, 0.0430, 0.0236, 0.0151, 0.0238, 0.0300, 0.0173, 0.0427,\n",
      "        0.0450, 0.0304, 0.0204, 0.0201, 0.0300, 0.0426, 0.0292, 0.0755, 0.0477,\n",
      "        0.0270, 0.0159, 0.0309, 0.0244, 0.0683, 0.0360, 0.0134, 0.0206, 0.0144,\n",
      "        0.0091, 0.0260, 0.0573, 0.0132, 0.0484, 0.0436, 0.0492, 0.0386, 0.0731,\n",
      "        0.0263, 0.0366, 0.0181, 0.0301, 0.1147, 0.0210, 0.0307, 0.0214, 0.0144,\n",
      "        0.0328, 0.0367, 0.0568, 0.0280, 0.0551, 0.0193, 0.0422, 0.1045, 0.0243])\n",
      "roc_auc score: 0.7140639923591213\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 17, Loss: 1.911537766456604\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-3.4430, -3.9304, -3.5356, -3.1208, -4.4025, -4.1808, -3.3446, -3.9174,\n",
      "        -3.4336, -4.9300, -4.0457, -3.4889, -3.1666, -2.8960, -3.0009, -3.5814,\n",
      "        -3.5157, -4.4129, -2.3799, -3.6030, -2.8849, -4.0179, -3.8695, -4.6191,\n",
      "        -4.2239, -2.6065, -4.2001, -3.7290, -3.8644, -4.2469, -4.7933, -4.3222,\n",
      "        -4.1829, -4.4210, -3.3478, -3.6183, -4.4660, -3.1972, -3.7255, -3.6877,\n",
      "        -3.1052, -3.6447, -4.2136, -3.8988, -2.9638, -3.1846, -2.8582, -2.6206,\n",
      "        -3.6840, -2.6783, -2.2580, -4.2913, -3.8673, -3.4816, -3.8039, -3.6764,\n",
      "        -2.9215, -3.5280, -2.8592, -2.9262, -4.0314, -3.1896, -3.6941, -3.5456,\n",
      "        -3.2482, -3.8485, -3.2015, -3.5092, -4.4030, -3.3847, -3.4529, -3.7783,\n",
      "        -2.6796, -3.4196, -2.9315, -3.9790, -2.4997, -3.7126, -3.6304, -4.1274,\n",
      "        -4.0352, -3.0112, -4.5226, -2.2883, -4.1769, -3.3672, -3.6565, -3.5968,\n",
      "        -5.0604, -4.5783, -3.1878, -3.9325, -4.1708, -3.4678, -3.3122, -3.3584,\n",
      "        -3.8920, -3.9578, -2.4584, -3.8044, -3.3234, -3.7144, -4.2946, -2.6848,\n",
      "        -2.8389, -4.2737, -3.9352, -3.6329, -3.8870, -3.2418, -3.1413, -2.6162,\n",
      "        -3.4539, -2.9676, -2.5012, -3.5661, -3.6629, -3.1547, -4.3832, -3.2483,\n",
      "        -3.3690, -2.9179, -3.6191, -3.7045, -4.0269, -2.9093, -3.5129, -4.5491,\n",
      "        -4.1540, -3.4056, -2.5202, -3.6547, -3.5588, -3.2925, -2.2122, -2.1915,\n",
      "        -3.0917, -3.1621, -3.7504, -2.9843, -3.6720, -2.9141, -3.7215, -3.8140,\n",
      "        -4.2089, -3.7863, -4.6875, -3.0959, -3.1804, -3.5556, -3.9329, -3.6366,\n",
      "        -3.2828, -4.1800, -4.1080, -3.6369, -3.0171, -4.5339, -2.9498, -3.1707,\n",
      "        -3.5240, -3.9004, -3.1458, -2.4710, -3.2866, -3.9308, -4.4115, -3.9116,\n",
      "        -3.6637, -4.2684, -3.2888, -3.2105, -3.6586, -4.0876, -4.1018, -3.6726,\n",
      "        -3.2921, -3.7032, -2.6561, -3.1619, -3.7938, -4.3521, -3.6346, -3.8810,\n",
      "        -2.7652, -3.4629, -4.5357, -4.0646, -4.4655, -4.9501, -3.8190, -2.9693,\n",
      "        -4.5570, -3.1407, -3.2601, -3.1399, -3.4071, -2.6853, -3.8219, -3.4622,\n",
      "        -4.2169, -3.6735, -2.1737, -4.0642, -3.6493, -4.0347, -4.4592, -3.5636,\n",
      "        -3.4539, -2.9676, -3.7506, -2.9987, -4.1456, -3.2972, -2.2788, -3.8976],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0310, 0.0193, 0.0283, 0.0423, 0.0121, 0.0151, 0.0341, 0.0195, 0.0313,\n",
      "        0.0072, 0.0172, 0.0296, 0.0404, 0.0524, 0.0474, 0.0271, 0.0289, 0.0120,\n",
      "        0.0847, 0.0265, 0.0529, 0.0177, 0.0204, 0.0098, 0.0144, 0.0687, 0.0148,\n",
      "        0.0235, 0.0205, 0.0141, 0.0082, 0.0131, 0.0150, 0.0119, 0.0340, 0.0261,\n",
      "        0.0114, 0.0393, 0.0235, 0.0244, 0.0429, 0.0255, 0.0146, 0.0199, 0.0491,\n",
      "        0.0397, 0.0543, 0.0678, 0.0245, 0.0643, 0.0947, 0.0135, 0.0205, 0.0298,\n",
      "        0.0218, 0.0247, 0.0511, 0.0285, 0.0542, 0.0509, 0.0174, 0.0396, 0.0243,\n",
      "        0.0280, 0.0374, 0.0209, 0.0391, 0.0291, 0.0121, 0.0328, 0.0307, 0.0224,\n",
      "        0.0642, 0.0317, 0.0506, 0.0184, 0.0759, 0.0238, 0.0258, 0.0159, 0.0174,\n",
      "        0.0469, 0.0107, 0.0921, 0.0151, 0.0333, 0.0252, 0.0267, 0.0063, 0.0102,\n",
      "        0.0396, 0.0192, 0.0152, 0.0302, 0.0352, 0.0336, 0.0200, 0.0187, 0.0788,\n",
      "        0.0218, 0.0348, 0.0238, 0.0135, 0.0639, 0.0553, 0.0137, 0.0192, 0.0258,\n",
      "        0.0201, 0.0376, 0.0414, 0.0681, 0.0307, 0.0489, 0.0758, 0.0275, 0.0250,\n",
      "        0.0409, 0.0123, 0.0374, 0.0333, 0.0513, 0.0261, 0.0240, 0.0175, 0.0517,\n",
      "        0.0289, 0.0105, 0.0155, 0.0321, 0.0745, 0.0252, 0.0277, 0.0358, 0.0987,\n",
      "        0.1005, 0.0435, 0.0406, 0.0230, 0.0481, 0.0248, 0.0515, 0.0236, 0.0216,\n",
      "        0.0146, 0.0222, 0.0091, 0.0433, 0.0399, 0.0278, 0.0192, 0.0257, 0.0362,\n",
      "        0.0151, 0.0162, 0.0257, 0.0467, 0.0106, 0.0497, 0.0403, 0.0286, 0.0198,\n",
      "        0.0413, 0.0779, 0.0360, 0.0193, 0.0120, 0.0196, 0.0250, 0.0138, 0.0360,\n",
      "        0.0388, 0.0251, 0.0165, 0.0163, 0.0248, 0.0358, 0.0241, 0.0656, 0.0406,\n",
      "        0.0220, 0.0127, 0.0257, 0.0202, 0.0592, 0.0304, 0.0106, 0.0169, 0.0114,\n",
      "        0.0070, 0.0215, 0.0488, 0.0104, 0.0415, 0.0370, 0.0415, 0.0321, 0.0638,\n",
      "        0.0214, 0.0304, 0.0145, 0.0248, 0.1021, 0.0169, 0.0253, 0.0174, 0.0114,\n",
      "        0.0276, 0.0307, 0.0489, 0.0230, 0.0475, 0.0156, 0.0357, 0.0929, 0.0199])\n",
      "roc_auc score: 0.713925588085775\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 18, Loss: 1.8238531351089478\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-3.6304, -4.1411, -3.7327, -3.2954, -4.6417, -4.4099, -3.5065, -4.1213,\n",
      "        -3.6169, -5.1924, -4.2488, -3.6793, -3.3442, -3.0636, -3.1732, -3.7815,\n",
      "        -3.6951, -4.6463, -2.5190, -3.8005, -3.0445, -4.2379, -4.0793, -4.8624,\n",
      "        -4.4539, -2.7597, -4.4221, -3.9184, -4.0745, -4.4805, -5.0412, -4.5375,\n",
      "        -4.4015, -4.6599, -3.5217, -3.7952, -4.7077, -3.3697, -3.9355, -3.8815,\n",
      "        -3.2599, -3.8468, -4.4395, -4.1148, -3.1333, -3.3622, -3.0229, -2.7509,\n",
      "        -3.8835, -2.8364, -2.3920, -4.5197, -4.0743, -3.6739, -4.0090, -3.8793,\n",
      "        -3.0758, -3.7207, -2.9987, -3.0954, -4.2447, -3.3525, -3.8974, -3.7388,\n",
      "        -3.4212, -4.0538, -3.3824, -3.6870, -4.6347, -3.5630, -3.6430, -3.9802,\n",
      "        -2.8282, -3.6051, -3.0990, -4.1975, -2.6448, -3.9178, -3.8156, -4.3533,\n",
      "        -4.2512, -3.1723, -4.7618, -2.4088, -4.3980, -3.5523, -3.8525, -3.7909,\n",
      "        -5.3291, -4.8234, -3.3530, -4.1302, -4.3793, -3.6371, -3.4993, -3.5374,\n",
      "        -4.1013, -4.1690, -2.5895, -4.0108, -3.4998, -3.9239, -4.5292, -2.8425,\n",
      "        -2.9882, -4.4996, -4.1511, -3.8353, -4.1032, -3.4238, -3.3187, -2.7451,\n",
      "        -3.6428, -3.1262, -2.6255, -3.7596, -3.8638, -3.3152, -4.6142, -3.4177,\n",
      "        -3.5465, -3.0594, -3.8017, -3.9064, -4.2403, -3.0620, -3.7042, -4.7879,\n",
      "        -4.3794, -3.5830, -2.6627, -3.8395, -3.7525, -3.4739, -2.3268, -2.3194,\n",
      "        -3.2545, -3.3409, -3.9518, -3.1541, -3.8743, -3.0678, -3.9233, -4.0255,\n",
      "        -4.4340, -3.9900, -4.9330, -3.2461, -3.3511, -3.7393, -4.1451, -3.8345,\n",
      "        -3.4503, -4.4015, -4.3252, -3.8215, -3.1814, -4.7724, -3.1189, -3.3477,\n",
      "        -3.7267, -4.1085, -3.3205, -2.6031, -3.4717, -4.1401, -4.6491, -4.1085,\n",
      "        -3.8537, -4.5012, -3.4709, -3.3663, -3.8545, -4.3048, -4.3198, -3.8692,\n",
      "        -3.4741, -3.9023, -2.8114, -3.3305, -4.0067, -4.5799, -3.8227, -4.0761,\n",
      "        -2.9183, -3.6405, -4.7727, -4.2690, -4.7063, -5.2121, -4.0166, -3.1388,\n",
      "        -4.8077, -3.3042, -3.4319, -3.3200, -3.5969, -2.8363, -4.0343, -3.6548,\n",
      "        -4.4427, -3.8757, -2.3051, -4.2861, -3.8486, -4.2486, -4.6945, -3.7449,\n",
      "        -3.6428, -3.1262, -3.9571, -3.1567, -4.3655, -3.4721, -2.4102, -4.1047],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0258, 0.0157, 0.0234, 0.0357, 0.0095, 0.0120, 0.0291, 0.0160, 0.0262,\n",
      "        0.0055, 0.0141, 0.0246, 0.0341, 0.0446, 0.0402, 0.0223, 0.0242, 0.0095,\n",
      "        0.0745, 0.0219, 0.0455, 0.0142, 0.0166, 0.0077, 0.0115, 0.0595, 0.0119,\n",
      "        0.0195, 0.0167, 0.0112, 0.0064, 0.0106, 0.0121, 0.0094, 0.0287, 0.0220,\n",
      "        0.0089, 0.0333, 0.0192, 0.0202, 0.0370, 0.0209, 0.0117, 0.0161, 0.0418,\n",
      "        0.0335, 0.0464, 0.0600, 0.0202, 0.0554, 0.0838, 0.0108, 0.0167, 0.0247,\n",
      "        0.0178, 0.0202, 0.0441, 0.0236, 0.0475, 0.0433, 0.0141, 0.0338, 0.0199,\n",
      "        0.0232, 0.0316, 0.0171, 0.0329, 0.0244, 0.0096, 0.0276, 0.0255, 0.0183,\n",
      "        0.0558, 0.0265, 0.0431, 0.0148, 0.0663, 0.0195, 0.0215, 0.0127, 0.0140,\n",
      "        0.0402, 0.0085, 0.0825, 0.0122, 0.0279, 0.0208, 0.0221, 0.0048, 0.0080,\n",
      "        0.0338, 0.0158, 0.0124, 0.0257, 0.0293, 0.0283, 0.0163, 0.0152, 0.0698,\n",
      "        0.0178, 0.0293, 0.0194, 0.0107, 0.0551, 0.0480, 0.0110, 0.0155, 0.0211,\n",
      "        0.0163, 0.0316, 0.0349, 0.0604, 0.0255, 0.0420, 0.0675, 0.0228, 0.0206,\n",
      "        0.0351, 0.0098, 0.0317, 0.0280, 0.0448, 0.0218, 0.0197, 0.0142, 0.0447,\n",
      "        0.0240, 0.0083, 0.0124, 0.0270, 0.0652, 0.0211, 0.0229, 0.0301, 0.0889,\n",
      "        0.0895, 0.0372, 0.0342, 0.0189, 0.0409, 0.0203, 0.0445, 0.0194, 0.0175,\n",
      "        0.0117, 0.0182, 0.0072, 0.0375, 0.0339, 0.0232, 0.0156, 0.0212, 0.0308,\n",
      "        0.0121, 0.0131, 0.0214, 0.0399, 0.0084, 0.0423, 0.0340, 0.0235, 0.0162,\n",
      "        0.0349, 0.0689, 0.0301, 0.0157, 0.0095, 0.0162, 0.0208, 0.0110, 0.0302,\n",
      "        0.0334, 0.0207, 0.0133, 0.0131, 0.0204, 0.0301, 0.0198, 0.0567, 0.0345,\n",
      "        0.0179, 0.0102, 0.0214, 0.0167, 0.0513, 0.0256, 0.0084, 0.0138, 0.0090,\n",
      "        0.0054, 0.0177, 0.0415, 0.0081, 0.0354, 0.0313, 0.0349, 0.0267, 0.0554,\n",
      "        0.0174, 0.0252, 0.0116, 0.0203, 0.0907, 0.0136, 0.0209, 0.0141, 0.0091,\n",
      "        0.0231, 0.0255, 0.0420, 0.0188, 0.0408, 0.0125, 0.0301, 0.0824, 0.0162])\n",
      "roc_auc score: 0.7136728498474905\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 19, Loss: 1.6861374378204346\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-3.8209, -4.3553, -3.9347, -3.4748, -4.8860, -4.6426, -3.6678, -4.3250,\n",
      "        -3.8047, -5.4597, -4.4515, -3.8728, -3.5266, -3.2355, -3.3478, -3.9866,\n",
      "        -3.8740, -4.8836, -2.6594, -4.0032, -3.2090, -4.4629, -4.2951, -5.1097,\n",
      "        -4.6905, -2.9123, -4.6451, -4.1073, -4.2900, -4.7170, -5.2904, -4.7525,\n",
      "        -4.6197, -4.9044, -3.6939, -3.9689, -4.9530, -3.5472, -4.1490, -4.0750,\n",
      "        -3.4137, -4.0535, -4.6722, -4.3363, -3.3051, -3.5426, -3.1919, -2.8788,\n",
      "        -4.0878, -2.9917, -2.5272, -4.7519, -4.2822, -3.8717, -4.2175, -4.0871,\n",
      "        -3.2288, -3.9180, -3.1374, -3.2611, -4.4589, -3.5147, -4.1058, -3.9366,\n",
      "        -3.5939, -4.2599, -3.5678, -3.8642, -4.8702, -3.7408, -3.8361, -4.1831,\n",
      "        -2.9821, -3.7938, -3.2688, -4.4231, -2.7924, -4.1311, -4.0003, -4.5843,\n",
      "        -4.4705, -3.3418, -5.0049, -2.5305, -4.6227, -3.7421, -4.0493, -3.9882,\n",
      "        -5.6021, -5.0721, -3.5181, -4.3245, -4.5876, -3.8039, -3.6908, -3.7161,\n",
      "        -4.3139, -4.3811, -2.7204, -4.2266, -3.6757, -4.1376, -4.7742, -2.9946,\n",
      "        -3.1364, -4.7266, -4.3737, -4.0421, -4.3250, -3.6081, -3.5007, -2.8723,\n",
      "        -3.8361, -3.2843, -2.7487, -3.9579, -4.0696, -3.4749, -4.8491, -3.5824,\n",
      "        -3.7237, -3.2001, -3.9838, -4.1169, -4.4570, -3.2135, -3.9000, -5.0305,\n",
      "        -4.6099, -3.7593, -2.8072, -4.0238, -3.9509, -3.6601, -2.4443, -2.4484,\n",
      "        -3.4162, -3.5243, -4.1564, -3.3283, -4.0816, -3.2203, -4.1284, -4.2422,\n",
      "        -4.6628, -4.1946, -5.1823, -3.3958, -3.5268, -3.9164, -4.3647, -4.0370,\n",
      "        -3.6172, -4.6267, -4.5434, -4.0055, -3.3503, -5.0149, -3.2902, -3.5277,\n",
      "        -3.9290, -4.3201, -3.4982, -2.7348, -3.6613, -4.3504, -4.8960, -4.3050,\n",
      "        -4.0420, -4.7389, -3.6535, -3.5213, -4.0512, -4.5258, -4.5415, -4.0667,\n",
      "        -3.6609, -4.1045, -2.9642, -3.4988, -4.2223, -4.8087, -4.0078, -4.2709,\n",
      "        -3.0735, -3.8258, -5.0110, -4.4731, -4.9508, -5.4808, -4.2101, -3.3033,\n",
      "        -5.0641, -3.4632, -3.6032, -3.5045, -3.7917, -2.9917, -4.2517, -3.8504,\n",
      "        -4.6723, -4.0826, -2.4329, -4.5130, -4.0567, -4.4635, -4.9336, -3.9257,\n",
      "        -3.8361, -3.2843, -4.1686, -3.3135, -4.5892, -3.6466, -2.5439, -4.3126],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0214, 0.0127, 0.0192, 0.0300, 0.0075, 0.0095, 0.0249, 0.0131, 0.0218,\n",
      "        0.0042, 0.0115, 0.0204, 0.0286, 0.0379, 0.0340, 0.0182, 0.0204, 0.0075,\n",
      "        0.0654, 0.0179, 0.0388, 0.0114, 0.0135, 0.0060, 0.0091, 0.0515, 0.0095,\n",
      "        0.0162, 0.0135, 0.0089, 0.0050, 0.0086, 0.0098, 0.0074, 0.0243, 0.0185,\n",
      "        0.0070, 0.0280, 0.0155, 0.0167, 0.0319, 0.0171, 0.0093, 0.0129, 0.0354,\n",
      "        0.0281, 0.0395, 0.0532, 0.0165, 0.0478, 0.0740, 0.0086, 0.0136, 0.0204,\n",
      "        0.0145, 0.0165, 0.0381, 0.0195, 0.0416, 0.0369, 0.0114, 0.0289, 0.0162,\n",
      "        0.0191, 0.0268, 0.0139, 0.0274, 0.0205, 0.0076, 0.0232, 0.0211, 0.0150,\n",
      "        0.0482, 0.0220, 0.0367, 0.0119, 0.0577, 0.0158, 0.0180, 0.0101, 0.0113,\n",
      "        0.0342, 0.0067, 0.0737, 0.0097, 0.0232, 0.0171, 0.0182, 0.0037, 0.0062,\n",
      "        0.0288, 0.0131, 0.0101, 0.0218, 0.0243, 0.0237, 0.0132, 0.0124, 0.0618,\n",
      "        0.0144, 0.0247, 0.0157, 0.0084, 0.0477, 0.0416, 0.0088, 0.0124, 0.0173,\n",
      "        0.0131, 0.0264, 0.0293, 0.0535, 0.0211, 0.0361, 0.0602, 0.0187, 0.0168,\n",
      "        0.0300, 0.0078, 0.0271, 0.0236, 0.0392, 0.0183, 0.0160, 0.0115, 0.0387,\n",
      "        0.0198, 0.0065, 0.0099, 0.0228, 0.0569, 0.0176, 0.0189, 0.0251, 0.0799,\n",
      "        0.0796, 0.0318, 0.0286, 0.0154, 0.0346, 0.0166, 0.0384, 0.0159, 0.0142,\n",
      "        0.0094, 0.0149, 0.0056, 0.0324, 0.0286, 0.0195, 0.0126, 0.0173, 0.0262,\n",
      "        0.0097, 0.0105, 0.0179, 0.0339, 0.0066, 0.0359, 0.0285, 0.0193, 0.0131,\n",
      "        0.0294, 0.0610, 0.0251, 0.0127, 0.0074, 0.0133, 0.0173, 0.0087, 0.0252,\n",
      "        0.0287, 0.0171, 0.0107, 0.0105, 0.0168, 0.0251, 0.0162, 0.0491, 0.0293,\n",
      "        0.0145, 0.0081, 0.0178, 0.0138, 0.0442, 0.0213, 0.0066, 0.0113, 0.0070,\n",
      "        0.0041, 0.0146, 0.0355, 0.0063, 0.0304, 0.0265, 0.0292, 0.0221, 0.0478,\n",
      "        0.0140, 0.0208, 0.0093, 0.0166, 0.0807, 0.0108, 0.0170, 0.0114, 0.0071,\n",
      "        0.0193, 0.0211, 0.0361, 0.0152, 0.0351, 0.0101, 0.0254, 0.0728, 0.0132])\n",
      "roc_auc score: 0.7129810691838432\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 20, Loss: 1.5512220859527588\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.0138, -4.5721, -4.1401, -3.6574, -5.1338, -4.8779, -3.8296, -4.5294,\n",
      "        -3.9947, -5.7301, -4.6547, -4.0687, -3.7123, -3.4106, -3.5244, -4.1950,\n",
      "        -4.0532, -5.1234, -2.8005, -4.2138, -3.3753, -4.6914, -4.5182, -5.3633,\n",
      "        -4.9348, -3.0641, -4.8695, -4.2967, -4.5111, -4.9560, -5.5410, -4.9682,\n",
      "        -4.8385, -5.1526, -3.8665, -4.1430, -5.2011, -3.7268, -4.3649, -4.2692,\n",
      "        -3.5675, -4.2635, -4.9078, -4.5615, -3.4789, -3.7253, -3.3641, -3.0062,\n",
      "        -4.2953, -3.1473, -2.6631, -4.9867, -4.4914, -4.0771, -4.4284, -4.2981,\n",
      "        -3.3817, -4.1185, -3.2764, -3.4287, -4.6744, -3.6771, -4.3173, -4.1377,\n",
      "        -3.7670, -4.4672, -3.7564, -4.0417, -5.1083, -3.9192, -4.0315, -4.3871,\n",
      "        -3.1379, -3.9848, -3.4405, -4.6523, -2.9381, -4.3479, -4.1855, -4.8201,\n",
      "        -4.6942, -3.5132, -5.2507, -2.6525, -4.8501, -3.9353, -4.2472, -4.1879,\n",
      "        -5.8781, -5.3237, -3.6839, -4.5193, -4.7963, -3.9712, -3.8857, -3.8954,\n",
      "        -4.5300, -4.5945, -2.8518, -4.4460, -3.8522, -4.3538, -5.0226, -3.1462,\n",
      "        -3.2806, -4.9551, -4.5989, -4.2524, -4.5504, -3.7945, -3.6857, -2.9997,\n",
      "        -4.0326, -3.4428, -2.8717, -4.1593, -4.2787, -3.6347, -5.0865, -3.7452,\n",
      "        -3.9014, -3.3411, -4.1662, -4.3308, -4.6771, -3.3621, -4.0989, -5.2758,\n",
      "        -4.8454, -3.9314, -2.9529, -4.2084, -4.1526, -3.8496, -2.5625, -2.5783,\n",
      "        -3.5738, -3.7108, -4.3635, -3.5055, -4.2921, -3.3726, -4.3360, -4.4623,\n",
      "        -4.8943, -4.4005, -5.4401, -3.5457, -3.7047, -4.0939, -4.5879, -4.2450,\n",
      "        -3.7843, -4.8545, -4.7628, -4.1900, -3.5210, -5.2600, -3.4635, -3.7099,\n",
      "        -4.1340, -4.5342, -3.6780, -2.8670, -3.8541, -4.5618, -5.1480, -4.5019,\n",
      "        -4.2249, -4.9802, -3.8381, -3.6768, -4.2491, -4.7493, -4.7657, -4.2652,\n",
      "        -3.8510, -4.3092, -3.1189, -3.6675, -4.4402, -5.0389, -4.1890, -4.4660,\n",
      "        -3.2301, -4.0171, -5.2506, -4.6778, -5.1982, -5.7526, -4.4007, -3.4630,\n",
      "        -5.3241, -3.6201, -3.7749, -3.6923, -3.9903, -3.1502, -4.4724, -4.0483,\n",
      "        -4.9045, -4.2947, -2.5609, -4.7433, -4.2693, -4.6796, -5.1753, -4.1068,\n",
      "        -4.0326, -3.4428, -4.3833, -3.4686, -4.8153, -3.8215, -2.6809, -4.5218],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0177, 0.0102, 0.0157, 0.0252, 0.0059, 0.0076, 0.0213, 0.0107, 0.0181,\n",
      "        0.0032, 0.0094, 0.0168, 0.0238, 0.0320, 0.0286, 0.0148, 0.0171, 0.0059,\n",
      "        0.0573, 0.0146, 0.0331, 0.0091, 0.0108, 0.0047, 0.0071, 0.0446, 0.0076,\n",
      "        0.0134, 0.0109, 0.0070, 0.0039, 0.0069, 0.0079, 0.0058, 0.0205, 0.0156,\n",
      "        0.0055, 0.0235, 0.0126, 0.0138, 0.0275, 0.0139, 0.0073, 0.0103, 0.0299,\n",
      "        0.0235, 0.0334, 0.0471, 0.0134, 0.0412, 0.0652, 0.0068, 0.0111, 0.0167,\n",
      "        0.0118, 0.0134, 0.0329, 0.0160, 0.0364, 0.0314, 0.0092, 0.0247, 0.0132,\n",
      "        0.0157, 0.0226, 0.0113, 0.0228, 0.0173, 0.0060, 0.0195, 0.0174, 0.0123,\n",
      "        0.0416, 0.0183, 0.0311, 0.0094, 0.0503, 0.0128, 0.0150, 0.0080, 0.0091,\n",
      "        0.0289, 0.0052, 0.0658, 0.0078, 0.0192, 0.0141, 0.0150, 0.0028, 0.0049,\n",
      "        0.0245, 0.0108, 0.0082, 0.0185, 0.0201, 0.0199, 0.0107, 0.0100, 0.0546,\n",
      "        0.0116, 0.0208, 0.0127, 0.0065, 0.0412, 0.0362, 0.0070, 0.0100, 0.0140,\n",
      "        0.0105, 0.0220, 0.0245, 0.0474, 0.0174, 0.0310, 0.0536, 0.0154, 0.0137,\n",
      "        0.0257, 0.0061, 0.0231, 0.0198, 0.0342, 0.0153, 0.0130, 0.0092, 0.0335,\n",
      "        0.0163, 0.0051, 0.0078, 0.0192, 0.0496, 0.0147, 0.0155, 0.0208, 0.0716,\n",
      "        0.0705, 0.0273, 0.0239, 0.0126, 0.0292, 0.0135, 0.0332, 0.0129, 0.0114,\n",
      "        0.0074, 0.0121, 0.0043, 0.0280, 0.0240, 0.0164, 0.0101, 0.0141, 0.0222,\n",
      "        0.0077, 0.0085, 0.0149, 0.0287, 0.0052, 0.0304, 0.0239, 0.0158, 0.0106,\n",
      "        0.0247, 0.0538, 0.0208, 0.0103, 0.0058, 0.0110, 0.0144, 0.0068, 0.0211,\n",
      "        0.0247, 0.0141, 0.0086, 0.0084, 0.0139, 0.0208, 0.0133, 0.0423, 0.0249,\n",
      "        0.0117, 0.0064, 0.0149, 0.0114, 0.0380, 0.0177, 0.0052, 0.0092, 0.0055,\n",
      "        0.0032, 0.0121, 0.0304, 0.0048, 0.0261, 0.0224, 0.0243, 0.0182, 0.0411,\n",
      "        0.0113, 0.0172, 0.0074, 0.0135, 0.0717, 0.0086, 0.0138, 0.0092, 0.0056,\n",
      "        0.0162, 0.0174, 0.0310, 0.0123, 0.0302, 0.0080, 0.0214, 0.0641, 0.0108])\n",
      "roc_auc score: 0.7120685637920942\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_20.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_20.html. Open it in your browser.\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 21, Loss: 1.4334313869476318\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.2093, -4.7919, -4.3495, -3.8441, -5.3856, -5.1159, -3.9911, -4.7339,\n",
      "        -4.1869, -6.0038, -4.8579, -4.2673, -3.9029, -3.5894, -3.7030, -4.4075,\n",
      "        -4.2322, -5.3664, -2.9393, -4.4288, -3.5436, -4.9243, -4.7452, -5.6227,\n",
      "        -5.1836, -3.2114, -5.0948, -4.4859, -4.7347, -5.1974, -5.7931, -5.1839,\n",
      "        -5.0562, -5.4053, -4.0333, -4.3171, -5.4519, -3.9087, -4.5831, -4.4635,\n",
      "        -3.7189, -4.4775, -5.1463, -4.7910, -3.6545, -3.9102, -3.5399, -3.1330,\n",
      "        -4.5109, -3.3049, -2.7979, -5.2246, -4.7014, -4.2869, -4.6423, -4.5133,\n",
      "        -3.5306, -4.3258, -3.4150, -3.5983, -4.8907, -3.8391, -4.5330, -4.3448,\n",
      "        -3.9400, -4.6754, -3.9487, -4.2189, -5.3513, -4.0974, -4.2292, -4.5919,\n",
      "        -3.2948, -4.1783, -3.6141, -4.8854, -3.0850, -4.5691, -4.3705, -5.0646,\n",
      "        -4.9244, -3.6868, -5.5037, -2.7744, -5.0804, -4.1359, -4.4460, -4.3902,\n",
      "        -6.1575, -5.5782, -3.8495, -4.7138, -5.0050, -4.1382, -4.0844, -4.0747,\n",
      "        -4.7532, -4.8089, -2.9830, -4.6693, -4.0286, -4.5723, -5.2757, -3.2994,\n",
      "        -3.4251, -5.1846, -4.8268, -4.4667, -4.7802, -3.9828, -3.8746, -3.1267,\n",
      "        -4.2343, -3.6011, -2.9925, -4.3666, -4.4918, -3.7941, -5.3298, -3.9077,\n",
      "        -4.0790, -3.4816, -4.3484, -4.5491, -4.9044, -3.5079, -4.3036, -5.5283,\n",
      "        -5.0891, -4.1016, -3.1002, -4.3928, -4.3602, -4.0438, -2.6805, -2.7132,\n",
      "        -3.7297, -3.9011, -4.5732, -3.6865, -4.5067, -3.5199, -4.5464, -4.6866,\n",
      "        -5.1289, -4.6072, -5.7016, -3.6954, -3.8848, -4.2710, -4.8137, -4.4594,\n",
      "        -3.9511, -5.0852, -4.9832, -4.3742, -3.6938, -5.5107, -3.6386, -3.8943,\n",
      "        -4.3431, -4.7512, -3.8601, -2.9991, -4.0508, -4.7741, -5.4045, -4.6986,\n",
      "        -4.4075, -5.2254, -4.0247, -3.8318, -4.4477, -4.9757, -4.9929, -4.4646,\n",
      "        -4.0460, -4.5166, -3.2754, -3.8361, -4.6604, -5.2700, -4.3698, -4.6610,\n",
      "        -3.3865, -4.2109, -5.4913, -4.8823, -5.4485, -6.0277, -4.5912, -3.6246,\n",
      "        -5.5886, -3.7766, -3.9465, -3.8840, -4.1975, -3.3127, -4.6975, -4.2487,\n",
      "        -5.1398, -4.5138, -2.6903, -4.9817, -4.4862, -4.8966, -5.4208, -4.2876,\n",
      "        -4.2343, -3.6011, -4.6021, -3.6197, -5.0445, -3.9964, -2.8176, -4.7318],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0146, 0.0082, 0.0127, 0.0210, 0.0046, 0.0060, 0.0181, 0.0087, 0.0150,\n",
      "        0.0025, 0.0077, 0.0138, 0.0198, 0.0269, 0.0241, 0.0120, 0.0143, 0.0046,\n",
      "        0.0502, 0.0118, 0.0281, 0.0072, 0.0086, 0.0036, 0.0056, 0.0387, 0.0061,\n",
      "        0.0111, 0.0087, 0.0055, 0.0030, 0.0056, 0.0063, 0.0045, 0.0174, 0.0132,\n",
      "        0.0043, 0.0197, 0.0101, 0.0114, 0.0237, 0.0112, 0.0058, 0.0082, 0.0252,\n",
      "        0.0196, 0.0282, 0.0418, 0.0109, 0.0354, 0.0574, 0.0054, 0.0090, 0.0136,\n",
      "        0.0095, 0.0108, 0.0285, 0.0131, 0.0318, 0.0266, 0.0075, 0.0211, 0.0106,\n",
      "        0.0128, 0.0191, 0.0092, 0.0189, 0.0145, 0.0047, 0.0163, 0.0144, 0.0100,\n",
      "        0.0357, 0.0151, 0.0262, 0.0075, 0.0437, 0.0103, 0.0125, 0.0063, 0.0072,\n",
      "        0.0244, 0.0041, 0.0587, 0.0062, 0.0157, 0.0116, 0.0122, 0.0021, 0.0038,\n",
      "        0.0208, 0.0089, 0.0067, 0.0157, 0.0166, 0.0167, 0.0086, 0.0081, 0.0482,\n",
      "        0.0093, 0.0175, 0.0102, 0.0051, 0.0356, 0.0315, 0.0056, 0.0079, 0.0114,\n",
      "        0.0083, 0.0183, 0.0203, 0.0420, 0.0143, 0.0266, 0.0478, 0.0125, 0.0111,\n",
      "        0.0220, 0.0048, 0.0197, 0.0166, 0.0298, 0.0128, 0.0105, 0.0074, 0.0291,\n",
      "        0.0133, 0.0040, 0.0061, 0.0163, 0.0431, 0.0122, 0.0126, 0.0172, 0.0641,\n",
      "        0.0622, 0.0234, 0.0198, 0.0102, 0.0244, 0.0109, 0.0288, 0.0105, 0.0091,\n",
      "        0.0059, 0.0099, 0.0033, 0.0242, 0.0201, 0.0138, 0.0081, 0.0114, 0.0189,\n",
      "        0.0061, 0.0068, 0.0124, 0.0243, 0.0040, 0.0256, 0.0200, 0.0128, 0.0086,\n",
      "        0.0206, 0.0475, 0.0171, 0.0084, 0.0045, 0.0090, 0.0120, 0.0053, 0.0176,\n",
      "        0.0212, 0.0116, 0.0069, 0.0067, 0.0114, 0.0172, 0.0108, 0.0364, 0.0211,\n",
      "        0.0094, 0.0051, 0.0125, 0.0094, 0.0327, 0.0146, 0.0041, 0.0075, 0.0043,\n",
      "        0.0024, 0.0100, 0.0260, 0.0037, 0.0224, 0.0190, 0.0202, 0.0148, 0.0351,\n",
      "        0.0090, 0.0141, 0.0058, 0.0108, 0.0636, 0.0068, 0.0111, 0.0074, 0.0044,\n",
      "        0.0136, 0.0143, 0.0266, 0.0099, 0.0261, 0.0064, 0.0180, 0.0564, 0.0087])\n",
      "roc_auc score: 0.7107613053424531\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 22, Loss: 1.3184406757354736\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.4041, -5.0109, -4.5588, -4.0333, -5.6369, -5.3528, -4.1513, -4.9366,\n",
      "        -4.3787, -6.2765, -5.0594, -4.4651, -4.0972, -3.7683, -3.8809, -4.6198,\n",
      "        -4.4097, -5.6114, -3.0775, -4.6437, -3.7116, -5.1623, -4.9718, -5.8811,\n",
      "        -5.4320, -3.3576, -5.3187, -4.6735, -4.9574, -5.4375, -6.0493, -5.3977,\n",
      "        -5.2714, -5.6576, -4.1999, -4.4896, -5.7015, -4.0901, -4.8002, -4.6561,\n",
      "        -3.8691, -4.6913, -5.3839, -5.0202, -3.8295, -4.0944, -3.7159, -3.2589,\n",
      "        -4.7268, -3.4621, -2.9309, -5.4644, -4.9102, -4.4967, -4.8604, -4.7281,\n",
      "        -3.6766, -4.5346, -3.5524, -3.7675, -5.1057, -3.9997, -4.7484, -4.5541,\n",
      "        -4.1116, -4.8823, -4.1411, -4.3946, -5.5975, -4.2742, -4.4261, -4.7954,\n",
      "        -3.4515, -4.3712, -3.7870, -5.1182, -3.2296, -4.7901, -4.5540, -5.3087,\n",
      "        -5.1537, -3.8600, -5.7578, -2.8952, -5.3099, -4.3379, -4.6435, -4.5917,\n",
      "        -6.4357, -5.8316, -4.0131, -4.9066, -5.2119, -4.3038, -4.2831, -4.2525,\n",
      "        -4.9756, -5.0218, -3.1130, -4.8922, -4.2036, -4.7898, -5.5283, -3.4520,\n",
      "        -3.5686, -5.4126, -5.0538, -4.6808, -5.0098, -4.1723, -4.0634, -3.2526,\n",
      "        -4.4390, -3.7581, -3.1122, -4.5763, -4.7046, -3.9523, -5.5753, -4.0687,\n",
      "        -4.2551, -3.6210, -4.5291, -4.7672, -5.1308, -3.6524, -4.5108, -5.7817,\n",
      "        -5.3326, -4.2704, -3.2446, -4.5757, -4.5702, -4.2420, -2.7975, -2.8466,\n",
      "        -3.8842, -4.0914, -4.7840, -3.8674, -4.7210, -3.6654, -4.7611, -4.9160,\n",
      "        -5.3651, -4.8126, -5.9622, -3.8438, -4.0644, -4.4466, -5.0385, -4.6736,\n",
      "        -4.1165, -5.3152, -5.2021, -4.5570, -3.8663, -5.7640, -3.8131, -4.0780,\n",
      "        -4.5520, -4.9675, -4.0415, -3.1299, -4.2473, -4.9851, -5.6605, -4.8937,\n",
      "        -4.5884, -5.4700, -4.2108, -3.9856, -4.6451, -5.2014, -5.2193, -4.6627,\n",
      "        -4.2448, -4.7247, -3.4310, -4.0033, -4.8794, -5.4996, -4.5491, -4.8544,\n",
      "        -3.5383, -4.4040, -5.7304, -5.0852, -5.6978, -6.3018, -4.7799, -3.7857,\n",
      "        -5.8527, -3.9317, -4.1166, -4.0757, -4.4046, -3.4728, -4.9231, -4.4486,\n",
      "        -5.3744, -4.7327, -2.8190, -5.2215, -4.7030, -5.1122, -5.6707, -4.4670,\n",
      "        -4.4390, -3.7581, -4.8207, -3.7694, -5.2728, -4.1699, -2.9537, -4.9405],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0121, 0.0066, 0.0104, 0.0174, 0.0036, 0.0047, 0.0155, 0.0071, 0.0124,\n",
      "        0.0019, 0.0063, 0.0114, 0.0163, 0.0226, 0.0202, 0.0098, 0.0120, 0.0036,\n",
      "        0.0440, 0.0095, 0.0239, 0.0057, 0.0069, 0.0028, 0.0044, 0.0336, 0.0049,\n",
      "        0.0093, 0.0070, 0.0043, 0.0024, 0.0045, 0.0051, 0.0035, 0.0148, 0.0111,\n",
      "        0.0033, 0.0165, 0.0082, 0.0094, 0.0205, 0.0091, 0.0046, 0.0066, 0.0213,\n",
      "        0.0164, 0.0238, 0.0370, 0.0088, 0.0304, 0.0506, 0.0042, 0.0073, 0.0110,\n",
      "        0.0077, 0.0088, 0.0247, 0.0106, 0.0279, 0.0226, 0.0060, 0.0180, 0.0086,\n",
      "        0.0104, 0.0161, 0.0075, 0.0157, 0.0122, 0.0037, 0.0137, 0.0118, 0.0082,\n",
      "        0.0307, 0.0125, 0.0222, 0.0060, 0.0381, 0.0082, 0.0104, 0.0049, 0.0057,\n",
      "        0.0206, 0.0031, 0.0524, 0.0049, 0.0129, 0.0095, 0.0100, 0.0016, 0.0029,\n",
      "        0.0178, 0.0073, 0.0054, 0.0133, 0.0136, 0.0140, 0.0069, 0.0065, 0.0426,\n",
      "        0.0074, 0.0147, 0.0082, 0.0040, 0.0307, 0.0274, 0.0044, 0.0063, 0.0092,\n",
      "        0.0066, 0.0152, 0.0169, 0.0372, 0.0117, 0.0228, 0.0426, 0.0102, 0.0090,\n",
      "        0.0188, 0.0038, 0.0168, 0.0140, 0.0261, 0.0107, 0.0084, 0.0059, 0.0253,\n",
      "        0.0109, 0.0031, 0.0048, 0.0138, 0.0375, 0.0102, 0.0103, 0.0142, 0.0575,\n",
      "        0.0549, 0.0201, 0.0164, 0.0083, 0.0205, 0.0088, 0.0250, 0.0085, 0.0073,\n",
      "        0.0047, 0.0081, 0.0026, 0.0210, 0.0169, 0.0116, 0.0064, 0.0093, 0.0160,\n",
      "        0.0049, 0.0055, 0.0104, 0.0205, 0.0031, 0.0216, 0.0167, 0.0104, 0.0069,\n",
      "        0.0173, 0.0419, 0.0141, 0.0068, 0.0035, 0.0074, 0.0101, 0.0042, 0.0146,\n",
      "        0.0182, 0.0095, 0.0055, 0.0054, 0.0094, 0.0141, 0.0088, 0.0313, 0.0179,\n",
      "        0.0075, 0.0041, 0.0105, 0.0077, 0.0282, 0.0121, 0.0032, 0.0061, 0.0033,\n",
      "        0.0018, 0.0083, 0.0222, 0.0029, 0.0192, 0.0160, 0.0167, 0.0121, 0.0301,\n",
      "        0.0072, 0.0116, 0.0046, 0.0087, 0.0563, 0.0054, 0.0090, 0.0060, 0.0034,\n",
      "        0.0114, 0.0117, 0.0228, 0.0080, 0.0225, 0.0051, 0.0152, 0.0496, 0.0071])\n",
      "roc_auc score: 0.7094675262655206\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 23, Loss: 1.2360916137695312\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.5961, -5.2266, -4.7646, -4.2214, -5.8843, -5.5862, -4.3096, -5.1367,\n",
      "        -4.5678, -6.5450, -5.2584, -4.6607, -4.2885, -3.9446, -4.0571, -4.8297,\n",
      "        -4.5852, -5.8557, -3.2118, -4.8551, -3.8773, -5.3964, -5.1950, -6.1357,\n",
      "        -5.6763, -3.5018, -5.5396, -4.8590, -5.1769, -5.6743, -6.3021, -5.6088,\n",
      "        -5.4838, -5.9057, -4.3671, -4.6601, -5.9475, -4.2692, -5.0144, -4.8463,\n",
      "        -4.0175, -4.9019, -5.6180, -5.2457, -4.0022, -4.2761, -3.8893, -3.3834,\n",
      "        -4.9395, -3.6172, -3.0623, -5.7036, -5.1162, -4.7031, -5.0753, -4.9415,\n",
      "        -3.8211, -4.7401, -3.6883, -3.9343, -5.3177, -4.1586, -4.9616, -4.7602,\n",
      "        -4.2811, -5.0864, -4.3306, -4.5683, -5.8399, -4.4488, -4.6245, -4.9964,\n",
      "        -3.6061, -4.5614, -3.9577, -5.3475, -3.3718, -5.0076, -4.7353, -5.5488,\n",
      "        -5.3798, -4.0310, -6.0081, -3.0144, -5.5372, -4.5367, -4.8385, -4.7933,\n",
      "        -6.7096, -6.0813, -4.1705, -5.0970, -5.4162, -4.4674, -4.4788, -4.4281,\n",
      "        -5.1949, -5.2320, -3.2413, -5.1118, -4.3763, -5.0043, -5.7769, -3.6023,\n",
      "        -3.7104, -5.6375, -5.2774, -4.8918, -5.2356, -4.3619, -4.2494, -3.3772,\n",
      "        -4.6406, -3.9133, -3.2308, -4.7828, -4.9140, -4.1087, -5.8171, -4.2279,\n",
      "        -4.4291, -3.7588, -4.7076, -4.9818, -5.3540, -3.7953, -4.7148, -6.0314,\n",
      "        -5.5720, -4.4372, -3.3841, -4.7563, -4.7769, -4.4370, -2.9129, -2.9730,\n",
      "        -4.0371, -4.2789, -4.9950, -4.0458, -4.9318, -3.8093, -4.9727, -5.1417,\n",
      "        -5.6010, -5.0154, -6.2188, -3.9906, -4.2416, -4.6202, -5.2600, -4.8845,\n",
      "        -4.2800, -5.5452, -5.4182, -4.7375, -4.0364, -6.0136, -3.9858, -4.2592,\n",
      "        -4.7584, -5.1806, -4.2203, -3.2590, -4.4410, -5.1933, -5.9123, -5.0864,\n",
      "        -4.7672, -5.7109, -4.3942, -4.1376, -4.8400, -5.4237, -5.4423, -4.8583,\n",
      "        -4.4404, -4.9333, -3.5845, -4.1685, -5.0955, -5.7261, -4.7262, -5.0454,\n",
      "        -3.6879, -4.5946, -5.9675, -5.2856, -5.9434, -6.5717, -4.9664, -3.9446,\n",
      "        -6.1124, -4.0851, -4.2847, -4.2645, -4.6084, -3.6309, -5.1496, -4.6502,\n",
      "        -5.6089, -4.9483, -2.9457, -5.4575, -4.9163, -5.3250, -5.9168, -4.6443,\n",
      "        -4.6406, -3.9133, -5.0405, -3.9175, -5.4998, -4.3412, -3.0877, -5.1464],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0100, 0.0053, 0.0085, 0.0145, 0.0028, 0.0037, 0.0133, 0.0058, 0.0103,\n",
      "        0.0014, 0.0052, 0.0094, 0.0135, 0.0190, 0.0170, 0.0079, 0.0101, 0.0029,\n",
      "        0.0387, 0.0077, 0.0203, 0.0045, 0.0055, 0.0022, 0.0034, 0.0293, 0.0039,\n",
      "        0.0077, 0.0056, 0.0034, 0.0018, 0.0037, 0.0041, 0.0027, 0.0125, 0.0094,\n",
      "        0.0026, 0.0138, 0.0066, 0.0078, 0.0177, 0.0074, 0.0036, 0.0052, 0.0179,\n",
      "        0.0137, 0.0201, 0.0328, 0.0071, 0.0262, 0.0447, 0.0033, 0.0060, 0.0090,\n",
      "        0.0062, 0.0071, 0.0214, 0.0087, 0.0244, 0.0192, 0.0049, 0.0154, 0.0070,\n",
      "        0.0085, 0.0136, 0.0061, 0.0130, 0.0103, 0.0029, 0.0116, 0.0097, 0.0067,\n",
      "        0.0264, 0.0103, 0.0187, 0.0047, 0.0332, 0.0066, 0.0087, 0.0039, 0.0046,\n",
      "        0.0174, 0.0025, 0.0468, 0.0039, 0.0106, 0.0079, 0.0082, 0.0012, 0.0023,\n",
      "        0.0152, 0.0061, 0.0044, 0.0113, 0.0112, 0.0118, 0.0055, 0.0053, 0.0376,\n",
      "        0.0060, 0.0124, 0.0067, 0.0031, 0.0265, 0.0239, 0.0035, 0.0051, 0.0075,\n",
      "        0.0053, 0.0126, 0.0141, 0.0330, 0.0096, 0.0196, 0.0380, 0.0083, 0.0073,\n",
      "        0.0162, 0.0030, 0.0144, 0.0118, 0.0228, 0.0089, 0.0068, 0.0047, 0.0220,\n",
      "        0.0089, 0.0024, 0.0038, 0.0117, 0.0328, 0.0085, 0.0084, 0.0117, 0.0515,\n",
      "        0.0487, 0.0173, 0.0137, 0.0067, 0.0172, 0.0072, 0.0217, 0.0069, 0.0058,\n",
      "        0.0037, 0.0066, 0.0020, 0.0182, 0.0142, 0.0098, 0.0052, 0.0075, 0.0137,\n",
      "        0.0039, 0.0044, 0.0087, 0.0174, 0.0024, 0.0182, 0.0139, 0.0085, 0.0056,\n",
      "        0.0145, 0.0370, 0.0116, 0.0055, 0.0027, 0.0061, 0.0084, 0.0033, 0.0122,\n",
      "        0.0157, 0.0078, 0.0044, 0.0043, 0.0077, 0.0117, 0.0072, 0.0270, 0.0152,\n",
      "        0.0061, 0.0032, 0.0088, 0.0064, 0.0244, 0.0100, 0.0026, 0.0050, 0.0026,\n",
      "        0.0014, 0.0069, 0.0190, 0.0022, 0.0165, 0.0136, 0.0139, 0.0099, 0.0258,\n",
      "        0.0058, 0.0095, 0.0037, 0.0070, 0.0499, 0.0042, 0.0073, 0.0048, 0.0027,\n",
      "        0.0095, 0.0096, 0.0196, 0.0064, 0.0195, 0.0041, 0.0129, 0.0436, 0.0058])\n",
      "roc_auc score: 0.7082803786548355\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 24, Loss: 1.1546133756637573\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.7858, -5.4376, -4.9678, -4.4045, -6.1253, -5.8142, -4.4649, -5.3325,\n",
      "        -4.7526, -6.8068, -5.4534, -4.8558, -4.4745, -4.1164, -4.2330, -5.0378,\n",
      "        -4.7573, -6.0939, -3.3442, -5.0608, -4.0392, -5.6242, -5.4124, -6.3839,\n",
      "        -5.9142, -3.6449, -5.7557, -5.0408, -5.3913, -5.9058, -6.5494, -5.8157,\n",
      "        -5.6920, -6.1472, -4.5311, -4.8286, -6.1877, -4.4441, -5.2238, -5.0325,\n",
      "        -4.1633, -5.1072, -5.8465, -5.4652, -4.1749, -4.4568, -4.0583, -3.5058,\n",
      "        -5.1466, -3.7688, -3.1906, -5.9370, -5.3178, -4.9039, -5.2852, -5.1521,\n",
      "        -3.9630, -4.9404, -3.8218, -4.0973, -5.5252, -4.3146, -5.1727, -4.9610,\n",
      "        -4.4472, -5.2862, -4.5153, -4.7388, -6.0765, -4.6198, -4.8188, -5.1930,\n",
      "        -3.7573, -4.7514, -4.1287, -5.5708, -3.5097, -5.2191, -4.9131, -5.7825,\n",
      "        -5.6005, -4.1980, -6.2522, -3.1312, -5.7631, -4.7300, -5.0293, -4.9921,\n",
      "        -6.9768, -6.3250, -4.3276, -5.2837, -5.6164, -4.6279, -4.6696, -4.6001,\n",
      "        -5.4089, -5.4377, -3.3669, -5.3257, -4.5456, -5.2140, -6.0188, -3.7510,\n",
      "        -3.8494, -5.8577, -5.4959, -5.0974, -5.4554, -4.5473, -4.4308, -3.4997,\n",
      "        -4.8369, -4.0654, -3.3473, -4.9839, -5.1205, -4.2622, -6.0529, -4.3841,\n",
      "        -4.5995, -3.8942, -4.8827, -5.1905, -5.5719, -3.9357, -4.9135, -6.2748,\n",
      "        -5.8051, -4.6008, -3.5271, -4.9335, -4.9783, -4.6267, -3.0260, -3.1019,\n",
      "        -4.1871, -4.4616, -5.2010, -4.2196, -5.1410, -3.9506, -5.1793, -5.3614,\n",
      "        -5.8311, -5.2138, -6.4690, -4.1346, -4.4148, -4.7904, -5.4764, -5.0900,\n",
      "        -4.4405, -5.7714, -5.6296, -4.9146, -4.2027, -6.2570, -4.1584, -4.4403,\n",
      "        -4.9632, -5.3884, -4.3954, -3.3855, -4.6298, -5.3970, -6.1573, -5.2752,\n",
      "        -4.9425, -5.9456, -4.5728, -4.2867, -5.0307, -5.6451, -5.6639, -5.0498,\n",
      "        -4.6308, -5.1371, -3.7346, -4.3304, -5.3068, -5.9504, -4.8999, -5.2327,\n",
      "        -3.8393, -4.7807, -6.2041, -5.4820, -6.1832, -6.8349, -5.1492, -4.0996,\n",
      "        -6.3652, -4.2356, -4.4494, -4.4486, -4.8066, -3.7853, -5.3700, -4.8471,\n",
      "        -5.8397, -5.1582, -3.0721, -5.6872, -5.1238, -5.5331, -6.1569, -4.8182,\n",
      "        -4.8369, -4.0654, -5.2548, -4.0629, -5.7245, -4.5090, -3.2215, -5.3479],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0083, 0.0043, 0.0069, 0.0121, 0.0022, 0.0030, 0.0114, 0.0048, 0.0086,\n",
      "        0.0011, 0.0043, 0.0077, 0.0113, 0.0160, 0.0143, 0.0064, 0.0085, 0.0023,\n",
      "        0.0341, 0.0063, 0.0173, 0.0036, 0.0044, 0.0017, 0.0027, 0.0255, 0.0032,\n",
      "        0.0064, 0.0045, 0.0027, 0.0014, 0.0030, 0.0034, 0.0021, 0.0107, 0.0079,\n",
      "        0.0021, 0.0116, 0.0054, 0.0065, 0.0153, 0.0060, 0.0029, 0.0042, 0.0151,\n",
      "        0.0115, 0.0170, 0.0291, 0.0058, 0.0226, 0.0395, 0.0026, 0.0049, 0.0074,\n",
      "        0.0050, 0.0058, 0.0187, 0.0071, 0.0214, 0.0163, 0.0040, 0.0132, 0.0056,\n",
      "        0.0070, 0.0116, 0.0050, 0.0108, 0.0087, 0.0023, 0.0098, 0.0080, 0.0055,\n",
      "        0.0228, 0.0086, 0.0158, 0.0038, 0.0290, 0.0054, 0.0073, 0.0031, 0.0037,\n",
      "        0.0148, 0.0019, 0.0418, 0.0031, 0.0087, 0.0065, 0.0067, 0.0009, 0.0018,\n",
      "        0.0130, 0.0050, 0.0036, 0.0097, 0.0093, 0.0100, 0.0045, 0.0043, 0.0333,\n",
      "        0.0048, 0.0105, 0.0054, 0.0024, 0.0230, 0.0208, 0.0028, 0.0041, 0.0061,\n",
      "        0.0043, 0.0105, 0.0118, 0.0293, 0.0079, 0.0169, 0.0340, 0.0068, 0.0059,\n",
      "        0.0139, 0.0023, 0.0123, 0.0100, 0.0200, 0.0075, 0.0055, 0.0038, 0.0192,\n",
      "        0.0073, 0.0019, 0.0030, 0.0099, 0.0286, 0.0071, 0.0068, 0.0097, 0.0463,\n",
      "        0.0430, 0.0150, 0.0114, 0.0055, 0.0145, 0.0058, 0.0189, 0.0056, 0.0047,\n",
      "        0.0029, 0.0054, 0.0015, 0.0158, 0.0120, 0.0082, 0.0042, 0.0061, 0.0117,\n",
      "        0.0031, 0.0036, 0.0073, 0.0147, 0.0019, 0.0154, 0.0117, 0.0069, 0.0045,\n",
      "        0.0122, 0.0328, 0.0097, 0.0045, 0.0021, 0.0051, 0.0071, 0.0026, 0.0102,\n",
      "        0.0136, 0.0065, 0.0035, 0.0035, 0.0064, 0.0097, 0.0058, 0.0233, 0.0130,\n",
      "        0.0049, 0.0026, 0.0074, 0.0053, 0.0211, 0.0083, 0.0020, 0.0041, 0.0021,\n",
      "        0.0011, 0.0058, 0.0163, 0.0017, 0.0143, 0.0116, 0.0116, 0.0081, 0.0222,\n",
      "        0.0046, 0.0078, 0.0029, 0.0057, 0.0443, 0.0034, 0.0059, 0.0039, 0.0021,\n",
      "        0.0080, 0.0079, 0.0169, 0.0052, 0.0169, 0.0033, 0.0109, 0.0384, 0.0047])\n",
      "roc_auc score: 0.707120671195736\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 25, Loss: 1.0957515239715576\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-4.9721, -5.6462, -5.1656, -4.5809, -6.3583, -6.0353, -4.6167, -5.5234,\n",
      "        -4.9320, -7.0603, -5.6437, -5.0450, -4.6538, -4.2824, -4.4040, -5.2385,\n",
      "        -4.9253, -6.3244, -3.4778, -5.2591, -4.1961, -5.8439, -5.6224, -6.6242,\n",
      "        -6.1437, -3.7883, -5.9687, -5.2183, -5.5992, -6.1306, -6.7901, -6.0188,\n",
      "        -5.8995, -6.3804, -4.6911, -4.9957, -6.4206, -4.6139, -5.4271, -5.2139,\n",
      "        -4.3057, -5.3056, -6.0681, -5.6769, -4.3431, -4.6332, -4.2216, -3.6257,\n",
      "        -5.3468, -3.9159, -3.3167, -6.1629, -5.5139, -5.0975, -5.4888, -5.3552,\n",
      "        -4.1017, -5.1338, -3.9525, -4.2549, -5.7277, -4.4670, -5.3764, -5.1549,\n",
      "        -4.6093, -5.4806, -4.6939, -4.9052, -6.3054, -4.7867, -5.0073, -5.3843,\n",
      "        -3.9040, -4.9360, -4.2950, -5.7865, -3.6478, -5.4232, -5.0867, -6.0079,\n",
      "        -5.8144, -4.3601, -6.4885, -3.2451, -5.9818, -4.9165, -5.2150, -5.1849,\n",
      "        -7.2355, -6.5614, -4.4810, -5.4659, -5.8117, -4.7855, -4.8540, -4.7678,\n",
      "        -5.6165, -5.6377, -3.4894, -5.5324, -4.7107, -5.4177, -6.2522, -3.8989,\n",
      "        -3.9850, -6.0772, -5.7077, -5.2961, -5.6675, -4.7275, -4.6060, -3.6197,\n",
      "        -5.0266, -4.2139, -3.4616, -5.1782, -5.3217, -4.4122, -6.2813, -4.5368,\n",
      "        -4.7657, -4.0267, -5.0537, -5.3919, -5.7831, -4.0731, -5.1055, -6.5105,\n",
      "        -6.0299, -4.7607, -3.6659, -5.1065, -5.1729, -4.8095, -3.1364, -3.2270,\n",
      "        -4.3337, -4.6382, -5.4008, -4.3875, -5.3436, -4.0889, -5.3797, -5.5732,\n",
      "        -6.0538, -5.4069, -6.7113, -4.2755, -4.5829, -4.9566, -5.6862, -5.2885,\n",
      "        -4.5974, -5.9905, -5.8370, -5.0876, -4.3642, -6.4926, -4.3262, -4.6162,\n",
      "        -5.1607, -5.5942, -4.5692, -3.5087, -4.8123, -5.5952, -6.3938, -5.4595,\n",
      "        -5.1136, -6.1724, -4.7460, -4.4324, -5.2163, -5.8602, -5.8796, -5.2360,\n",
      "        -4.8142, -5.3346, -3.8804, -4.4883, -5.5120, -6.1715, -5.0696, -5.4155,\n",
      "        -3.9881, -4.9614, -6.4342, -5.6735, -6.4157, -7.0898, -5.3277, -4.2572,\n",
      "        -6.6092, -4.3829, -4.6101, -4.6264, -4.9977, -3.9347, -5.5825, -5.0380,\n",
      "        -6.0630, -5.3610, -3.1998, -5.9087, -5.3240, -5.7356, -6.3893, -4.9880,\n",
      "        -5.0266, -4.2139, -5.4615, -4.2052, -5.9421, -4.6728, -3.3504, -5.5440],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0069, 0.0035, 0.0057, 0.0101, 0.0017, 0.0024, 0.0098, 0.0040, 0.0072,\n",
      "        0.0009, 0.0035, 0.0064, 0.0094, 0.0136, 0.0121, 0.0053, 0.0072, 0.0018,\n",
      "        0.0300, 0.0052, 0.0148, 0.0029, 0.0036, 0.0013, 0.0021, 0.0221, 0.0026,\n",
      "        0.0054, 0.0037, 0.0022, 0.0011, 0.0024, 0.0027, 0.0017, 0.0091, 0.0067,\n",
      "        0.0016, 0.0098, 0.0044, 0.0054, 0.0133, 0.0049, 0.0023, 0.0034, 0.0128,\n",
      "        0.0096, 0.0145, 0.0259, 0.0047, 0.0195, 0.0350, 0.0021, 0.0040, 0.0061,\n",
      "        0.0041, 0.0047, 0.0163, 0.0059, 0.0188, 0.0140, 0.0032, 0.0114, 0.0046,\n",
      "        0.0057, 0.0099, 0.0041, 0.0091, 0.0074, 0.0018, 0.0083, 0.0066, 0.0046,\n",
      "        0.0198, 0.0071, 0.0135, 0.0031, 0.0254, 0.0044, 0.0061, 0.0025, 0.0030,\n",
      "        0.0126, 0.0015, 0.0375, 0.0025, 0.0073, 0.0054, 0.0056, 0.0007, 0.0014,\n",
      "        0.0112, 0.0042, 0.0030, 0.0083, 0.0077, 0.0084, 0.0036, 0.0035, 0.0296,\n",
      "        0.0039, 0.0089, 0.0044, 0.0019, 0.0199, 0.0183, 0.0023, 0.0033, 0.0050,\n",
      "        0.0034, 0.0088, 0.0099, 0.0261, 0.0065, 0.0146, 0.0304, 0.0056, 0.0049,\n",
      "        0.0120, 0.0019, 0.0106, 0.0084, 0.0175, 0.0063, 0.0045, 0.0031, 0.0167,\n",
      "        0.0060, 0.0015, 0.0024, 0.0085, 0.0249, 0.0060, 0.0056, 0.0081, 0.0416,\n",
      "        0.0382, 0.0129, 0.0096, 0.0045, 0.0123, 0.0048, 0.0165, 0.0046, 0.0038,\n",
      "        0.0023, 0.0045, 0.0012, 0.0137, 0.0101, 0.0070, 0.0034, 0.0050, 0.0100,\n",
      "        0.0025, 0.0029, 0.0061, 0.0126, 0.0015, 0.0130, 0.0098, 0.0057, 0.0037,\n",
      "        0.0103, 0.0291, 0.0081, 0.0037, 0.0017, 0.0042, 0.0060, 0.0021, 0.0086,\n",
      "        0.0117, 0.0054, 0.0028, 0.0028, 0.0053, 0.0080, 0.0048, 0.0202, 0.0111,\n",
      "        0.0040, 0.0021, 0.0062, 0.0044, 0.0182, 0.0070, 0.0016, 0.0034, 0.0016,\n",
      "        0.0008, 0.0048, 0.0140, 0.0013, 0.0123, 0.0099, 0.0097, 0.0067, 0.0192,\n",
      "        0.0037, 0.0064, 0.0023, 0.0047, 0.0392, 0.0027, 0.0048, 0.0032, 0.0017,\n",
      "        0.0068, 0.0065, 0.0146, 0.0042, 0.0147, 0.0026, 0.0093, 0.0339, 0.0039])\n",
      "roc_auc score: 0.7061511191730597\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_25.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_25.html. Open it in your browser.\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 26, Loss: 1.0539638996124268\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-5.1525, -5.8482, -5.3576, -4.7523, -6.5842, -6.2493, -4.7631, -5.7074,\n",
      "        -5.1058, -7.3058, -5.8272, -5.2282, -4.8280, -4.4438, -4.5695, -5.4333,\n",
      "        -5.0873, -6.5477, -3.6074, -5.4517, -4.3477, -6.0570, -5.8261, -6.8569,\n",
      "        -6.3663, -3.9274, -6.1770, -5.3894, -5.8005, -6.3481, -7.0226, -6.2183,\n",
      "        -6.0998, -6.6066, -4.8458, -5.1571, -6.6461, -4.7785, -5.6240, -5.3889,\n",
      "        -4.4432, -5.4983, -6.2828, -5.8825, -4.5060, -4.8041, -4.3804, -3.7413,\n",
      "        -5.5409, -4.0586, -3.4433, -6.3816, -5.7033, -5.2857, -5.6859, -5.5524,\n",
      "        -4.2354, -5.3215, -4.0785, -4.4078, -5.9278, -4.6139, -5.5740, -5.3431,\n",
      "        -4.7655, -5.6682, -4.8674, -5.0658, -6.5272, -4.9475, -5.1899, -5.5690,\n",
      "        -4.0464, -5.1148, -4.4560, -5.9957, -3.7830, -5.6213, -5.2540, -6.2266,\n",
      "        -6.0215, -4.5173, -6.7172, -3.3550, -6.1935, -5.0976, -5.3943, -5.3717,\n",
      "        -7.4860, -6.7903, -4.6292, -5.6415, -6.0017, -4.9405, -5.0332, -4.9296,\n",
      "        -5.8176, -5.8334, -3.6076, -5.7329, -4.8699, -5.6148, -6.4786, -4.0418,\n",
      "        -4.1160, -6.2893, -5.9128, -5.4892, -5.8733, -4.9020, -4.7763, -3.7354,\n",
      "        -5.2107, -4.3570, -3.5718, -5.3668, -5.5170, -4.5569, -6.5024, -4.6840,\n",
      "        -4.9259, -4.1544, -5.2185, -5.5875, -5.9877, -4.2054, -5.2918, -6.7387,\n",
      "        -6.2481, -4.9147, -3.8005, -5.2733, -5.3616, -4.9872, -3.2429, -3.3485,\n",
      "        -4.4751, -4.8098, -5.5943, -4.5508, -5.5402, -4.2221, -5.5738, -5.7788,\n",
      "        -6.2695, -5.5932, -6.9459, -4.4114, -4.7459, -5.1169, -5.8894, -5.4811,\n",
      "        -4.7486, -6.2026, -6.0408, -5.2543, -4.5208, -6.7207, -4.4886, -4.7866,\n",
      "        -5.3524, -5.7939, -4.7374, -3.6245, -4.9896, -5.7898, -6.6231, -5.6372,\n",
      "        -5.2786, -6.3922, -4.9138, -4.5730, -5.3955, -6.0684, -6.0885, -5.4158,\n",
      "        -4.9926, -5.5260, -4.0218, -4.6406, -5.7106, -6.3851, -5.2331, -5.5917,\n",
      "        -4.1326, -5.1366, -6.6565, -5.8583, -6.6408, -7.3365, -5.4997, -4.4104,\n",
      "        -6.8458, -4.5248, -4.7650, -4.7992, -5.1834, -4.0793, -5.7886, -5.2228,\n",
      "        -6.2793, -5.5577, -3.3250, -6.1237, -5.5184, -5.9344, -6.6143, -5.1517,\n",
      "        -5.2107, -4.3570, -5.6621, -4.3422, -6.1527, -4.8306, -3.4755, -5.7335],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0058, 0.0029, 0.0047, 0.0086, 0.0014, 0.0019, 0.0085, 0.0033, 0.0060,\n",
      "        0.0007, 0.0029, 0.0053, 0.0079, 0.0116, 0.0103, 0.0043, 0.0061, 0.0014,\n",
      "        0.0264, 0.0043, 0.0128, 0.0023, 0.0029, 0.0011, 0.0017, 0.0193, 0.0021,\n",
      "        0.0045, 0.0030, 0.0017, 0.0009, 0.0020, 0.0022, 0.0013, 0.0078, 0.0057,\n",
      "        0.0013, 0.0083, 0.0036, 0.0045, 0.0116, 0.0041, 0.0019, 0.0028, 0.0109,\n",
      "        0.0081, 0.0124, 0.0232, 0.0039, 0.0170, 0.0310, 0.0017, 0.0033, 0.0050,\n",
      "        0.0034, 0.0039, 0.0143, 0.0049, 0.0167, 0.0120, 0.0027, 0.0098, 0.0038,\n",
      "        0.0048, 0.0084, 0.0034, 0.0076, 0.0063, 0.0015, 0.0071, 0.0055, 0.0038,\n",
      "        0.0172, 0.0060, 0.0115, 0.0025, 0.0222, 0.0036, 0.0052, 0.0020, 0.0024,\n",
      "        0.0108, 0.0012, 0.0337, 0.0020, 0.0061, 0.0045, 0.0046, 0.0006, 0.0011,\n",
      "        0.0097, 0.0035, 0.0025, 0.0071, 0.0065, 0.0072, 0.0030, 0.0029, 0.0264,\n",
      "        0.0032, 0.0076, 0.0036, 0.0015, 0.0173, 0.0160, 0.0019, 0.0027, 0.0041,\n",
      "        0.0028, 0.0074, 0.0084, 0.0233, 0.0054, 0.0127, 0.0273, 0.0046, 0.0040,\n",
      "        0.0104, 0.0015, 0.0092, 0.0072, 0.0155, 0.0054, 0.0037, 0.0025, 0.0147,\n",
      "        0.0050, 0.0012, 0.0019, 0.0073, 0.0219, 0.0051, 0.0047, 0.0068, 0.0376,\n",
      "        0.0339, 0.0113, 0.0081, 0.0037, 0.0104, 0.0039, 0.0145, 0.0038, 0.0031,\n",
      "        0.0019, 0.0037, 0.0010, 0.0120, 0.0086, 0.0060, 0.0028, 0.0041, 0.0086,\n",
      "        0.0020, 0.0024, 0.0052, 0.0108, 0.0012, 0.0111, 0.0083, 0.0047, 0.0030,\n",
      "        0.0087, 0.0260, 0.0068, 0.0030, 0.0013, 0.0036, 0.0051, 0.0017, 0.0073,\n",
      "        0.0102, 0.0045, 0.0023, 0.0023, 0.0044, 0.0067, 0.0040, 0.0176, 0.0096,\n",
      "        0.0033, 0.0017, 0.0053, 0.0037, 0.0158, 0.0058, 0.0013, 0.0028, 0.0013,\n",
      "        0.0007, 0.0041, 0.0120, 0.0011, 0.0107, 0.0085, 0.0082, 0.0056, 0.0166,\n",
      "        0.0031, 0.0054, 0.0019, 0.0038, 0.0347, 0.0022, 0.0040, 0.0026, 0.0013,\n",
      "        0.0058, 0.0054, 0.0127, 0.0035, 0.0128, 0.0021, 0.0079, 0.0300, 0.0032])\n",
      "roc_auc score: 0.7052388544843948\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 27, Loss: 1.0112884044647217\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-5.3261, -6.0427, -5.5430, -4.9182, -6.8020, -6.4550, -4.9037, -5.8866,\n",
      "        -5.2733, -7.5419, -6.0026, -5.4044, -4.9965, -4.5997, -4.7283, -5.6214,\n",
      "        -5.2420, -6.7624, -3.7325, -5.6379, -4.4939, -6.2628, -6.0225, -7.0808,\n",
      "        -6.5811, -4.0616, -6.3766, -5.5530, -5.9939, -6.5569, -7.2455, -6.4092,\n",
      "        -6.2913, -6.8251, -4.9939, -5.3115, -6.8627, -4.9371, -5.8129, -5.5560,\n",
      "        -4.5745, -5.6844, -6.4891, -6.0812, -4.6624, -4.9683, -4.5338, -3.8517,\n",
      "        -5.7281, -4.1961, -3.5653, -6.5920, -5.8879, -5.4675, -5.8754, -5.7428,\n",
      "        -4.3629, -5.5025, -4.1989, -4.5551, -6.1195, -4.7542, -5.7649, -5.5246,\n",
      "        -4.9146, -5.8522, -5.0350, -5.2191, -6.7405, -5.1011, -5.3654, -5.7481,\n",
      "        -4.1837, -5.2868, -4.6106, -6.1974, -3.9134, -5.8128, -5.4139, -6.4377,\n",
      "        -6.2207, -4.6688, -6.9373, -3.4580, -6.3973, -5.2728, -5.5660, -5.5513,\n",
      "        -7.7269, -7.0102, -4.7711, -5.8094, -6.1865, -5.0887, -5.2062, -5.0840,\n",
      "        -6.0109, -6.0234, -3.7204, -5.9262, -5.0219, -5.8040, -6.6971, -4.1796,\n",
      "        -4.2413, -6.4926, -6.1099, -5.6754, -6.0723, -5.0694, -4.9408, -3.8459,\n",
      "        -5.3883, -4.4937, -3.6770, -5.5487, -5.7056, -4.6950, -6.7152, -4.8245,\n",
      "        -5.0788, -4.2765, -5.3761, -5.7766, -6.1845, -4.3316, -5.4715, -6.9582,\n",
      "        -6.4586, -5.0617, -3.9303, -5.4327, -5.5437, -5.1591, -3.3445, -3.4658,\n",
      "        -4.6099, -4.9756, -5.7803, -4.7085, -5.7301, -4.3492, -5.7604, -5.9774,\n",
      "        -6.4770, -5.7729, -7.1716, -4.5412, -4.9030, -5.2699, -6.0847, -5.6668,\n",
      "        -4.8930, -6.4067, -6.2361, -5.4136, -4.6717, -6.9402, -4.6446, -4.9504,\n",
      "        -5.5376, -5.9860, -4.8991, -3.7351, -5.1608, -5.9780, -6.8444, -5.8070,\n",
      "        -5.4360, -6.6042, -5.0755, -4.7074, -5.5671, -6.2689, -6.2895, -5.5880,\n",
      "        -5.1650, -5.7101, -4.1581, -4.7860, -5.9012, -6.5898, -5.3892, -5.7602,\n",
      "        -4.2718, -5.3052, -6.8696, -6.0348, -6.8572, -7.5738, -5.6639, -4.5580,\n",
      "        -7.0741, -4.6601, -4.9129, -4.9660, -5.3629, -4.2188, -5.9877, -5.4005,\n",
      "        -6.4874, -5.7475, -3.4452, -6.3311, -5.7063, -6.1266, -6.8308, -5.3081,\n",
      "        -5.3883, -4.4937, -5.8558, -4.4729, -6.3555, -4.9814, -3.5962, -5.9197],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0048, 0.0024, 0.0039, 0.0073, 0.0011, 0.0016, 0.0074, 0.0028, 0.0051,\n",
      "        0.0005, 0.0025, 0.0045, 0.0067, 0.0100, 0.0088, 0.0036, 0.0053, 0.0012,\n",
      "        0.0234, 0.0035, 0.0111, 0.0019, 0.0024, 0.0008, 0.0014, 0.0169, 0.0017,\n",
      "        0.0039, 0.0025, 0.0014, 0.0007, 0.0016, 0.0018, 0.0011, 0.0067, 0.0049,\n",
      "        0.0010, 0.0071, 0.0030, 0.0038, 0.0102, 0.0034, 0.0015, 0.0023, 0.0094,\n",
      "        0.0069, 0.0106, 0.0208, 0.0032, 0.0148, 0.0275, 0.0014, 0.0028, 0.0042,\n",
      "        0.0028, 0.0032, 0.0126, 0.0041, 0.0148, 0.0104, 0.0022, 0.0085, 0.0031,\n",
      "        0.0040, 0.0073, 0.0029, 0.0065, 0.0054, 0.0012, 0.0061, 0.0047, 0.0032,\n",
      "        0.0150, 0.0050, 0.0098, 0.0020, 0.0196, 0.0030, 0.0044, 0.0016, 0.0020,\n",
      "        0.0093, 0.0010, 0.0305, 0.0017, 0.0051, 0.0038, 0.0039, 0.0004, 0.0009,\n",
      "        0.0084, 0.0030, 0.0021, 0.0061, 0.0055, 0.0062, 0.0024, 0.0024, 0.0237,\n",
      "        0.0027, 0.0065, 0.0030, 0.0012, 0.0151, 0.0142, 0.0015, 0.0022, 0.0034,\n",
      "        0.0023, 0.0062, 0.0071, 0.0209, 0.0045, 0.0111, 0.0247, 0.0039, 0.0033,\n",
      "        0.0091, 0.0012, 0.0080, 0.0062, 0.0137, 0.0046, 0.0031, 0.0021, 0.0130,\n",
      "        0.0042, 0.0009, 0.0016, 0.0063, 0.0193, 0.0044, 0.0039, 0.0057, 0.0341,\n",
      "        0.0303, 0.0099, 0.0069, 0.0031, 0.0089, 0.0032, 0.0128, 0.0031, 0.0025,\n",
      "        0.0015, 0.0031, 0.0008, 0.0105, 0.0074, 0.0051, 0.0023, 0.0034, 0.0074,\n",
      "        0.0016, 0.0020, 0.0044, 0.0093, 0.0010, 0.0095, 0.0070, 0.0039, 0.0025,\n",
      "        0.0074, 0.0233, 0.0057, 0.0025, 0.0011, 0.0030, 0.0043, 0.0014, 0.0062,\n",
      "        0.0089, 0.0038, 0.0019, 0.0019, 0.0037, 0.0057, 0.0033, 0.0154, 0.0083,\n",
      "        0.0027, 0.0014, 0.0045, 0.0031, 0.0138, 0.0049, 0.0010, 0.0024, 0.0011,\n",
      "        0.0005, 0.0035, 0.0104, 0.0008, 0.0094, 0.0073, 0.0069, 0.0047, 0.0145,\n",
      "        0.0025, 0.0045, 0.0015, 0.0032, 0.0309, 0.0018, 0.0033, 0.0022, 0.0011,\n",
      "        0.0049, 0.0045, 0.0111, 0.0029, 0.0113, 0.0017, 0.0068, 0.0267, 0.0027])\n",
      "roc_auc score: 0.7042835039436792\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 28, Loss: 0.9765599370002747\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-5.4917, -6.2284, -5.7207, -5.0774, -7.0102, -6.6510, -5.0399, -6.0579,\n",
      "        -5.4334, -7.7672, -6.1723, -5.5726, -5.1583, -4.7492, -4.8797, -5.8016,\n",
      "        -5.3888, -6.9674, -3.8522, -5.8164, -4.6336, -6.4598, -6.2105, -7.2944,\n",
      "        -6.7868, -4.1900, -6.5665, -5.7081, -6.1783, -6.7558, -7.4574, -6.5905,\n",
      "        -6.4731, -7.0344, -5.1346, -5.4581, -7.0692, -5.0887, -5.9929, -5.7146,\n",
      "        -4.6991, -5.8625, -6.6859, -6.2716, -4.8115, -5.1249, -4.6810, -3.9564,\n",
      "        -5.9075, -4.3276, -3.6820, -6.7930, -6.0650, -5.6420, -6.0563, -5.9252,\n",
      "        -4.4837, -5.6759, -4.3131, -4.6960, -6.3018, -4.8872, -5.9477, -5.6984,\n",
      "        -5.0561, -6.0278, -5.1956, -5.3646, -6.9442, -5.2468, -5.5327, -5.9209,\n",
      "        -4.3150, -5.4509, -4.7580, -6.3904, -4.0382, -5.9964, -5.5655, -6.6397,\n",
      "        -6.4107, -4.8135, -7.1473, -3.5552, -6.5919, -5.4409, -5.7315, -5.7228,\n",
      "        -7.9567, -7.2201, -4.9060, -5.9726, -6.3619, -5.2295, -5.3720, -5.2305,\n",
      "        -6.1954, -6.2042, -3.8256, -6.1113, -5.1661, -5.9843, -6.9062, -4.3115,\n",
      "        -4.3631, -6.6859, -6.2977, -5.8538, -6.2629, -5.2290, -5.0985, -3.9507,\n",
      "        -5.5584, -4.6232, -3.7768, -5.7229, -5.8864, -4.8261, -6.9183, -4.9576,\n",
      "        -5.2229, -4.3922, -5.5255, -5.9578, -6.3722, -4.4512, -5.6436, -7.1678,\n",
      "        -6.6602, -5.2010, -4.0546, -5.5839, -5.7181, -5.3240, -3.4410, -3.5780,\n",
      "        -4.7377, -5.1345, -5.9579, -4.8597, -5.9121, -4.4695, -5.9385, -6.1675,\n",
      "        -6.6751, -5.9472, -7.3870, -4.6644, -5.0532, -5.4148, -6.2708, -5.8447,\n",
      "        -5.0300, -6.6016, -6.4218, -5.5648, -4.8159, -7.1497, -4.7932, -5.1066,\n",
      "        -5.7151, -6.1696, -5.0534, -3.8403, -5.3250, -6.1571, -7.0563, -5.9691,\n",
      "        -5.5852, -6.8070, -5.2301, -4.8367, -5.7324, -6.4603, -6.4815, -5.7536,\n",
      "        -5.3305, -5.8857, -4.2886, -4.9239, -6.0828, -6.7845, -5.5372, -5.9200,\n",
      "        -4.4050, -5.4664, -7.0723, -6.2054, -7.0636, -7.8002, -5.8195, -4.6992,\n",
      "        -7.2926, -4.7884, -5.0532, -5.1259, -5.5350, -4.3522, -6.1783, -5.5698,\n",
      "        -6.6860, -5.9290, -3.5596, -6.5298, -5.8865, -6.3095, -7.0374, -5.4565,\n",
      "        -5.5584, -4.6232, -6.0414, -4.5968, -6.5490, -5.1243, -3.7117, -6.0968],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0041, 0.0020, 0.0033, 0.0062, 0.0009, 0.0013, 0.0064, 0.0023, 0.0043,\n",
      "        0.0004, 0.0021, 0.0038, 0.0057, 0.0086, 0.0075, 0.0030, 0.0045, 0.0009,\n",
      "        0.0208, 0.0030, 0.0096, 0.0016, 0.0020, 0.0007, 0.0011, 0.0149, 0.0014,\n",
      "        0.0033, 0.0021, 0.0012, 0.0006, 0.0014, 0.0015, 0.0009, 0.0059, 0.0042,\n",
      "        0.0009, 0.0061, 0.0025, 0.0033, 0.0090, 0.0028, 0.0012, 0.0019, 0.0081,\n",
      "        0.0059, 0.0092, 0.0188, 0.0027, 0.0130, 0.0246, 0.0011, 0.0023, 0.0035,\n",
      "        0.0023, 0.0027, 0.0112, 0.0034, 0.0132, 0.0090, 0.0018, 0.0075, 0.0026,\n",
      "        0.0033, 0.0063, 0.0024, 0.0055, 0.0047, 0.0010, 0.0052, 0.0039, 0.0027,\n",
      "        0.0132, 0.0043, 0.0085, 0.0017, 0.0173, 0.0025, 0.0038, 0.0013, 0.0016,\n",
      "        0.0081, 0.0008, 0.0278, 0.0014, 0.0043, 0.0032, 0.0033, 0.0004, 0.0007,\n",
      "        0.0073, 0.0025, 0.0017, 0.0053, 0.0046, 0.0053, 0.0020, 0.0020, 0.0213,\n",
      "        0.0022, 0.0057, 0.0025, 0.0010, 0.0132, 0.0126, 0.0012, 0.0018, 0.0029,\n",
      "        0.0019, 0.0053, 0.0061, 0.0189, 0.0038, 0.0097, 0.0224, 0.0033, 0.0028,\n",
      "        0.0080, 0.0010, 0.0070, 0.0054, 0.0122, 0.0040, 0.0026, 0.0017, 0.0115,\n",
      "        0.0035, 0.0008, 0.0013, 0.0055, 0.0170, 0.0037, 0.0033, 0.0048, 0.0310,\n",
      "        0.0272, 0.0087, 0.0059, 0.0026, 0.0077, 0.0027, 0.0113, 0.0026, 0.0021,\n",
      "        0.0013, 0.0026, 0.0006, 0.0093, 0.0063, 0.0044, 0.0019, 0.0029, 0.0065,\n",
      "        0.0014, 0.0016, 0.0038, 0.0080, 0.0008, 0.0082, 0.0060, 0.0033, 0.0021,\n",
      "        0.0063, 0.0210, 0.0048, 0.0021, 0.0009, 0.0026, 0.0037, 0.0011, 0.0053,\n",
      "        0.0079, 0.0032, 0.0016, 0.0015, 0.0032, 0.0048, 0.0028, 0.0135, 0.0072,\n",
      "        0.0023, 0.0011, 0.0039, 0.0027, 0.0121, 0.0042, 0.0008, 0.0020, 0.0009,\n",
      "        0.0004, 0.0030, 0.0090, 0.0007, 0.0083, 0.0063, 0.0059, 0.0039, 0.0127,\n",
      "        0.0021, 0.0038, 0.0012, 0.0027, 0.0277, 0.0015, 0.0028, 0.0018, 0.0009,\n",
      "        0.0043, 0.0038, 0.0097, 0.0024, 0.0100, 0.0014, 0.0059, 0.0239, 0.0022])\n",
      "roc_auc score: 0.7033093785624056\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 29, Loss: 0.9494343996047974\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-5.6488, -6.4044, -5.8895, -5.2288, -7.2079, -6.8367, -5.1688, -6.2199,\n",
      "        -5.5852, -7.9806, -6.3339, -5.7321, -5.3120, -4.8914, -5.0231, -5.9728,\n",
      "        -5.5275, -7.1617, -3.9659, -5.9861, -4.7663, -6.6468, -6.3890, -7.4969,\n",
      "        -6.9820, -4.3119, -6.7463, -5.8547, -6.3530, -6.9441, -7.6580, -6.7619,\n",
      "        -6.6450, -7.2330, -5.2679, -5.5968, -7.2647, -5.2326, -6.1634, -5.8680,\n",
      "        -4.8169, -6.0317, -6.8724, -6.4524, -4.9527, -5.2734, -4.8210, -4.0555,\n",
      "        -6.0777, -4.4525, -3.7919, -6.9834, -6.2327, -5.8078, -6.2278, -6.0984,\n",
      "        -4.5978, -5.8406, -4.4211, -4.8297, -6.4745, -5.0130, -6.1214, -5.8635,\n",
      "        -5.1898, -6.1940, -5.3483, -5.5021, -7.1372, -5.3839, -5.6913, -6.0845,\n",
      "        -4.4397, -5.6067, -4.8977, -6.5737, -4.1567, -6.1708, -5.7088, -6.8315,\n",
      "        -6.5908, -4.9510, -7.3464, -3.6472, -6.7764, -5.6007, -5.8904, -5.8854,\n",
      "        -8.1745, -7.4189, -5.0337, -6.1269, -6.5278, -5.3627, -5.5296, -5.3690,\n",
      "        -6.3703, -6.3753, -3.9245, -6.2871, -5.3023, -6.1550, -7.1047, -4.4367,\n",
      "        -4.4788, -6.8689, -6.4757, -6.0233, -6.4439, -5.3802, -5.2484, -4.0498,\n",
      "        -5.7200, -4.7457, -3.8711, -5.8883, -6.0581, -4.9499, -7.1109, -5.0834,\n",
      "        -5.3592, -4.5017, -5.6667, -6.1300, -6.5501, -4.5642, -5.8072, -7.3664,\n",
      "        -6.8515, -5.3327, -4.1726, -5.7267, -5.8837, -5.4808, -3.5303, -3.6848,\n",
      "        -4.8584, -5.2855, -6.1262, -5.0035, -6.0849, -4.5832, -6.1074, -6.3481,\n",
      "        -6.8630, -6.1123, -7.5911, -4.7809, -5.1956, -5.5518, -6.4472, -6.0136,\n",
      "        -5.1594, -6.7864, -6.5976, -5.7076, -4.9529, -7.3483, -4.9340, -5.2547,\n",
      "        -5.8837, -6.3436, -5.1997, -3.9399, -5.4809, -6.3266, -7.2572, -6.1254,\n",
      "        -5.7262, -6.9994, -5.3767, -4.9603, -5.8911, -6.6418, -6.6635, -5.9128,\n",
      "        -5.4878, -6.0523, -4.4124, -5.0542, -6.2549, -6.9688, -5.6770, -6.0727,\n",
      "        -4.5315, -5.6193, -7.2641, -6.3680, -7.2591, -8.0147, -5.9671, -4.8332,\n",
      "        -7.4999, -4.9096, -5.1855, -5.2779, -5.6986, -4.4788, -6.3594, -5.7304,\n",
      "        -6.8743, -6.1015, -3.6684, -6.7184, -6.0576, -6.4826, -7.2333, -5.5966,\n",
      "        -5.7200, -4.7457, -6.2176, -4.7138, -6.7325, -5.2594, -3.8216, -6.2645],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0035, 0.0017, 0.0028, 0.0053, 0.0007, 0.0011, 0.0057, 0.0020, 0.0037,\n",
      "        0.0003, 0.0018, 0.0032, 0.0049, 0.0075, 0.0065, 0.0025, 0.0040, 0.0008,\n",
      "        0.0186, 0.0025, 0.0084, 0.0013, 0.0017, 0.0006, 0.0009, 0.0132, 0.0012,\n",
      "        0.0029, 0.0017, 0.0010, 0.0005, 0.0012, 0.0013, 0.0007, 0.0051, 0.0037,\n",
      "        0.0007, 0.0053, 0.0021, 0.0028, 0.0080, 0.0024, 0.0010, 0.0016, 0.0070,\n",
      "        0.0051, 0.0080, 0.0170, 0.0023, 0.0115, 0.0221, 0.0009, 0.0020, 0.0030,\n",
      "        0.0020, 0.0022, 0.0100, 0.0029, 0.0119, 0.0079, 0.0015, 0.0066, 0.0022,\n",
      "        0.0028, 0.0055, 0.0020, 0.0047, 0.0041, 0.0008, 0.0046, 0.0034, 0.0023,\n",
      "        0.0117, 0.0037, 0.0074, 0.0014, 0.0154, 0.0021, 0.0033, 0.0011, 0.0014,\n",
      "        0.0070, 0.0006, 0.0254, 0.0011, 0.0037, 0.0028, 0.0028, 0.0003, 0.0006,\n",
      "        0.0065, 0.0022, 0.0015, 0.0047, 0.0040, 0.0046, 0.0017, 0.0017, 0.0194,\n",
      "        0.0019, 0.0050, 0.0021, 0.0008, 0.0117, 0.0112, 0.0010, 0.0015, 0.0024,\n",
      "        0.0016, 0.0046, 0.0052, 0.0171, 0.0033, 0.0086, 0.0204, 0.0028, 0.0023,\n",
      "        0.0070, 0.0008, 0.0062, 0.0047, 0.0110, 0.0034, 0.0022, 0.0014, 0.0103,\n",
      "        0.0030, 0.0006, 0.0011, 0.0048, 0.0152, 0.0032, 0.0028, 0.0041, 0.0285,\n",
      "        0.0245, 0.0077, 0.0050, 0.0022, 0.0067, 0.0023, 0.0101, 0.0022, 0.0017,\n",
      "        0.0010, 0.0022, 0.0005, 0.0083, 0.0055, 0.0039, 0.0016, 0.0024, 0.0057,\n",
      "        0.0011, 0.0014, 0.0033, 0.0070, 0.0006, 0.0071, 0.0052, 0.0028, 0.0018,\n",
      "        0.0055, 0.0191, 0.0041, 0.0018, 0.0007, 0.0022, 0.0032, 0.0009, 0.0046,\n",
      "        0.0070, 0.0028, 0.0013, 0.0013, 0.0027, 0.0041, 0.0023, 0.0120, 0.0063,\n",
      "        0.0019, 0.0009, 0.0034, 0.0023, 0.0107, 0.0036, 0.0007, 0.0017, 0.0007,\n",
      "        0.0003, 0.0026, 0.0079, 0.0006, 0.0073, 0.0056, 0.0051, 0.0033, 0.0112,\n",
      "        0.0017, 0.0032, 0.0010, 0.0022, 0.0249, 0.0012, 0.0023, 0.0015, 0.0007,\n",
      "        0.0037, 0.0033, 0.0086, 0.0020, 0.0089, 0.0012, 0.0052, 0.0214, 0.0019])\n",
      "roc_auc score: 0.7024695655020489\n",
      "Max input embedding for MLP: 0.982603907585144\n",
      "Min input embedding for MLP: -0.9755895137786865\n",
      "------------------------------------------\n",
      "Max input embedding for MLP: 0.9890250563621521\n",
      "Min input embedding for MLP: -0.9954797029495239\n",
      "------------------------------------------\n",
      "Epoch 30, Loss: 0.9308869242668152\n",
      "Max input embedding for MLP: 0.9771515727043152\n",
      "Min input embedding for MLP: -0.9932307600975037\n",
      "------------------------------------------\n",
      "Edge weights for last sub_problem in last problem batch: tensor([-5.7975, -6.5709, -6.0491, -5.3718, -7.3949, -7.0125, -5.2911, -6.3731,\n",
      "        -5.7288, -8.1825, -6.4870, -5.8830, -5.4573, -5.0258, -5.1590, -6.1346,\n",
      "        -5.6589, -7.3454, -4.0735, -6.1464, -4.8918, -6.8236, -6.5579, -7.6884,\n",
      "        -7.1666, -4.4273, -6.9165, -5.9936, -6.5184, -7.1223, -7.8478, -6.9243,\n",
      "        -6.8079, -7.4207, -5.3943, -5.7284, -7.4498, -5.3687, -6.3249, -6.0130,\n",
      "        -4.9310, -6.1918, -7.0490, -6.6233, -5.0865, -5.4140, -4.9534, -4.1495,\n",
      "        -6.2389, -4.5706, -3.8959, -7.1635, -6.3916, -5.9644, -6.3901, -6.2622,\n",
      "        -4.7061, -5.9964, -4.5236, -4.9562, -6.6379, -5.1321, -6.2855, -6.0198,\n",
      "        -5.3164, -6.3514, -5.4928, -5.6323, -7.3198, -5.5134, -5.8414, -6.2395,\n",
      "        -4.5578, -5.7540, -5.0300, -6.7471, -4.2688, -6.3356, -5.8446, -7.0129,\n",
      "        -6.7613, -5.0810, -7.5347, -3.7346, -6.9508, -5.7517, -6.0408, -6.0393,\n",
      "        -8.3805, -7.6071, -5.1550, -6.2733, -6.6850, -5.4891, -5.6787, -5.5001,\n",
      "        -6.5359, -6.5373, -4.0184, -6.4535, -5.4314, -6.3167, -7.2924, -4.5551,\n",
      "        -4.5887, -7.0422, -6.6442, -6.1836, -6.6151, -5.5234, -5.3902, -4.1439,\n",
      "        -5.8729, -4.8617, -3.9607, -6.0449, -6.2203, -5.0672, -7.2930, -5.2026,\n",
      "        -5.4882, -4.6056, -5.8004, -6.2928, -6.7185, -4.6714, -5.9619, -7.5542,\n",
      "        -7.0324, -5.4575, -4.2843, -5.8620, -6.0405, -5.6290, -3.6149, -3.7857,\n",
      "        -4.9729, -5.4284, -6.2856, -5.1395, -6.2483, -4.6911, -6.2672, -6.5188,\n",
      "        -7.0406, -6.2687, -7.7841, -4.8928, -5.3305, -5.6817, -6.6141, -6.1735,\n",
      "        -5.2820, -6.9611, -6.7642, -5.8429, -5.0825, -7.5361, -5.0674, -5.3949,\n",
      "        -6.0431, -6.5082, -5.3382, -4.0343, -5.6285, -6.4871, -7.4473, -6.2736,\n",
      "        -5.8619, -7.1815, -5.5154, -5.0777, -6.0414, -6.8134, -6.8356, -6.0636,\n",
      "        -5.6365, -6.2099, -4.5297, -5.1777, -6.4177, -7.1432, -5.8095, -6.2197,\n",
      "        -4.6511, -5.7640, -7.4457, -6.5221, -7.4442, -8.2176, -6.1105, -4.9600,\n",
      "        -7.6958, -5.0246, -5.3102, -5.4217, -5.8532, -4.5987, -6.5305, -5.8823,\n",
      "        -7.0523, -6.2647, -3.7713, -6.8967, -6.2194, -6.6465, -7.4185, -5.7294,\n",
      "        -5.8729, -4.8617, -6.3842, -4.8248, -6.9060, -5.3874, -3.9255, -6.4233],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "Edge probabilites for last sub_problem in last problem batch: tensor([0.0030, 0.0014, 0.0024, 0.0046, 0.0006, 0.0009, 0.0050, 0.0017, 0.0032,\n",
      "        0.0003, 0.0015, 0.0028, 0.0042, 0.0065, 0.0057, 0.0022, 0.0035, 0.0006,\n",
      "        0.0167, 0.0021, 0.0075, 0.0011, 0.0014, 0.0005, 0.0008, 0.0118, 0.0010,\n",
      "        0.0025, 0.0015, 0.0008, 0.0004, 0.0010, 0.0011, 0.0006, 0.0045, 0.0032,\n",
      "        0.0006, 0.0046, 0.0018, 0.0024, 0.0072, 0.0020, 0.0009, 0.0013, 0.0061,\n",
      "        0.0044, 0.0070, 0.0155, 0.0019, 0.0102, 0.0199, 0.0008, 0.0017, 0.0026,\n",
      "        0.0017, 0.0019, 0.0090, 0.0025, 0.0107, 0.0070, 0.0013, 0.0059, 0.0019,\n",
      "        0.0024, 0.0049, 0.0017, 0.0041, 0.0036, 0.0007, 0.0040, 0.0029, 0.0019,\n",
      "        0.0104, 0.0032, 0.0065, 0.0012, 0.0138, 0.0018, 0.0029, 0.0009, 0.0012,\n",
      "        0.0062, 0.0005, 0.0233, 0.0010, 0.0032, 0.0024, 0.0024, 0.0002, 0.0005,\n",
      "        0.0057, 0.0019, 0.0012, 0.0041, 0.0034, 0.0041, 0.0014, 0.0014, 0.0177,\n",
      "        0.0016, 0.0044, 0.0018, 0.0007, 0.0104, 0.0101, 0.0009, 0.0013, 0.0021,\n",
      "        0.0013, 0.0040, 0.0045, 0.0156, 0.0028, 0.0077, 0.0187, 0.0024, 0.0020,\n",
      "        0.0063, 0.0007, 0.0055, 0.0041, 0.0099, 0.0030, 0.0018, 0.0012, 0.0093,\n",
      "        0.0026, 0.0005, 0.0009, 0.0042, 0.0136, 0.0028, 0.0024, 0.0036, 0.0262,\n",
      "        0.0222, 0.0069, 0.0044, 0.0019, 0.0058, 0.0019, 0.0091, 0.0019, 0.0015,\n",
      "        0.0009, 0.0019, 0.0004, 0.0074, 0.0048, 0.0034, 0.0013, 0.0021, 0.0051,\n",
      "        0.0009, 0.0012, 0.0029, 0.0062, 0.0005, 0.0063, 0.0045, 0.0024, 0.0015,\n",
      "        0.0048, 0.0174, 0.0036, 0.0015, 0.0006, 0.0019, 0.0028, 0.0008, 0.0040,\n",
      "        0.0062, 0.0024, 0.0011, 0.0011, 0.0023, 0.0036, 0.0020, 0.0107, 0.0056,\n",
      "        0.0016, 0.0008, 0.0030, 0.0020, 0.0095, 0.0031, 0.0006, 0.0015, 0.0006,\n",
      "        0.0003, 0.0022, 0.0070, 0.0005, 0.0065, 0.0049, 0.0044, 0.0029, 0.0100,\n",
      "        0.0015, 0.0028, 0.0009, 0.0019, 0.0225, 0.0010, 0.0020, 0.0013, 0.0006,\n",
      "        0.0032, 0.0028, 0.0077, 0.0017, 0.0080, 0.0010, 0.0046, 0.0194, 0.0016])\n",
      "roc_auc score: 0.7017262743784083\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_30.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_30.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_edge_ij_30.html\n",
      "Visualization saved as results/replication/seed0_vis_edge_ij_30.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_vis_gt.html\n",
      "Visualization saved as results/replication/seed0_vis_gt.html. Open it in your browser.\n",
      "Highest individual auc: 0.8164945919370697\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_highestAUC.html\n",
      "Visualization saved as results/replication/seed0_highestAUC.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_highestAUC_gt.html\n",
      "Visualization saved as results/replication/seed0_highestAUC_gt.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_highestAUC_commonEdges.html\n",
      "Visualization saved as results/replication/seed0_highestAUC_commonEdges.html. Open it in your browser.\n",
      "Highest AUC example masked edges: [[183 293]\n",
      " [181 294]\n",
      " [ 52 296]\n",
      " [ 50 298]\n",
      " [180 298]\n",
      " [180 299]\n",
      " [ 55 300]\n",
      " [182 300]\n",
      " [181 301]\n",
      " [180 303]\n",
      " [181 304]\n",
      " [ 50 305]\n",
      " [ 50 310]\n",
      " [180 311]\n",
      " [ 50 312]\n",
      " [ 51 314]\n",
      " [182 317]\n",
      " [ 52 318]\n",
      " [ 51 320]\n",
      " [ 50 321]\n",
      " [176 323]\n",
      " [180 327]\n",
      " [ 50 332]\n",
      " [ 55 332]\n",
      " [180 333]\n",
      " [ 52 334]\n",
      " [177 335]\n",
      " [180 337]\n",
      " [183 338]\n",
      " [ 52 339]\n",
      " [ 52 342]\n",
      " [ 55 343]\n",
      " [181 343]\n",
      " [ 52 344]\n",
      " [ 52 345]\n",
      " [ 54 352]]\n",
      "Lowest individual auc: 0.5208643163327793\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_lowestAUC.html\n",
      "Visualization saved as results/replication/seed0_lowestAUC.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_lowestAUC_gt.html\n",
      "Visualization saved as results/replication/seed0_lowestAUC_gt.html. Open it in your browser.\n",
      "Warning: When  cdn_resources is 'local' jupyter notebook has issues displaying graphics on chrome/safari. Use cdn_resources='in_line' or cdn_resources='remote' if you have issues viewing graphics in a notebook.\n",
      "results/replication/seed0_lowestAUC_commonEdges.html\n",
      "Visualization saved as results/replication/seed0_lowestAUC_commonEdges.html. Open it in your browser.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/Loss</td><td>███▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val/roc_auc</td><td>▁▃▅▆▇▇██████████████████████▇▇</td></tr><tr><td>val/temperature</td><td>██▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/Loss</td><td>0.93089</td></tr><tr><td>val/roc_auc</td><td>0.70173</td></tr><tr><td>val/temperature</td><td>1</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">absurd-frog-1</strong> at: <a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR/runs/a98prypb' target=\"_blank\">https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR/runs/a98prypb</a><br> View project at: <a href='https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR' target=\"_blank\">https://wandb.ai/tristan-schulz2001-tu-dortmund/NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250407_195935-a98prypb\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    mlp, downstreamTask = trainExplainer(datasetName,opts, wandb_project=\"NeuroSAT-seeded-train_val-threeEmbeddings-consistencyReg-lowerLR\", runSeed=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sweeping config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"method\": 'grid',                    # random, grid or Bayesian search\n",
    "    \"metric\": {\"goal\": \"maximize\", \"name\": \"val/mean_AUC\"},\n",
    "    \"parameters\": {\n",
    "        'epochs': {\n",
    "            'values': [30]\n",
    "            },\n",
    "        'tT': {\n",
    "            'values': [1.0, 5.0]\n",
    "            },\n",
    "        'size_reg': {\n",
    "            'values': [1.0, 0.1, 0.01]\n",
    "            },\n",
    "        'entropy_reg': {\n",
    "            'values': [0.1, 1.0, 10.0]\n",
    "            },\n",
    "        'lr_mlp':{\n",
    "            'values': [0.003, 0.001, 0.0003]\n",
    "            },\n",
    "        'seed':{\n",
    "            'values': [74, 75]\n",
    "            },\n",
    "    },\n",
    "}   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(sweep_config, project=\"Sweep-NeuroSAT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m wandb\u001b[38;5;241m.\u001b[39magent(sweep_id, sweepExplainerNeuroSAT\u001b[38;5;241m.\u001b[39mtrainExplainer)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "import sweepExplainerNeuroSAT\n",
    "\n",
    "wandb.agent(sweep_id, sweepExplainerNeuroSAT.trainExplainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuroSAT",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
