To compare baseline methods GRAD, ATT, Gradient: reimplement as well or take values from paper => Don't reimplement, take from somewhere

Accuracys of Model from best accuracy over all epochs, mean or last epoch => Last epoch, or go back to earlier stage of model

How to create datasets on a singular graph => Simply use one data object(graph)

Use dropout => Sure, as long as eval() is used when not training




MLP learning lowest weights!! -> "Fixed" by setting ADAM to max or calculating reparam trick with -w_ij
 -> This is crucial, possible to "hotfix" with -1?

Performance on MUTAG?

How do I calc AUC? Different results for three different metrics

Apply constraints => size+entropy done
Budget and Connectivity only for node?

Mirror edge weights during training? Fix efficient method!

Graph order determines whether learns highest or lowest?!?!
First 10 epochs learns to maximize, after minimizes? Only on some architectures/hyperparams => TRY!

Triple bonds are detect as double bonds, predicted edge should be there?



WEIGHTS TEND TO BE WAY TOO HIGH FOR TREE-CYCLES. -> PROBABILITIES OF EDGES ARE ALL 1.
CAN BE REDUCED VIA L2NORM, ALTHOUGH APPARENTLY NOT USED IN ORIGINAL!
NORMALIZATION ERROR IN MY MODEL? WHERE? POSSIBLE CAUSES?

Take topK(motif) edges for node tasks as well! => Calculate AUC with topK set to 1, others set to 0 vs. GroundTruth



Background: How much into detail? Where to start? Neural Networks? Machine Learning? ...
Mathematical background -> Probably needed, how deep, where to start?
How to quote? Mathematical stuff/formulas quotes? General: Definitions from original works? Own "dictionary"? 

BA2Motif now struggling on 64 architecture. DOES NOT LEARN ANYMORE. Loss fluctuating!

Training of NodeExplainer: Train on motifs? Node splits of original do not really make sense?? Same for AUC evaluation. HOW CAN I GET EVERY MOTIF ONCE FROM MY DATASET??
    OR HOW CAN I VALIDATE ORIGINAL NODE SPLIT TO MAKE SENSE??
    Motfis I use to calculate AUC are Subragph edges where both connected nodes are of class 1.



plot ground truth from original
Train over motif nodes, with node labels of motif class(each computanional graph should contain a motfi to detect it!?).  Compare to training on ALL nodes. 



When training TreeGrid explainer on all nodes: "Detects" motfis as lowest weights?! Probably simply wrong, since model just learns to detect base nodes as important since usually represented?

BA-Community: 3-Hop graph contains multiple motifs. How to calculate topK/compare to gt?!
Order of pSample and pOriginal in Loss

\section{Linear Algebra} with Scalar, Vector, Matrix... needed? 


Weights for Graph classification too low/high -> All one or all zero. Where to normalize? In forward pass?



Switch back pOriginal and pSample in Loss. Log is apllied to the distribution we want to learn.

Think about creating a dataset for SAT

Presentation: Vorschlag no motfis in graph included prediction

Change graph convolution layer. Implement one according to original code? Replace? -> Exact layer should not really matter, downstream task is treated as blackbox

Train/Test split in Explainer? YES

Absolute values to normalization?! Disregards positively learned values as worst?



Try PGExplainer training for BA-2Motif with only class 1 or only class 2
Calculate downstream task accuracy after explainer training to validate, that downstream task is fixed!




Next presentation:
- quick recap

Generalisierung: Modell soll im voraus lernen wieviele Kanten zu Motif zählen bzw. ob Motif überhaupt vorhanden

Nächste Woche paar Folien Zeigen


How to calc stddev in WandB?!?!?!?
Loss.detach() on initialization??
What needs to be detached? What needs to require grad??


How far into detail? Only basics like in Introduction to GNNs or deeper? Graphics?

Embrace the follow up paper? Use configs from follow up that probably work well or use original and say not working well? Both?
If using their configs, what is own work so far?! How to I write about that?
Compare results to results from reimplementation? Yes

Next steps: Try to play around so that BA-2Motif works?? No
Try on SAT anyways? Predict motif size?? -> Try SAT







For NeuroSAT:
Explainer MLP gets all embeddings batched, as it is done in Original. Should work like this, right?
Explicit extraction of sub_problems only for calculation of loss?

How to avoid predicted weights getting closer and closer to 0 instead of staying around the same dimension and varying? Normalization?

How to evaluate Sweeps? Trust Sweep importance scores across 2 seeds? Values obviously dependant on seed.

Writing: PGExplainer explanation in theoretical background and implementation/changes made etc. in main part or everything in main part?

Things done: Sweeps for 5/6 datasets, adapted NeuroSAT and evaluated first runs on 1 sub_problem, ran replication with train/test split

Compared CPU and GPU: Only for BA-2Motif big difference, 3 datasets identical -> Valid to use cpu for 5/6 datasets and gpu only for BA-2Motif? /Does not matter whether cpu/gpu used?

How to dislay standard deviation in wandb???

Replication TODO: Batching for node task to improve performance, validate early stopping and save respective model state for ds task training, Sweep BA-2Motif
NeuroSAT TODO: Evaluate on multiple sub_problems, implement train/test/val split?, Add normalization?, Size of gt passed into qual evaluation: necessary?, Add unsat core to data, Sweep, try different hiddem dim?, MUS?, try with satisfiable problems and backbones as gt?, try on GPU

------------------------------

Logical conclusion of graph probability P(G) for graph generation?; Only direct dependency to Gilbert graph is principle of edge probabilites beeing drawn independently?

PGE formula 5 to 6: Replace conditional entropy with cross entropy?! Explanation in GNNExplainer: "We can modify the conditional entropy objective in Equation 4 with a cross entropy objective between the label class and the model prediction"

Convention citing: Seitenzahlen? Chapter?


random graphs !?! Gilbert: "every possible edge occurs independently with probability 0 < p < 1"
Is that what happens in PGE? edges occur independetly, but probabilities are learned/differ from edge to edge? Or only relevant for defintion; instantiation with bernoulli == what Gilbert definded?
IDEA: instantiation as bernoulli is "constant", not learned. Sampling process: ALWAYS draws from random(0,1), learned weight only added and therefore influences the outcome.
Eq. 3: At this point we only randomly sample a graph and define the goal as the probability that the label is the same for the sampled graph?
-> Graph probability == epsilon?
=> Random graphs/Gilbert model explanation satisfies PGE background, general probabilistic graphical model not needed?


Normalization already present in NeuroSAT, embeddings should be normalized. Additional normalization leads to slight decrease in AUC, does not lead to larger probabilites 


UNSAT core calculation assumptions literature o.ä.

Gilbert graph / Diestel definition: Definition with p is correct for different p per edge? "Fixed probability" for each experiment means a) fixed for all edges or b) fixed for edge e across all Graphs on V?

To PGE: "A straightforward instantiation of P (eij ) is the Bernoulli distribution eij ∼ Bern(θij )" What is Teta ij?

"In the random graph proposed by Gilbert, each potential edge is independently chosen from a Bernoulli distribution" -> Wrong!? Since Gilbert graph also uses same probability for each edge?


Explanations from Lucas:
Intuition: conditional entropy only for one concrete sampled graph G_s, cross entropy for two probability distributions and therefore over distribution of all sampled graphs

Gilbert random graph: PGExplainer uses base idea of Gilbert random graph, but adapts it for varying probabilites per edge -> Independent experiments with DIFFERENT probabalities


------------------------------


When to use detach()? detach() original predictions?