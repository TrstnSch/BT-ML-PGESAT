\chapter{PGExplainer - Methodology}
\label{ch:PGExplainer}
%V1: In the following chapter, we introduce the PGExplainer\cite{luo2020parameterized} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models for any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. This approach seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{ying2019gnnexplainer}.

%V2: In the following chapter, we introduce the PGExplainer\cite{luo2020parameterized} and its concepts. The idea is to generate explanations in the form of probabilistic graph generative models that have proven to learn the concise underlying structures of GNNs most relevant to their predictions. This approach may be applied to any learned GNN model, henceforth referred to as the downstream task (DT), by utilizing a deep neural network to parameterize the generation process. PGExplainer seeks to explain multiple instances collectively, as they share the same neural network parameters, and therefore improve the generalizability to previous works, particularly the GNNExplainer\cite{ying2019gnnexplainer}. This means that all edges in the dataset are predicted by the same model, which leads to a global understanding of the DT.

In this chapter we present the Methodology (TODO: ?) of our work, as well as the detailed theory of the explainer model used in our approach. Therefore, we start by presenting the concept of the Parameterized explainer for graph neural networks by Luo et al. \cite{luo2020parameterized} in \ref{sec:TheoryPGE}.

In section \ref{} we present our idea of applying the PGExplainer on the NeuroSAT framework to generate explanations for a machine learning SAT-solving approach and comparing these to "human-understandable" concepts like UNSAT cores. 

We then describe our imlpementation in detail (section \ref{}), including the changes made and difficulties during the process.

In section \ref{} we describe the experimental setups for both the replication of the PGExplainer in the inductive setting, as well as the application on NeuroSAT.

\section{Theoretical Foundations of PGExplainer}
\label{sec:TheoryPGE}

In the following subchapter, we introduce the PGExplainer by Luo et al. \cite{luo2020parameterized} and its concepts. The idea is to generate explanations in the form of edge distributions or soft masks using a probabilistic generative model for graph data, known for being able to learn the concise underlying structures from the observed graph data. The explainer uncovers said underlying structures, believed to have the biggest impact on the prediction of a GNNs, as explanations. This approach may be applied to any trained GNN model, henceforth referred to as the target model (TM). 
By utilizing a deep neural network to parameterize the generation process, the explainer learns to collectively explain multiple instances of a model. Since the parameters of the neural network are shared across the population of explained instance, PGExplainer provides "model-level explanations for each instance with a global view of the GNN model" \cite{luo2020parameterized}. Furthermore, this approach cannot only be used in a collective setting, but also in an inductive setting, where explanations for unexplained nodes can be generated without retraining the explanation model. This improves the generalizability compared to previous works, particularly the GNNExplainer by Ying et al. \cite{ying2019gnnexplainer}.


TODO: The focus in this approach lies in explaining the graph structure, rather than the graph features, as feature explanations are already common in non-graph neural networks. \bigskip

We follow the structure of the original paper \cite{luo2020parameterized} and start by describing the learning objective in \ref{sec:learning_objective}, the utilized reparameterization trick in \ref{sec:Reparameterization_Trick}, the idea of global explanations in \ref{sec:Global_Explanations} and finally the applied regularization terms in \ref{sec:Regularization_Terms}.

\subsection{Learning Objective}
\label{sec:learning_objective}
To explain the predictions made by a GNN model for an original input graph $G_o$ with $m$ edges we first define the graph as a combination of two subgraphs: $G_o = G_s + \Delta G$, where $G_s$ represents the subgraph holding the most relevant information for the prediction of a GNN, referred to as explanatory graph. $\Delta G$ contains the remaining edges that are deemed irrelevant for the prediction of the GNN. Inspired by GNNexplainer \cite{ying2019gnnexplainer}, the PGExplainer then finds $G_s$ by maximizing the mutual information between the predictions of the target model and the underlying $G_s$:
\begin{equation}
    \max_{G_s} MI(Y_o;G_s) = H(Y_o) - H(Y_o|G=G_s),
\end{equation} 
where $Y_o$ (TODO: $\in (0,1)^c$ ??) is the prediction of the target model with $G_o$ as input and number of possible classes $c$. This quantifies the probability of prediction $Y_o$ when the input graph is restricted to the explanatory graph $G_s$, as in the case of $I(Y_o;G_s) = 1$, knowing the explanatory graph $G_s$ gives us complete information about $Y_o$, and vice versa. Intuitively, if removing an edge $(i,j)$ changes the prediction of a GNN drastically, this edge is considered important and should therefore be included in $G_s$. This idea originates from traditional forward propagation based methods for whitebox explanations (see Dabkowski et al. \cite{dabkowski2017real}).
It is important to note that $H(Y_o)$ is only related to the target model with fixed parameters during the evaluation/explanation stage. This leads to the objective being equivalent to minimizing the conditional entropy $H(Y_o|G=G_s)$.

To optimize this function a relaxation is applied for the edges, since normally there would be $2^m$ candidates for $G_s$. The explanatory graph is henceforth assumed to be a Gilbert random graph, where the selections of edges from $G_o$ are conditionally independent to each other. However, the authors describe a random graph with each edge having its own probability, rather than a shared probability as described in \ref{sec:random-graphs}, as follows: Let $e_{ij}\in V \times V$ be the binary variable indicating whether the edge is selected, with $e_{ij} = 1$ if edge $(i,j)$ is selected to be in the graph, and 0 otherwise. For the random graph variable $G$ the probability of a graph $G$ can be factorized as 
\begin{equation}
    P(G) = \prod_{(i,j)\in E}P(e_{ij}).
\end{equation}
TODO: Inhomogeneous Erdos Renyi model? Mention that this is a generative model? (A Gilbert random graph is an example of a generative probabilistic model on graph data?)

 $P(e_{ij})$ is instantiated with the Bernoulli distribution $e_{ij} \sim Bern(\theta_{ij})$, where $P(e_{ij} = 1) = \theta_{ij}$ is the probability that edge $(i,j)$ exists in $G$.
After this relaxation the learning objective becomes:
\begin{equation}
    \label{eq:init_learning_obj}
    \min_{G_s}H(Y_o|G = G_s) = \min_{G_s} \mathbb{E}_{G_s}[H(Y_o|G = G_s)] \approx \min_{\Theta} \mathbb{E}_{G_s \sim q(\Theta)}[H(Y_o|G = G_s)],
\end{equation}
where $q(\Theta)$ is the distribution of the explanatory graph that is parameterized by $\Theta$'s.

\subsection{Reparameterization Trick}
\label{sec:Reparameterization_Trick}
As described in section \ref{sec:gnn_explainability}, a reparameterization trick can be utilized to relax discrete edge weights to continuous variables in the range $(0,1)$. PGExplainer uses the reparameterizable Gumbel-Softmax estimator \cite{jang2016categorical} to allow for efficiently optimizing the objective function with gradient-based methods. This method introduces the Gumbel-Softmax distribution, a continuous distribution used to approximate samples from a categorical distribution. A temperature $\tau$ is used to control the approximation, usually starting from a high value and annealing to a small, non-zero value. Samples with $\tau > 0$ are not identical to samples from the corresponding continuous distribution, but are differentiable and therefore allow back-propagation. The sampling process $G_s \sim q(\Theta)$ of PGExplainer is therefore approximated with a determinant function that takes as input the parameters $\Omega$, a temperature $\tau$ and an independent random variable $\epsilon$: $G_s \approx \hat{G}=f_\Omega(G_o,\tau,\epsilon)$. The binary concrete distribution \cite{maddison2016concrete}, also referred to as Gumbel-Softmax distribution, is utilized as an instantiation for the sampling, yielding the weight $\hat{e}_{ij} \in (0,1)$ for edge $(i,j)$ in $\hat{G}_s$, computed by:
\begin{equation}
    \label{eq:reparam_trick}
    \epsilon \sim \text{Uniform}(0,1), \qquad \hat{e}_{ij}=\sigma((\log \epsilon - \log(1-\epsilon)+\omega_{ij}/\tau),
\end{equation}
where $\sigma(\cdot)$ is the Sigmoid function and $\omega_{ij} \in \mathbb{R}$ is an explainer logit for the corresponding edge used as a parameter. When $\tau \rightarrow 0$, e.g. during the explanation stage, the weight $\hat{e}_{ij}$ is binarized with the sigmoid function $\lim_{\tau\rightarrow 0}P(\hat{e}_{ij} = 1) = \frac{\exp (\omega{ij})}{1+\exp (\omega{ij})}$. Since $P(e_{ij} = 1) = \theta_{ij}$, choosing $\omega_{ij} = \log\frac{\theta_{ij}}{1-\theta_{ij}}$ leads to $\lim_{\tau\rightarrow 0}\hat{G}_s = G_s$ and justifies the approximation of the Bernoulli distribution with the binary concrete distribution. During training, when $\tau > 0$, the objective function in \eqref{eq:init_learning_obj} is smoothed with a well-defined gradient $\frac{\partial\hat{e}_{ij}}{\partial\omega_{ij}}$ and becomes:
\begin{equation}
    \min_\Omega \mathbb{E}_{\epsilon \sim \text{Uniform}(0,1)}H(Y_o| G = \hat{G}_s)
\end{equation}

The authors follow the approach of GNNExplainer \cite{ying2019gnnexplainer} and modify the objective by replacing the conditional entropy with cross entropy between the label class and the prediction of the target model. This is justified by the greater importance of understanding the model's prediction of a certain class, rather than providing an explanation based solely on its confidence.

With the modification to cross-entropy $H(Y_o, \hat{Y}_s)$, where $\hat{Y}_s$ is the prediction of the target model when $\hat{G}_s$ is given as input, as well as the adaption of Monte Carlo sampling, the learning objective becomes:
\begin{equation}
    \label{eq:monte_carlo}
    \begin{aligned}
        \min_\Omega\mathbb{E}_{\epsilon\sim\text{Uniform}(0,1)}H(Y_o, \hat{Y}_s) &\approx \min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P(Y_o = c) \log P(\hat{Y}_s = c) \\
        &= \min_\Omega -\frac{1}{K}\sum_{k=1}^K\sum_{c=1}^C P_\Phi (Y_o = c|G = G_o) \log P_\Phi(\hat{Y}_s = c|G=\hat{G}_s^{(k)}).
    \end{aligned}
    \end{equation}
$\Phi$ denotes the parameters in the target model, $K$ is the number of total sampled graphs, $C$ is the number of class labels, and $\hat{G}_s^{(k)}$ denotes the $k$-th graph sampled with equation \ref{eq:reparam_trick}, parameterized by $\Omega$. %Note that this objective is defined per explainable instance.



%The approach in PGExplainer is a common approach in ML for simplifying objectives? FIND LITERATURE THAT EXPLAINS APPROXIMATION OF COND. ENTROPY WITH CROSS ENTROPY. Explanation as simple as one formula for one graph variable example, cross entropy applied to whole distribution? \bigskip


\subsection{Global Explanations}
\label{sec:Global_Explanations}
The novelty of PGExplainer lies in the ability to generate explanations for graph data with a global perspective, that allow for understanding the general picture of a model across a population. This saves resources when analyzing large graph datasets, as new instances can be explained without retraining the model, and can also be helpful for establishing the users' trust in these explanations. To achieve this the authors propose the use of a parameterized network that learns to generate explanations from the target model, which also apply to not yet explained instances. 

Since GNNs apply two functions $F$ and $G$ to calculate the global state embeddings and downstream task outputs respectively, we denote these two functions as $\text{GNNE}_{\Phi_0}(\cdot)$ and $\text{GNNC}_{\Phi_1}(\cdot)$ for any GNN in the context of PGExplainer. For models without explicit classification layers the last layer is used to compute the output instead. It follows
\begin{equation}
    \mathbf{Z} = \text{GNNE}_{\Phi_0}(G_o, \mathbf{X}), \qquad Y = \text{GNNC}_{\Phi_1}(\mathbf{Z}),
\end{equation}
where $\mathbf{Z}$ denotes the matrix of final node representations $z$, referred to as node embeddings, and the initial state is $G_o$. TODO: (Because of focus on graph structure rather than features?) For generalizability across different GNN layers the output is only dependent on the node representation, that encapsulates both features and structure of the input graph. This representation also serves as the input for the explainer network $g$, defined as:
TODO: Rename g?
\begin{equation}
    \label{eq:explainer_network}
    \Omega = g_\Psi(G_o,\mathbf{Z}).
\end{equation}
$\Psi$ denotes the parameters in the explanation network and the output $\Omega$ is treated as parameter in equation \ref{eq:monte_carlo}. Since $\Psi$ is shared by all edges among the population, PGExplainer collectively provides explanations for multiple instances. Thus, the learning objective in a collective setting with $\mathcal{I}$ being the set of instances becomes:
\begin{equation}
    \min_\Psi -\frac{1}{K}\sum_{i\in \mathcal{I}}\sum_{k=1}^K\sum_{c=1}^C P_\Phi (Y_o = c|G = G_o^{(i)}) \log P_\Phi(\hat{Y}_s = c|G=\hat{G}_s^{(i,k)}).
\end{equation}
Consequently, $G^{(i)}$ and $G_s^{(i,k)}$ denote the input graph and the $k$-th graph sampled with equation \ref{eq:reparam_trick} in \ref{eq:explainer_network} respectively for instance $i$. The full pipeline of the PGExplainer can be observed in figure \ref{fig:PGExplainer_pipeline}. 

The authors propose two slightly different instantiations of $\Omega$ for node classification and graph classification tasks.

\textbf{Explanation network for graph classification}

For graph level tasks, the authors consider each graph to be an instance, independent  of specific nodes. The output $\Omega$ of the network in \ref{eq:explainer_network} is thus specified as:
\begin{equation}
    \label{eq:mlp_graph_input}
    \omega_{ij} = \text{MLP}_\Psi ([\mathbf{z}_i\oplus\mathbf{z}_j]),
\end{equation}
where $\text{MLP}_\Psi$ is an MLP ( see \ref{} for implementation details) parameterized with $\Psi$ and $\oplus$ denotes the concatenation operation. Effectively, for each edge in $G_o$ a concatenation of both its nodes is fed through the MLP. The output $\omega_{ij}$ is therefore an edge logit (TODO: NOT IN GENERAL? ONLY FOR THE LATER SPECIFIED MLP?), which serves as a parameter in the sampling process.

\textbf{Explanation network for node classification}
%Since explanations for different nodes may not share a common explanation pattern, especially for nodes with different labels, ... DOES THIS NOT ALSO APPLY FOR GRAPHS?!

For node level tasks on the other hand, each prediction node is considered as an instance. Let an edge $(i,j)$ be considered relevant for the prediction of a node $u$, but irrelevant for the prediction of a node $v$. To explain the prediction of node $v$ the authors specify the output $\Omega$ of the network in \ref{eq:explainer_network} as:
\begin{equation}
    \omega_{ij} = \text{MLP}_\Psi ([\mathbf{z}_i\oplus\mathbf{z}_j\oplus\mathbf{z}_v]).
\end{equation}
Thus, a concatenation of the node embeddings of nodes $i, j$ and $v$ respectively is fed through the network.

Note that in their codebase the authors use a concatenation of all hidden representations instead of solely the final representation as node embeddings for node level tasks.
For a target GNN consisting of $L$ graph layers with
\begin{align*}
    \mathbf{H}_1 &= F_1(G_o, \mathbf{X}), \\
    \mathbf{H}_2 &= F_2(\mathbf{H}_1, \mathbf{X}), \\
    &\vdots \\
    \mathbf{H}_L &= F_L(\mathbf{H}_{L-1}, \mathbf{X}), \\
\end{align*}
this leads to $\mathbf{Z} \in \mathbb{R}^{V(G_o)\times (Ld)}$ being the matrix of node embeddings $\mathbf{z}$ that are computed as:
\begin{equation}
    \mathbf{z}_i = \mathbf{h}_{1,i} \oplus \mathbf{h}_{2,i} \oplus ... \oplus \mathbf{h}_{L,i},
\end{equation}
with $\mathbf{h}_{L,i}$ denoting the hidden representations of node $i$ in layer $L$\bigskip

Furthermore, it is important to note, that for target GNNs utilizing a message passing mechanism, the prediction at node $v$ is fully determined by its local computation graph. This is defined by its $L-$hop neighborhood $\mathcal{N}_L(v)$ \cite{ying2019gnnexplainer}. TODO: SEE FIGURE\bigskip

\textbf{Collective and inductive setting}

Due to the nature of its predecessor GNNExplainer, the authors main focus was the application in a collective setting, where the goal is to explain a full population of instances by training on all these and thus being able to provide explanations for every single one. However, since the PGExplainer utilizes a deep neural network to parameterize the generation process of explanations for a population, it can be utilized in an inductive setting, unlike its predecessor. This means, that explanations can be generated for instances from the same population, that have not been seen during training. Thus, it is not necessary to retrain the explainer for new instances of the same population, effectively reducing the computational complexity by the training time when compared to the GNNExplainer.

We discuss the results of the inductive performance study by the authors in \ref{}.


%"PGExplainer learns a latent variable for each edge in the original input graph with a neural network parameterized by Ψ, which is shared by all edges in the population of input graphs." "In GNNExplainer, the parameter size is linear to the number of edges"

This leads to an improved computational complexity when compared to their baseline GNNExplainer, since for one the number of parameters in the explainer does no longer depend on the size of the input graph and since for another the explainer does not have to be retrained for every unexplained instance.

\tikzstyle{process} = [rectangle, rounded corners, minimum width=3cm, minimum height=1cm, text centered, draw=black, fill=purple!30, font=\small]
\tikzstyle{module} = [rectangle, rounded corners, minimum width=3cm, minimum height=1.2cm, text centered, draw=black, fill=gray!30, font=\small]
\tikzstyle{data} = [rectangle, sharp corners, minimum width=1.5cm, minimum height=1cm, text centered, draw=black, fill=cyan!30, font=\small]
\tikzstyle{emb} = [rectangle, sharp corners, minimum width=1.5cm, minimum height=1cm, text centered, draw=black, fill=yellow!30, font=\small]
\tikzstyle{arrow} = [very thick,->,>=Stealth]    %very thick

\begin{figure}
\centering
\begin{tikzpicture}[node distance=0.9cm and 1.2cm]

\tikzset{
  mymini/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (b) -- (x);
    \draw[-] (a) -- (x);
  }
}

\tikzset{
  mymini3/.pic={
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$z_i$] (x) at (0,0) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=above:$z_j$] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$z_k$] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, minimum size=4pt, inner sep=2pt, label=left:$z_l$] (b) at (0,-0.5) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (b) -- (x);
    \draw[-] (a) -- (x);
  }
}

\tikzset{
  mymini2/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- node[midway, right, font=\scriptsize] {$0.9$} (y);
    \draw[-] (a) -- node[midway, above, font=\scriptsize] {$0.8$}(y);
    \draw[-] (b) -- node[midway, left, font=\scriptsize] {$0.1$}(x);
    \draw[-] (a) -- node[midway, left, font=\scriptsize] {$0.9$}(x);
  }
}

\tikzset{
  mymini4/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- node[midway, right, font=\scriptsize] {$z_{ij}$} (y);
    \draw[-] (a) -- node[midway, above, font=\scriptsize] {$z_{jk}$}(y);
    \draw[-] (b) -- node[midway, left, font=\scriptsize] {$z_{il}$}(x);
    \draw[-] (a) -- node[midway, left, font=\scriptsize] {$z_{ik}$}(x);
  }
}

\tikzset{
  mymini5/.pic={
    \node[circle, draw, fill=black, inner sep=2pt] (x) at (0,0) {};
    \node[circle, draw, fill=black, inner sep=2pt] (y) at (0.5,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (a) at (0,0.5) {};
    \node[circle, draw, fill=black, inner sep=2pt] (b) at (0,-0.5 ) {};
    \draw[-] (x) -- (y);
    \draw[-] (a) -- (y);
    \draw[-] (a) -- (x);
  }
}


% Nodes
\node (input) [data] {\small Input Graph $G_o$};
\pic at ([xshift=7cm]input.center) {mymini};
\node (target) [module, below=of input] {Target GNN};
\node (embeddings) [emb, below=of target] {Node Embeddings $\mathbf{Z}$ of $G_o$};
\node[anchor=center] at ([xshift=4cm]embeddings.center) 
 (node_embs) 
 {$\begin{bmatrix} z_i \\ z_j \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of node_embs, anchor=west] 
 {\scriptsize $[|V(G_o)|]$};
\pic at ([xshift=7cm]embeddings.center) {mymini3};
\node (embedding_transformation) [process, below=of embeddings] {Edge Embedding Transformation};
\node (edge_embeddings) [emb, below=of embedding_transformation] {Edge Embeddings of $G_o$};
\node[anchor=center] at ([xshift=4cm]edge_embeddings.center) 
 (edge_embs) 
 {$\begin{bmatrix} z_{ij} \\ z_{jk} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of edge_embs, anchor=west] 
 {\scriptsize $[|E(G_o)|]$};
\pic at ([xshift=7cm]edge_embeddings.center) {mymini4};
\node (explainer) [module, below=of edge_embeddings] {PGExplainer MLP};
\node (logits) [emb, below=of explainer] {Edge Logits/Latent variables $\Omega$};
\node[anchor=center] at ([xshift=4cm]logits.center) 
 (omega) 
 {$\begin{bmatrix} \omega_{ij} \\ \omega_{jk} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of omega, anchor=west] 
 {\scriptsize $[|E(G_o)|]$};
\node (trick) [process, below=of logits] {Reparameterization Trick};
\node (weights) [emb, below=of trick] {Sampled edge importance weights};
\node[anchor=center] at ([xshift=4cm]weights.center) 
 (edge_score) 
 {$\begin{bmatrix} \hat{e}_{ij} \\ \hat{e}_{jk} \\ \vdots \end{bmatrix}$};
\node[right=0.01cm of edge_score, anchor=west] 
 {\scriptsize $[|E(G_o)|]$};

\node (sampled_graph) [data, below= of weights] {Sampled graph $\hat{G}_s$};
\pic at ([xshift=7cm]sampled_graph.center) {mymini2};

\node (sample_target) [module, below=of sampled_graph] {Target GNN};
\node (original_target) [module, left=2cm of sample_target] {Target GNN};
\node (sample_prediction) [data, below=of sample_target] {$\hat{Y}_s$};
\pic at ([xshift=7cm]sample_prediction.center) {mymini5};
\node (original_prediction) [data, below=of original_target] {$Y_o$};

\node (topK) [process, right=of sample_target] {Sample top-$k$ edges};
\node (explanation) [data, below=of topK] {$G_s$};

%\node[draw=red, thick, dashed, fit=(input) (target) (embeddings), label=above:input Block] {};

%\begin{pgfonlayer}{background}
%    \node[draw=gray, thick, rounded corners, fit=(edge_embeddings) (weights), fill=blue!10, label=above:Sampling of PGExplainer] {};
%\end{pgfonlayer}

\begin{pgfonlayer}{background}
    \node[draw=gray, thick, rounded corners, fit=(original_target) (sample_target) (original_prediction) (sample_prediction), fill=orange!30] (backgroundtraining) {};
    \node[
        rotate=90, 
        anchor=center
    ] at ([xshift=-3mm]backgroundtraining.west) {Training};
\end{pgfonlayer}

\begin{pgfonlayer}{background}
    \node[draw=gray, thick, rounded corners, fit=(topK) (explanation), fill=green!30] (backgroundevaluation) {};
    \node[
        rotate=90, 
        anchor=center
    ] at ([xshift=-3mm]backgroundevaluation.west) {Evaluation};
\end{pgfonlayer}

\coordinate (weight_sample_mid) at ($ (weights)!0.5!(sampled_graph) $);
\coordinate (left_of_input) at ($ (input) + (-3.5cm, 0) $);
\coordinate (right_of_input) at ($ (original_target |- input) $);
\coordinate (drop) at (left_of_input |- weight_sample_mid);
\coordinate (right_of_sampled_graph) at ($ (topK |- sampled_graph) $);

% Arrows
\draw [arrow] (input) -- (target);
\draw [arrow] (target) -- (embeddings);
\draw [arrow] (embeddings) -- (embedding_transformation);
\draw [arrow] (embedding_transformation) -- (edge_embeddings);
\draw [arrow] (edge_embeddings) -- (explainer);
\draw [arrow] (explainer) -- (logits);
\draw [arrow] (logits) -- (trick);
\draw [arrow] (trick) -- (weights);

%\draw [arrow] (input.west) --  ++(-2cm,0) -- ($ (input) + (-2cm,0) $ |- sampled_graph) -- (sampled graph.west);
\draw [arrow] (weights) -- (sampled_graph);

\draw[arrow] 
  (input) -- (left_of_input) 
       -- (drop) 
       -- (weight_sample_mid);

\draw[arrow] 
  (left_of_input) -- (right_of_input) 
       -- (original_target);

\draw [arrow] (sampled_graph) -- (sample_target);
\draw [arrow] (sample_target) -- (sample_prediction);

\draw [arrow] (original_target) -- (original_prediction);

\draw [arrow] (sampled_graph) -- (right_of_sampled_graph) -- (topK);

\draw [arrow] (topK) -- (explanation);

\draw [<->, dashed] (sample_prediction) -- node[midway, above, font=\scriptsize] {$\min_\Omega H(Y_o,\hat{Y}_s)$} (original_prediction);

\end{tikzpicture}
\caption{TODO: The complete pipeline of PGExplainer.}
\label{fig:PGExplainer_pipeline}
\end{figure}

\begin{figure}
\centering
\begin{tikzpicture}
    
    \node (node_emb_i1) [data] {$z_i$};
    \node (node_emb_j1) [data, right=0.5cm of node_emb_i1] {$z_j$};
    
    \coordinate (i_j_mid) at ($ (node_emb_i1)!0.5!(node_emb_j1) $);
    
    \node (concat) at ($(i_j_mid) + (0,-1)$) {\Large $\oplus$};
    
    \node (edge_emb_ij) [data] at ($(i_j_mid) + (0,-2)$) {Edge Embedding of $(i,j)$};
    
    
    \node (node_emb_i2) [data, right=3cm of node_emb_j1] {$z_i$};
    \node (node_emb_j2) [data, right=0.5cm of node_emb_i2] {$z_j$};
    \node (node_emb_v2) [data, right=0.5cm of node_emb_j2] {$z_v$};
    
    \coordinate (i_v_mid) at ($ (node_emb_i2)!0.5!(node_emb_v2) $);
    
    \node (concat2) at ($(i_v_mid) + (0,-1)$) {\Large $\oplus$};
    
    \node (edge_emb_ijv) [data] at ($(i_v_mid) + (0,-2)$) {Edge Embedding of $(i,j)$ with target node $v$};


    \node (z_i) [data, above=2cm of node_emb_j2] {$z_i$};

    \node (h_2) [data, above=1cm of z_i] {$h_{2,i}$};
    \node (h_1) [data, left=0.5cm of h_2] {$h_{1,i}$};
    \node (h_3) [data, right=0.5cm of h_2] {$h_{3,i}$};
 
    \node (O) [module, above=1cm of h_2] {$O$};
    \node (F_3) [module, above=0.2cm of O] {$F_3$};
    \node (F_2) [module, above=0.2cm of F_3] {$F_2$};
    \node (F_1) [module, above=0.2cm of F_2] {$F_1$};
    \node (H_2) [data, right=0.5cm of F_2] {$H_2 = [h_{2,i}, h_{2,j},...]$};
    \node (H_1) [data, right=0.5cm of F_1] {$H_1 = [h_{1,i}, h_{1,j},...]$};
    \node (H_3) [data, right=0.5cm of F_3] {$H_3 = [h_{3,i}, h_{3,j},...]$};

    \node (input) [data, above=1cm of F_1] {$G_o$};

    \node (concat3) at ($ (h_2 |- z_i) + (0,1)$) {\Large $\oplus$};

    \begin{pgfonlayer}{background}
        \node[draw=gray, thick, rounded corners, fit=(F_1) (O) (H_1), fill=gray!30, inner sep=20pt, label=above:\text{Target GNN}] {};
    \end{pgfonlayer}



    \begin{pgfonlayer}{background}
        \node[draw=purple, dashed, rounded corners, fit=(node_emb_i1) (node_emb_j1) (node_emb_i2)(node_emb_v2) (edge_emb_ijv), inner sep=20pt, label=above:\text{Edge Embedding Transformation}] {};
    \end{pgfonlayer}
    
    \begin{pgfonlayer}{background}
        \node[draw=gray, thick, rounded corners, fit=(node_emb_i1) (node_emb_j1) (edge_emb_ij), fill=red!30, label=above:Graph Task] {};
    \end{pgfonlayer}
    
    \begin{pgfonlayer}{background}
        \node[draw=gray, thick, rounded corners, fit=(node_emb_i2) (node_emb_v2) (edge_emb_ijv), fill=orange!30, label=above:Node Task] {};
    \end{pgfonlayer}
\end{tikzpicture}
\caption{TODO: Visualization of the edge embedding transformation used to create inputs for the explainer network. Depending on the downstream task used in the target model the created edge embedding differs slightly.}
\end{figure}

\subsection{Regularization Terms}
\label{sec:Regularization_Terms}
To enhance the preservation of desired properties of explanations the authors propose various regularization terms. These are added to the learning objective, depending on the specific downstream task at hand.\bigskip

\textbf{Size and entropy constraints}

Inspired by GNNExplainer \cite{ying2019gnnexplainer}, to obtain compact and precise explanations, a constraint on the size of the explanations is added in the form of $||\Omega||_1$, the $l_1$ norm on latent variables $\Omega$. Additionally, to encourage the discreteness of edge weights, element-wise entropy is added as a constraint:
\begin{equation}
    H_{\hat{G}_s} = -\frac{1}{|\varepsilon|}\sum_{(i,j)\in \varepsilon} (\hat{e}_{ij}\log \hat{e}_{ij} + (1-\hat{e}_{ij})\log(1-\hat{e}_{ij})),
\end{equation}
for one explanatory graph $\hat{G_s}$ with $\varepsilon$ edges. For the collective setting, this is added as a mean over all instances in $\mathcal{I}$. \bigskip

Note that the following two constraints are not used in the original experimental setup, but serve as inspiration for constraints introduced in our NeuroSAT application\ref{} and are therefore included. \bigskip

\textbf{Budget constraint}

The authors propose the modification of the size constraint to a budget constraint, for a predefined available budget $B$. Let $|\hat{G}_s| \leq B$, then the budget regularization is defined as:
\begin{equation}
    R_b = \text{ReLU}(\sum_{(i,j)\in \varepsilon}\hat{e}_{ij}-B).
\end{equation}
Note that $R_b = 0$ when the explanatory graph is smaller than the budget. When out of budget, the regularization is similar to that of the size constraint. \bigskip

\textbf{Connectivity constraint}

To enhance the effect of the explainer detecting a small, connected subgrap, motivated through real-life motifs being inherently connected, the authors suggest adding the cross-entropy of adjacent edges. Let $(i,j)$ and $(i,k)$ be two edges that both connect to the node $i$, then $(i,k)$ should rather be included in the explanatory graph if the edge $(i,j)$ is selected to be included. This is formally defined as:
\begin{equation}
    H(\hat{e}_{ij},\hat{e}_{ik}) = -[1-\hat{e}_{ij}\log(1-\hat{e}_{ik})+\hat{e}_{ij}\log \hat{e}_{ik}].
\end{equation}
We note that in practice this is implemented only for the two highest edge weights for each edge. The definition therefore would change to $(i,j)$ and $(i,k)$ being the edges carrying the top two edge weights from the nodes connecting to node $i$.


\section{Extension to application on NeuroSAT}
In this section we propose additional restraints to fit the explanations of PGExplainer to the structure of SAT formulae. We start by giving a short introduction to NeuroSAT\cite{selsam2018learning} and how it may function as a downstream model.

 \bigskip
Proposed by Selsam et al. \cite{selsam2018learning}, NeuroSAT utilizes a MPNN, generally utilized to express GNNS, to solve SAT formulae. It is able to generalize in the sense that it may solve substantially larger and more difficult formulae than seen during training by running for more iterations. Though it may be used to calculate variable assignments that satisfy a formula, it is unable to provide proofs for formulae that are unsatisfiable.

In a separate experiment the authors were able to make NeuroSAT identify specific unsatisfiable cores, however this is assumed to be due to the network memorizing the subgraphs rather than learning a generic procedure that proves unsatisfiability. To verify this we aim to generate explanations for unsatisfiable formulae processed by a trained NeuroSAT model.

%Let $n$ denote the number of variables in a SAT formula and $m$ the number of clauses.
Since the following restraints are tailored to the application on bipartite graphs representing SAT formulae, we have to define the specifics of the input graph. A formula is encoded as an undirected bipartite graph $G_b$, with bipartition $(L,C)$. It contains one node for every literal $l \in L$, one for every clause $c \in C$ and edges for all combinations $(l,c)$ where $l$ appears in clause $c$. Additionally, connections exist between each literal and its negation, since messages are also passed along these (TODO: RELEVANT?). Note that these edges are not present in the biadjacency matrix $\mathbf{B}\in (0,1)^{L\times C}$. $\mathbf{B}$ is used as input for the NeuroSAT model, without explicit node features. In the following definitions we let $G_b$ be the original input graph for the PGExplainer, completely defined by its biadjacency matrix $\mathbf{B}$.

%aggregation: sum the outgoing messages of each of a node’s neighbors to form the incoming message

Since PGExplainer generates edge wise explanations, and we want to evaluate the SAT problem evaluations with unsatisfiable cores as ground truth, we need to adapt the framework to account for the definition of unsatisfiable cores. Since a core is a subset of clauses, predicting singular edges that represent a literal being present in a clause, may not provide sufficient results in the sense of human understandable explanations. Therefore, we propose a soft and a hard restraint that encourage the explainer to predict sets of edges that connect to the same node $c$, approximating the prediction of a complete clause.

The remainder of the explainer pipeline stays identical. The downstream task - NeuroSAT - calculates hidden node representations $\mathbf{h}^t$ for the input graph $G_b$, now modeling a SAT instance, at each iteration $t$. The representations in the last iteration $T$ are extracted as node embeddings $\mathbf{z}$ for clause and literal nodes respectively, and transformed into edge embeddings (see equation \ref{eq:mlp_graph_input}). Though this is only done for node level tasks in the original, we also consider using a concatenation of multiple hidden representations $\mathbf{h}^{\frac{1}{2}T} \oplus \mathbf{h}^{\frac{3}{4}T} \oplus \mathbf{h}^T$ as node embeddings $\mathbf{z}$. These serve as the input of the explainer MLP and are processed as usual, with either of the following additional limitations. \bigskip


\textbf{Soft modified connectivity constraint}

To account for the definition of unsatisfiable cores - a subset of clauses in the original formula whose conjunction is still unsatisfiable - we add a constraint that reinforces the prediction of complete clauses. Therefore, if the explainer assigns a high score to an edge $(l_1,c)$, all edges $(l_k,c) \in E(c)$ that also connect to the clause node $c$ should receive a high score. Therefore, we introduce a soft constraint that punishes varying edge weights for the same clause. For our sampled bipartite Graph $\hat{G}_s$ with node sets $L$ and $C$ containing literal nodes and clause nodes respectively, we define:
\begin{equation*}
    R_C = \sum_{c \in C}  \text{Var}(\hat{E}_c) = \sum_{c \in C} \frac{1}{|E(c)|} \sum_{(l,c) \in E(c)} (\hat{e}_{l,c} - \bar{E_c})^2,
\end{equation*}
where $\hat{E}_c = \{\hat{e}_{l,c} \mid (l,c)\in E(\hat{G}_s)\}$ is the set of edge weights corresponding to edges incident to $c$ and $\bar{E_c} = \frac{1}{|\hat{E}_c|}\sum_{\hat{e}_{l,c} \in \hat{E}_c} \hat{e}_{l,c}$ denotes the mean of $\hat{E}_c$. TODO: $\mu_{\hat{e}_c}$ This is added to our objective function during training. \bigskip


\textbf{Hard constraint}

Since the soft constraint only encourages the prediction of entire clauses but does not enforce it, we also propose a hard constraint that modifies the predicion process. We restrain the edge logits $\omega_{i,j}$ calculated by the MLP to be identical for all edges that connect to the same clause. %Let $\Omega_c = \{\hat{\omega}_{i,c} \mid (i,c)\in E\}$ denote the set of logits corresponding to the edges connected to node $c$. We update these logits with:
%\begin{equation}
%    \mu_c = \frac{1}{|\Omega_c|} \sum_{\hat{\omega}_{i,c} \in \Omega_c} \hat{\omega}_{i,c}
%\end{equation}
For all clause nodes $c \in C$, we calculate the mean logit $\mu_c$ (TODO: OR $ \mu_{\omega_c}$) of all edges incident to $c$ with
\begin{equation}
    \mu_c = \frac{1}{|E(c)|} \sum_{(l,c)\in E(c)}\omega_{l,c}.
\end{equation}
The update rule is then defined as 
%\begin{equation}
%    \omega_{l,c}' =\mu_c, \qquad \forall(l,c) \in E(c) \mid 
%\end{equation}
\begin{equation}
    \omega_{l,c}'\leftarrow \mu_c,
\end{equation}
since edges in the biadjacency matrix are from literals to clauses at all times.


The reparameterization trick is still applied for each edge, but $\epsilon_c$ is sampled per clause instead of per edge, so that all edges that connect to a clause are forced to not only bear the same logit, but also the same importance score during training. The equation \ref{eq:reparam_trick} thus changes to:
\begin{equation}
    \epsilon_c \sim \text{Uniform}(0,1), \qquad \hat{e}_{l,c}=\sigma((\log \epsilon_c - \log(1-\epsilon_c)+\omega_{l,c}/\tau).
\end{equation}


\section{Implementation details}
In the following, we provide the implementation details needed to reproduce our results. This includes the general replication of the PGExplainer in section \ref{sec:Replication_of_PGExplainer} and the specifics for the adaptation on NeuroSAT in section \ref{sec:Application_to_NeuroSAT}.

TODO: Here? Since the paper differs from the codebase and is imprecise about certain descriptions (see Holdijk), we aim to give a thorough introduction that regards everything.


\subsection{Replication of PGExplainer}
\label{sec:Replication_of_PGExplainer}

TODO: ALGORITHMS IN APPENDIX

\textbf{General description}: What framework (e.g., PyTorch, TensorFlow, scikit-learn) and tools you used.

\textbf{Architecture specifics}: Model structures, key layers, differences from the original paper.
- Architecture of MLP - Linear(input, 64), ReLU, Linear(64,c);
 Original paper: Linear(input, 64), ReLU, Linear(20,c) (Unclear, but code same as ours), Paper: input = 60 for node classification, 40 for graph classification
 OG Code, which we adapt: input = 180 for node classification, 40 for graph classification
 - Xavier initialization for all layers, biases of zero

\textbf{Hyperparameters} (default ones):
- temperature schedule: $\tau^{(t) = \tau_0(\frac{\tau_T}{\tau_0})^t}$, initial temperature $\tau_0$, final temperature $\tau_T$. Small temperature leads to more discrete graphs, may hinder backprop optimization
- Adam optimizer
- Hyperparams according to paper: $3*10^{-3}$ LR, size reg 0.5, entropy reg 1.0, 30 epochs, $\tau_0 = 5.0$, $\tau_T = 2.0$ HYPERPARAMS VARY IN ACTUAL CODE, OVERSIMPLIFIED IN PAPER (contact with Holdijk)
RE-Paper settings tuned for each dataset. We conduct a search ourselves over both provided settings in experiments

\textbf{Extension description}: What exactly you changed/extended compared to the original work, and how you did it technically.

\textbf{Preprocessing}: If you had to massage or transform input data before feeding it to the model.

\textbf{Reproducibility}: Seed setting, tricks to make the results reproducible.
 - Adapted ability to set seeds from RE-PGE for reproducibility
 - Hard seeded train/eval/test split for data in inductive setting

\textbf{Challenges}: Brief notes if you faced and solved technical issues during reimplementation (e.g., missing details in the original paper).

\textbf{Downstream model specifics}:

Xavier initialization for all layers

Notes on Original code of target model: Not clearly specified in paper, but according to code:
- No specific args set for layers etc., except hyperparameters for different datasets (see sweeps)
- Therefore, assuming default settings for layers -> Graph conv layer uses ReLu activation (done), bias initialized with zeroes (done), no dropout(partially done), and embedding normalization! (TODO), default order 'AW'(see 4.17), weight init 'glorot' (Done with Xavier)
- WE USED PyG GraphConv since it allows for bipartite+edge weights; but compare to layer close to original!!\bigskip

"Default" graph layer used in original code; very similar to the semi-supervised layer\ref{}:
\begin{align}
    Z &= \hat{A}XW \\
    f(H(l),A)=\sigma(AH(l)W(l))
\end{align}

In different paper version authors state:
"Each GNN layer is represented by $f(H(l),A)=\sigma(W(l)AH(l))$, where $H(l)$ is the hidden representations of nodes in the $l$-th layer, $A$ is the normalized Laplacian matrix, and $W(l)$ is the weight matrix."

Note that a pytorch implementation of the semi-supervised layer is used in the replication paper, with Sigmoid activation:
\begin{align}
    X' &= \hat{D}^{-\frac{1}{2}}\hat{A}\hat{D}^{-\frac{1}{2}}X\Theta \\
    H^{l+1} &= \sigma(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)})
\end{align}
We use a pytoch implementation of the Weisfeiler and Leman Go Neural layer with sigmoid activation:
\begin{align}
    x_i' = W_1x_i+W_2 \sum_{j\in\mathcal{N}(i)}e_{j,i}\cdot x_j \\
    x_i^{(l)} = \sigma(W_1^{(l)}x_i^{(l-1)}+W_2^{(l)}\cdot\sum_{j\in\mathcal{N}(i)}e_{j,i}\cdot x_j^{(l-1)}),
\end{align}


\begin{itemize}
    \item Started by reimplementing the downstream tasks used in og paper for node and graph class.
    \item datasets taken from original and transformed to a "pyg format", to keep original structure and ground truths
    \item Graph: BA-2Motif from pyg library with self generated gt, MUTAG dataset taken from pyg and added gt-labels that were added in original dataset
    \item Created pytorch/pytorch geometric implementation of 3-layer graph conv networks
    \item architecture as described in paper: ReLu activation, Pooling for graph net
    \item Same hyperparameters?(Adam, $1*10^-3$ lr, 1000 epochs)/experimental setup?
    \item ADDED Dropout(0.1) to improve performance/overfitting on node tasks
    \item Fully connected layer: torch geometric GraphConv to allow passing of edge weights(+ bipartite)!?
    \item Original references sageConv in paper, pyG impl. does not allow edge weights, verify!
    \item GATConv for Graph attention(referenced by original)
    \item Alternatives: GCNConv(edge weights, no bipartite graphs)
    \item 80/10/10 split
    \item => Similar accuracies achieved
\end{itemize}
 

 Explainer:
 This is irrelevant for paper, description of why reparameterization trick necessary
 - First approach tried passing a masked edge index to downstream task with edge weights > 0.5
 => Unable to learn with hard cut-off (bad for gradients). (Same with TopK probably?!)
 - Pass calculated edge weights to downstream task for learning. If no edge weights are passed(downstream task), all edge weights are initialized with one to represent all edges beeing relevant
 
%\section{Benchmarking}
%\section{Application on Bipartite Graphs?}#

\subsection{Application to NeuroSAT}
\label{sec:Application_to_NeuroSAT}
and adapt the NeuroSAT model to allow passing edge weights into the adjacency matrix. PGExplainer adapted for SAT data, NeuroSAT embeddings and evaluation.


Reimplementation of NeuroSAT provided by Rodhi. As the code used for NeuroSAT can also be found in our Repository, we stress that only the changes described in the following chapter are part of our work. \\


Only change in NeuroSAT: pass edge weights into adjacency matrix. Calculates .... \\



TODO: NECESSARY?? OR MOVE TO IMPLEMENTATION DETAILS/SETUP??
Since NeuroSAT can be regarded as a black box model in the context of our work, we only describe how it passes messages superficially. In each iteration, each clause receives messages from its neighboring literals to update its embedding. Then, each literal receives messages not only from its neighboring clauses, but also from its complementary literal, to update its embedding.



\section{Experimental setup}
In this section we describe the setup for the experiments that we perform on PGExplainer. We start with the utilization of PGExplainer in the inductive setting, similar to the study performed by the authors. Additionally, we include experiments on the performance in the collective setting for the sake of completeness. Lastly, we present our approach for generating bipartite explanations for NeuroSAT \cite{} predictions of SAT problems.



Datasets used: Name, origin, train/test/validation split (or cross-validation if you use it).

Training procedures: How long you trained, on which hardware (especially if relevant), early stopping criteria, etc.

Evaluation metrics: AUROC score evaluation for quantitative evaluation; Paper does not describe calculation, code varies between datasets. Since explanations are collective, calculating global AUROC is valid, because all edges share the same network parameters (?). We propose consistently calculating local AUROC for each instance and meaning over the dataset, as we use inductive setting. Additionally, AUROC for (sub-)graphs that contain only or no motif edges cannot be calculated individually and is usually excluded/skipped. This is not regarded in the global calculation. This is not discussed in the original paper.

Hyperparameter tuning: Hyperparameter tuning using grid search for each dataset

Baselines: Original in both inductive and collective setting, RE-PGE in collective setting

Experimental protocol: Averaged over 10 seeds account for randomness, similar to original.

Visualization tools: If you generated plots/graphs during experiments (like loss curves), you can briefly mention this.

\subsection{PGExplainer in the inductive setting}


TODO: WE USE PGEXPLAINER IN INDUCTIVE SETTING, OPPOSED TO COLLECTIVE SETTING USED IN ORIGINAL/GNNExplainer. INDUCTIVE SETTING EXPERIMENTS ALSO PERFORMED IN PGEXPLAINER!

%"In this section, we empirically demonstrate the effectiveness of PGExplainer in the inductive setting. In the inductive setting, we select α instances for training, (N − α)/2 for validation, and the rest for testing. α is ranged from [1,2,3,4,5,30]. Note that, with α =1, our method degenerates to the single-instance explanation method. Recall that to explain a set of instances, GNNExplainer first detects a reference node and then computes the explanation for the reference node. The explanation is then generalized to other nodes with graph alignment(SOURCE GNNExplainer)"

\textbf{Datasets}
For our reimplementation we perform the experiments on the same datasets used in the original. These were constructed by the authors similarly to the ones used in the baseline GNNExpplainer. Four synthetic datasets were used for the node classification tasks. For the graph classification task the authors provide one synthetic dataset as well as the real-world dataset MUTAG. The synthetic datasets are constructed by creating a base graph and attaching motifs to random nodes of the base graph. These motifs determine the labels of the nodes or graphs, depending on the task at hand, and therefore serve as the ground truth explanations that the explainer shall detect. Statistics of each dataset can be found in table \ref{tab:dataset-statistics}. We will give a short description of each dataset.

Since three of the synthetic datasets use a Barabási-Albert (BA) graph as a base, we briefly introduce the BA model. The BA model generates scale-free networks that grow over time. Starting with an initialization network of $m_0 \geq m$ nodes, at each step a new node is added and connected to $m$ of the nodes already existing in the graph. The probability for each node to be selected as a neighbor depends on its degree, leading to a higher probability for nodes that already have a high degree rather than nodes with a low degree \cite{albert2002statistical}.

BA-Shapes is the first node dataset that consists of a single BA-graph with 300 nodes and 80 "house" motifs - five nodes resembling the shape of a house (TODO: see xy). Base graph nodes are labeled with 0 while nodes at the top/middle/bottom of the "house" are labeled with 1,2,3, respectively. The top node of each house motif is attached to a random base graph node. Additional edges are added for perturbation. Each node is assigned a 10-dimensional feature vector of 1s.
BA-Community consists of two unified BA-Shapes graphs (TODO: connected how). The features of the nodes are sampled from two Gaussian distributions. Nodes are labeled as in BA-Shapes for each community respectively, leading to 8 classes in total.
Tree-Cycles uses an 8-level balanced binary tree as a base graph. 80 cycle motifs, consisting of a 6 node cycle, are attached to random nodes from the base graph. Node features are assigned as a 10-dimensional vector of 1s. A node of the base graph is labeled as 0 and a motif node is labeled as 1.
The Tree-Grid dataset is assembled in the same way as Tree-Cycles, with the difference that the motifs are 3-by-3 grids. Node features and labels also follow the same procedure.
BA-2Motif is the first graph dataset with 800 graphs. Each of these graphs is obtained by attaching either a "house" or a cycle as a motif to a base BA graph with 20 nodes. According to the attached motif the graphs are assigned one of two labels, with 0 and 1 implying a house and circle, respectively (TODO: CHECK).
The real-world dataset MUTAG contains $4,337$ molecule graphs that are assigned to one of 2 classes, depending on the molecules mutagenic effect \ref{}. Following \ref{}, carbon rings with chemical groups $NH_2$ or $NO_2$ are known to be mutagenic, with carbon rings in general existing in both mutagenic and non-mutagenic graphs. The authors thus propose treating the carbon ring as a shared base graph and $NH_2$ and $NO_2$ as motifs for mutagenic graphs. Since there are no explicit motifs for the non-mutagenic graphs, these grapgs are not considered in PGExplainer.

NOTE that in the collective setting used in the original paper the explainer is trained and evaluated on the same data. This data is further reduced by only using graphs and nodes that contain a ground truth motif. This makes sense for evaluation, since the AUROC cannot be calculated for ground truths with only one class present. However, the authors do not specify why the training is performed only on these instances. Therefore, only the mutagenic graphs where either $NH_2$ or $NO_2$ are present are selected for the MUTAG experiment. In the node classification experiments the node sets used for training and evaluation were further finetuned per dataset. This leads to a selection of either all nodes that are part of a motif, or only one node per motif. This is also left unexplained by the authors.

\begin{table}[h]
    \centering
    \scriptsize
    \begin{tabular}{l|cccc|cc}
    \hline
    \textbf{} & \textbf{BA-Shapes} & \textbf{BA-Community} & \textbf{Tree-Cycles} & \textbf{Tree-Grid} & \textbf{BA-2motifs} & \textbf{MUTAG} \\
    \hline
    \#graphs & 1 & 1 & 1 & 1 & 1,000 & 4,337 \\
    \#nodes  & 700 & 1,400 & 871 & 1,231 & 25,000 & 131,488 \\
    \#edges  & 4,110 & 8,920 & 1,950 & 3,410 & 51,392 & 266,894 \\
    \#labels & 4 & 8 & 2 & 2 & 2 & 2 \\
    \hline
    \end{tabular}
    \caption{Dataset statistics for Node and Graph Classification tasks.}
    \label{tab:dataset-statistics}
    \end{table}

\bigskip

\textbf{Hyperparameter search}

As found by Holdijk et al.\ref{} the PGExplainer is very sensitive to hyperparameter setting on each dataset. Therefore, we conduct hyperparameter searches for each of the datasets to obtain best performing explainers. We follow Liashchynskyi et al. \cite{liashchynskyi2019grid} to perform grid searches over the parameter space that we define as an extended combination of the setting used in the original\ref{}, as well as the configs provided in Replication study by Holdijk et al.\ref{}. More details can be found in Appendix\ref{}.

\begin{table}[h!]
    \centering
    \begin{tabular}{|l|l|}
    \hline
    \textbf{Parameter} & \textbf{Values} \\ \hline
    epochs & 20 \\ \hline
    paper\_loss & True, False \\ \hline
    tT & 1.0, 5.0 \\ \hline
    size\_reg & 0.03 \\ \hline
    entropy\_reg & 0.01 \\ \hline
    lr\_mlp & 0.003, 0.0003 \\ \hline
    sampled\_graphs & 10 \\ \hline
    sample\_bias & 0.0, 0.5 \\ \hline
    batch\_size & 16, 64 \\ \hline
    seed & 74, 75, 76 \\ \hline
    \end{tabular}
    \caption{Grid Search Sweep Configuration}
    \label{tab:sweep_config}
\end{table}




We follow the experimental setup from the PGExplainer as closely as possible. Since the textual description refers to the setup from GNNExplainer and is lacking in some aspects, we extract the missing information from the codebase. As the hyperparameters are unclear or not comprehensible for some tasks we also draw information from the configs of PYTORCH REIMPL. \\



Additionally, experiment in the collective setting with the configuration and setup described for inductive setting - difference lies test data seen during training. Used for fair comparison with the original paper and the replication paper.




\subsection{PGExplainer applied to NeuroSAT}

We create required data with provided methods, add unsat cores and MUSs as gt


Generated batches of unsat problems that "turned" unsat because of last added clause. 10 literals per problem. Only unsat to test for unsat cores, that only apply for unsat problems. Calculated unsat cores with solver xy by adding negative assumption literals per clause and passing these as assumption for calulation. The edges of the clauses present in the unsat core were treated as ground truth. \\


For quant. eval. adapted roc auc as metric as done in PGExplainer. Results seem "good" but qual. eval. shows different result. roc auc bad metric? \\
For qual. eval. topk(=number of edges in gt) edges of predictions were highlighted to be compared to gt edges. For quant. eval. the edge probabilites were compared to gt with 1s for edges in gt and 0s for rest. \\


