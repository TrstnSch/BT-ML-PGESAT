\chapter{Discussion}

In the following chapter we discuss our results regarding the general reproducibility of the PGExplainer when using modified \acp{TM} in an inductive setting (see Section \ref{sec:discuss_pge}). Moreover, we elaborate on the explanations provided for predictions of the NeuroSAT model and their alignment with the human-understandable concepts in Section \ref{sec:discuss_sat}.


\section{Generalizability of PGExplainer}
\label{sec:discuss_pge}
When taking a closer look at the simplicity of replicating the PGExplainer, we share the concerns highlighted by Holdijk et al. \cite{holdijk2021re}. The many inconsistencies between paper and codebase, as well as the lackluster documentation of experiments hinder the implementation. However, we are able to achieve results comparable to the existing replication baseline, accentuating the explainer's functionality on modified \acp{TM}. Furthermore, we were able to achieve promising results in simplified experiment settings, highlight the general ability to explain instances. The explainer's generalization and global understanding of \acp{TM} were not sufficient, and we cannot confirm these claims at this point.

We stress the necessity of a general setting for evaluating explanations of node instances, as well as the importance of a stable explainer setup, to evaluate the actual explanations using any GNN rather than the performance of the explainer. We were unable to generalize the setup, which results in the explainer requiring a lot of fine-tuning to achieve consistent results. This in itself is impeded by slightly different \acp{TM} resulting in vastly different explanations.

Further research is required regarding the reason for opposite classification of selected tasks, to confirm whether this can be understood as an improvement in performance.

\section{Explanations for Unsatisfiable SAT Problems}
\label{sec:discuss_sat}

It seems that the PGExplainer is unable to learn the important edges for unsatisfiable problems well with regard to our assumed GT. Though we were able to obtain a range of importance scores comparable to the replication experiments, the explanations did ultimately not align with MUSes. We believe that a drawback in our approach may lie in the introduced evidence problem of edge weights, due to not knowing how weighted edges affect the predictions of NeuroSAT internally. Moreover, our GT assumption may not be ideal, since multiple MUSes may exist for each instance, though these are not unlikely to share detected clauses. The constraints introduced by us may also be lacking or too restrictive for the PGExplainer.

Furthermore, our perturbation approach only modifies edges between literals and clauses, which essentially define a sub problem, but we do not consider the connections between literals and their negation. These may play a substantial role in the NeuroSAT predictions of unsatisfiability, which may hinder the learning process of the explainer if disregarded in the masking process. Lastly, since the NeuroSAT model is far more complex than the classification models used in the replication, the explanation networks tested may not be complex enough to capture the general features of MUSes or unsatisfiable cores in general. \bigskip

It is important to note that our idea considers unsatisfiable cores as a conceptual motif, rather than a strict topological motif in the sense of a specific subgraph of nodes aligned in a fixed way. Though the MUS of a singular problem can be understood as a fixed subgraph, this cannot be generalized to other sub problems by its topological information alone. Thus, the approach of using the PGExplainer may not be sufficient, since it focuses on graph structure rather than graph features. However, the replication of MUTAG achieved promising results and was partially reliant on features, indicating that further features may be necessary for the representation unsatisfiable problems. \bigskip

Ultimately, we believe that explainability methods can provide important insights into the functionality of GNNs, and are essential for supporting predictions of message passing networks such as NeuroSAT. This also suggests many possibilities for future work, including the application of different explainer models and evaluation of feature importance to explain unsatisfiable problems using machine learning methods. Other directions may focus on explaining singular instances of said problems, as well as rethinking the evaluation methods, especially in regard to \ac{GT} motifs.