\chapter{Conclusion}

To conclude this work we highlight our key findings and incentivize future work on the subject.
Though the replication of PGExplainer came with many inconsistencies and proved to be not as general as originally claimed, our results suggest that the method is able to explain instances inductively, as long as the instances used share the same topological information regarding the motifs. Generally explaining different unexplained node instances from the same motif proved to be more difficult and requires more investigation. In addition, further experiments regarding the observable flipped explanations are necessary to better understand and leverage this behavior. 

In brief, our work presents the PGExplainer in a more general context, seeking to pave the way for more universal approaches to evaluating GNN explanations.

Moreover, we were able to apply the PGExplainer to a NeuroSAT model and generate limited bipartite explanations for unsatisfiable problems in the form of subclauses. However, these do not meet our assumption of aligning with \acp{MUS} and did not prove unsatisfiable themselves, limiting the significance of these explanations.

We are optimistic that further work may allow for creating explanations that support NeuroSAT predictions.